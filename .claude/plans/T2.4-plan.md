# T2.4: Portfolio Optimizer & Stress Testing - Implementation Plan

## Overview

**Task:** Implement mean-variance portfolio optimization with constraints and stress testing framework.

**Dependencies:**
- T2.3 (COMPLETE): BarraRiskModel provides factor covariance, specific risks
- cvxpy v1.7.3 (INSTALLED): CLARABEL, OSQP, SCS solvers available

**Deliverables:**
1. `libs/risk/portfolio_optimizer.py` - Mean-variance optimization
2. `libs/risk/stress_testing.py` - Historical and hypothetical stress tests
3. `tests/libs/risk/test_portfolio_optimizer.py`
4. `tests/libs/risk/test_stress_testing.py`
5. `docs/ADRs/ADR-0021-risk-model-implementation.md`

---

## Part 0: Integration with BarraRiskModel

### 0.1 Covariance Matrix Assembly

The optimizer requires the full N×N asset covariance matrix, constructed from BarraRiskModel:

```python
def build_asset_covariance(
    risk_model: BarraRiskModel,
    universe_permnos: list[int],
) -> tuple[np.ndarray, list[int]]:
    """
    Build full asset covariance matrix from Barra model.

    Sigma = B @ F @ B.T + D

    Where:
    - B = N×K factor loadings matrix
    - F = K×K factor covariance matrix (from T2.2)
    - D = N×N diagonal specific variance matrix

    Args:
        risk_model: BarraRiskModel from T2.3
        universe_permnos: List of permnos to include

    Returns:
        Tuple of:
        - sigma: N×N asset covariance matrix (daily)
        - aligned_permnos: Permnos in matrix order (after coverage filtering)
    """
    # 1. Filter to covered permnos (have both loadings and specific risk)
    loadings_permnos = set(risk_model.factor_loadings["permno"].to_list())
    specific_permnos = set(risk_model.specific_risks["permno"].to_list())
    covered = [p for p in universe_permnos if p in loadings_permnos and p in specific_permnos]

    if len(covered) < len(universe_permnos):
        logger.warning(
            f"Universe coverage: {len(covered)}/{len(universe_permnos)} "
            f"({len(covered)/len(universe_permnos):.1%})"
        )

    # 2. Extract aligned factor loadings (N×K)
    B = risk_model.factor_loadings.filter(
        pl.col("permno").is_in(covered)
    ).sort("permno").select(risk_model.factor_names).to_numpy()

    # 3. Get factor covariance (K×K)
    F = risk_model.factor_covariance

    # 4. Extract aligned specific variances (N×1 diagonal)
    D_diag = risk_model.specific_risks.filter(
        pl.col("permno").is_in(covered)
    ).sort("permno")["specific_variance"].to_numpy()

    # 5. Compute full covariance: Sigma = B @ F @ B.T + diag(D)
    sigma = B @ F @ B.T + np.diag(D_diag)

    # 6. Ensure PSD (regularize if needed)
    sigma = _regularize_covariance(sigma)

    return sigma, sorted(covered)
```

### 0.2 Coverage Validation

```python
def validate_universe_coverage(
    risk_model: BarraRiskModel,
    universe: list[int],
    min_coverage: float = 0.8,
) -> tuple[bool, float, list[int]]:
    """
    Validate universe has sufficient coverage in risk model.

    Returns:
        Tuple of (is_valid, coverage_ratio, missing_permnos)
    """
    loadings_permnos = set(risk_model.factor_loadings["permno"].to_list())
    specific_permnos = set(risk_model.specific_risks["permno"].to_list())
    covered = loadings_permnos & specific_permnos & set(universe)
    missing = list(set(universe) - covered)

    coverage = len(covered) / len(universe) if universe else 1.0
    return coverage >= min_coverage, coverage, missing
```

---

## Part 1: Portfolio Optimizer (`libs/risk/portfolio_optimizer.py`)

### 1.1 Core Classes

```python
@dataclass
class OptimizerConfig:
    """Configuration for portfolio optimization."""
    solver: str = "CLARABEL"  # Primary solver (OSQP as fallback)
    solver_timeout: float = 30.0  # Seconds
    verbose: bool = False

    # Constraint defaults
    max_position_weight: float = 0.10  # 10% max per position
    min_position_weight: float = 0.0   # No shorts by default
    max_sector_weight: float = 0.30    # 30% max per sector
    max_factor_exposure: float = 0.50  # |exposure| <= 0.5 sigma

    # Budget/Leverage constraints
    gross_leverage_max: float = 1.0    # sum(|w|) <= gross_leverage_max
    net_exposure_target: float = 1.0   # sum(w) = net_exposure_target

    # Transaction costs (wired into objective)
    # Linear cost: cost = tc_linear * sum(|w - w_current|)
    # Quadratic cost: cost = tc_quadratic * sum((w - w_current)^2)
    tc_linear_bps: float = 10.0        # Linear transaction cost (bps)
    tc_quadratic_bps: float = 0.0      # Quadratic market impact (bps)

    # Turnover penalty (lambda in objective)
    turnover_penalty: float = 0.0      # Penalty weight for turnover in objective

    # Risk-free rate for Sharpe calculations
    risk_free_rate: float = 0.0        # Annual risk-free rate

@dataclass
class OptimizationResult:
    """Result of portfolio optimization."""
    solution_id: str  # UUID
    as_of_date: date
    objective: str  # 'min_variance', 'max_sharpe', 'risk_parity'
    status: str  # 'optimal', 'suboptimal', 'infeasible'

    # Portfolio metrics
    optimal_weights: pl.DataFrame  # symbol, weight, delta_weight
    expected_return: float | None
    expected_risk: float  # Annualized volatility
    sharpe_ratio: float | None

    # Cost metrics
    turnover: float  # Sum of |delta_weight|
    transaction_cost: float  # Estimated cost

    # Solver info
    solver_time_ms: int
    solver_status: str

    # Provenance
    model_version: str
    dataset_version_ids: dict[str, str]
```

### 1.2 Optimizer Class Design

```python
class PortfolioOptimizer:
    """
    Mean-variance portfolio optimizer using cvxpy.

    Supports:
    - Minimum variance optimization
    - Maximum Sharpe ratio optimization
    - Risk parity optimization
    - Box constraints (min/max position weights)
    - Sector constraints (max sector exposure)
    - Factor exposure constraints (neutrality)
    - Transaction cost penalties
    """

    def __init__(
        self,
        risk_model: BarraRiskModel,
        config: OptimizerConfig | None = None,
    ):
        self.risk_model = risk_model
        self.config = config or OptimizerConfig()

    def optimize_min_variance(
        self,
        universe: list[str],  # Symbols or permnos
        current_weights: dict[str, float] | None = None,
        sector_map: dict[str, str] | None = None,  # symbol -> sector
        constraints: list[Constraint] | None = None,
    ) -> OptimizationResult:
        """Minimize portfolio variance subject to constraints."""

    def optimize_max_sharpe(
        self,
        universe: list[str],
        expected_returns: dict[str, float],  # symbol -> expected return
        current_weights: dict[str, float] | None = None,
        sector_map: dict[str, str] | None = None,
        constraints: list[Constraint] | None = None,
    ) -> OptimizationResult:
        """Maximize Sharpe ratio subject to constraints."""

    def optimize_risk_parity(
        self,
        universe: list[str],
        current_weights: dict[str, float] | None = None,
    ) -> OptimizationResult:
        """Equal risk contribution optimization."""
```

### 1.3 Constraint System

```python
class Constraint(Protocol):
    """Protocol for optimization constraints."""
    def apply(self, w: cp.Variable, context: dict) -> cp.Constraint | list[cp.Constraint]: ...
    def validate(self, context: dict) -> list[str]: ...  # Return validation errors

@dataclass
class BudgetConstraint:
    """
    Net exposure (sum of weights) constraint.

    Enforces: sum(w) = target (or within tolerance)
    """
    target: float = 1.0  # Fully invested
    tolerance: float = 0.0  # If >0, allows sum(w) in [target-tol, target+tol]

    def apply(self, w: cp.Variable, context: dict) -> list[cp.Constraint]:
        if self.tolerance == 0:
            return [cp.sum(w) == self.target]
        return [
            cp.sum(w) >= self.target - self.tolerance,
            cp.sum(w) <= self.target + self.tolerance,
        ]

    def validate(self, context: dict) -> list[str]:
        return []

@dataclass
class GrossLeverageConstraint:
    """
    Gross leverage (sum of absolute weights) constraint.

    Enforces: sum(|w|) <= max_leverage
    """
    max_leverage: float = 1.0  # Long-only: 1.0, 130/30: 1.6

    def apply(self, w: cp.Variable, context: dict) -> list[cp.Constraint]:
        return [cp.norm(w, 1) <= self.max_leverage]

    def validate(self, context: dict) -> list[str]:
        if self.max_leverage < 0:
            return ["max_leverage must be non-negative"]
        return []

@dataclass
class BoxConstraint:
    """
    Per-position weight bounds.

    Enforces: min_weight <= w_i <= max_weight for all i
    """
    min_weight: float = 0.0
    max_weight: float = 0.10

    def apply(self, w: cp.Variable, context: dict) -> list[cp.Constraint]:
        return [w >= self.min_weight, w <= self.max_weight]

    def validate(self, context: dict) -> list[str]:
        errors = []
        if self.min_weight > self.max_weight:
            errors.append(f"min_weight ({self.min_weight}) > max_weight ({self.max_weight})")
        return errors

@dataclass
class SectorConstraint:
    """
    Sector exposure bounds.

    Enforces: sum(w_i for i in sector) <= max_sector_weight for each sector
    """
    sector_map: dict[int, str]  # permno -> sector (validated against universe)
    max_sector_weight: float = 0.30
    min_sector_weight: float = 0.0  # Optional minimum

    def apply(self, w: cp.Variable, context: dict) -> list[cp.Constraint]:
        constraints = []
        permnos = context["permnos"]  # List of permnos in order

        # Group indices by sector
        sector_indices: dict[str, list[int]] = {}
        for i, p in enumerate(permnos):
            sector = self.sector_map.get(p)
            if sector:
                sector_indices.setdefault(sector, []).append(i)

        for sector, indices in sector_indices.items():
            sector_weight = cp.sum(w[indices])
            if self.min_sector_weight > 0:
                constraints.append(sector_weight >= self.min_sector_weight)
            constraints.append(sector_weight <= self.max_sector_weight)

        return constraints

    def validate(self, context: dict) -> list[str]:
        errors = []
        permnos = context.get("permnos", [])
        mapped = set(self.sector_map.keys()) & set(permnos)
        if len(mapped) < len(permnos) * 0.8:
            errors.append(
                f"Sector map covers only {len(mapped)}/{len(permnos)} universe permnos"
            )
        return errors

@dataclass
class FactorExposureConstraint:
    """
    Factor neutrality/exposure constraint.

    Enforces: |w' @ B_factor - target| <= tolerance
    Or if tolerance=0: w' @ B_factor = target (exact)
    """
    factor_name: str
    target_exposure: float = 0.0   # Target exposure (0 = neutral)
    tolerance: float = 0.50        # |exposure - target| <= tolerance

    def apply(self, w: cp.Variable, context: dict) -> list[cp.Constraint]:
        B = context["factor_loadings"]  # N×K matrix
        factor_names = context["factor_names"]

        if self.factor_name not in factor_names:
            raise ValueError(f"Unknown factor: {self.factor_name}")

        factor_idx = factor_names.index(self.factor_name)
        factor_loading = B[:, factor_idx]  # N×1

        exposure = w @ factor_loading

        if self.tolerance == 0:
            return [exposure == self.target_exposure]
        return [
            exposure >= self.target_exposure - self.tolerance,
            exposure <= self.target_exposure + self.tolerance,
        ]

    def validate(self, context: dict) -> list[str]:
        factor_names = context.get("factor_names", [])
        if self.factor_name not in factor_names:
            return [f"Factor '{self.factor_name}' not in model: {factor_names}"]
        return []

@dataclass
class TurnoverConstraint:
    """
    Maximum turnover from current portfolio.

    Enforces: sum(|w - w_current|) <= max_turnover
    """
    current_weights: dict[int, float]  # permno -> weight
    max_turnover: float = 0.50

    def apply(self, w: cp.Variable, context: dict) -> list[cp.Constraint]:
        permnos = context["permnos"]
        w_current = np.array([self.current_weights.get(p, 0.0) for p in permnos])
        return [cp.norm(w - w_current, 1) <= self.max_turnover]

    def validate(self, context: dict) -> list[str]:
        if self.max_turnover < 0:
            return ["max_turnover must be non-negative"]
        return []
```

### 1.4 cvxpy Formulation

**Minimum Variance with Transaction Costs:**
```
minimize    w' @ Sigma @ w
            + lambda_turnover * ||w - w_current||_1      # Linear turnover penalty
            + lambda_quadratic * ||w - w_current||_2^2   # Quadratic market impact
subject to  sum(w) = 1                                   # Budget constraint
            w_min <= w <= w_max                          # Box constraints
            sector_constraints
            factor_constraints
```

Where:
- `lambda_turnover = config.turnover_penalty + config.tc_linear_bps / 10000`
- `lambda_quadratic = config.tc_quadratic_bps / 10000`

**cvxpy Implementation:**
```python
def _build_min_variance_problem(
    self,
    sigma: np.ndarray,
    n: int,
    w_current: np.ndarray | None,
    constraints: list[Constraint],
    context: dict,
) -> tuple[cp.Variable, cp.Problem]:
    w = cp.Variable(n)

    # Objective: w' @ Sigma @ w + turnover costs
    objective = cp.quad_form(w, sigma)

    if w_current is not None:
        tc_linear = self.config.turnover_penalty + self.config.tc_linear_bps / 10000
        if tc_linear > 0:
            objective += tc_linear * cp.norm(w - w_current, 1)
        if self.config.tc_quadratic_bps > 0:
            tc_quad = self.config.tc_quadratic_bps / 10000
            objective += tc_quad * cp.sum_squares(w - w_current)

    # Apply all constraints
    all_constraints = []
    for c in constraints:
        all_constraints.extend(c.apply(w, context))

    problem = cp.Problem(cp.Minimize(objective), all_constraints)
    return w, problem
```

**Maximum Sharpe (Return-Target QP Formulation):**

Instead of the problematic y/kappa transformation, use return-target QP:
```
maximize    (mu' @ w - r_f) / sqrt(w' @ Sigma @ w)
```

This is equivalent to solving a sequence of QPs with varying return targets:
```
For target_return in [r_min, r_max]:
    minimize    w' @ Sigma @ w
    subject to  mu' @ w >= target_return
                sum(w) = 1
                other_constraints

Then select the portfolio with highest Sharpe ratio.
```

**cvxpy Implementation:**
```python
def optimize_max_sharpe(
    self,
    universe: list[int],
    expected_returns: dict[int, float],
    current_weights: dict[int, float] | None = None,
    constraints: list[Constraint] | None = None,
    n_return_targets: int = 20,  # Number of points on efficient frontier
) -> OptimizationResult:
    """
    Maximize Sharpe ratio via efficient frontier search.

    Approach: Solve min-variance for multiple return targets,
    then select the portfolio with highest Sharpe ratio.

    Note: Transaction costs are NOT applied in max-Sharpe mode
    because the y/kappa transformation makes L1 costs non-convex.
    Use optimize_mean_variance_cost() for cost-aware optimization.
    """
    sigma, permnos = self._build_covariance(universe)
    mu = np.array([expected_returns.get(p, 0.0) for p in permnos])
    r_f_daily = self.config.risk_free_rate / 252

    # Find feasible return range
    min_ret, max_ret = self._find_return_range(mu, sigma, constraints)

    best_sharpe = -np.inf
    best_result = None

    for target_ret in np.linspace(min_ret, max_ret, n_return_targets):
        # Add return target constraint
        return_constraint = ReturnTargetConstraint(
            expected_returns=expected_returns,
            min_return=target_ret,
        )
        augmented_constraints = (constraints or []) + [return_constraint]

        result = self._solve_min_variance(
            sigma, permnos, mu, None,  # No transaction costs
            augmented_constraints,
        )

        if result.status == "optimal":
            sharpe = (result.expected_return - r_f_daily * 252) / result.expected_risk
            if sharpe > best_sharpe:
                best_sharpe = sharpe
                best_result = result

    if best_result is None:
        return self._infeasible_result("No feasible point on efficient frontier")

    best_result.sharpe_ratio = best_sharpe
    return best_result

@dataclass
class ReturnTargetConstraint:
    """Return target constraint for efficient frontier."""
    expected_returns: dict[int, float]
    min_return: float

    def apply(self, w: cp.Variable, context: dict) -> list[cp.Constraint]:
        permnos = context["permnos"]
        mu = np.array([self.expected_returns.get(p, 0.0) for p in permnos])
        return [mu @ w >= self.min_return]

    def validate(self, context: dict) -> list[str]:
        return []
```

**Mean-Variance with Costs (Alternative to Max-Sharpe):**
```python
def optimize_mean_variance_cost(
    self,
    universe: list[int],
    expected_returns: dict[int, float],
    risk_aversion: float = 1.0,  # gamma: higher = more risk-averse
    current_weights: dict[int, float] | None = None,
    constraints: list[Constraint] | None = None,
) -> OptimizationResult:
    """
    Mean-variance optimization with transaction costs.

    Objective: maximize mu' @ w - (gamma/2) * w' @ Sigma @ w - costs

    This formulation naturally handles transaction costs in QP form.
    """
    minimize    (gamma/2) * w' @ Sigma @ w
                - mu' @ w
                + tc_linear * ||w - w_current||_1
                + tc_quadratic * ||w - w_current||_2^2
    subject to  sum(w) = 1
                other_constraints
```

### 1.5 Solver Fallback Logic

```python
def _solve_with_fallback(self, problem: cp.Problem) -> str:
    """Try primary solver, fall back to secondary on failure."""
    solvers = [self.config.solver, "OSQP", "SCS"]

    for solver in solvers:
        try:
            problem.solve(solver=solver, verbose=self.config.verbose)
            if problem.status in [cp.OPTIMAL, cp.OPTIMAL_INACCURATE]:
                return solver
        except cp.SolverError:
            logger.warning(f"Solver {solver} failed, trying next")
            continue

    return "FAILED"
```

### 1.6 Ill-Conditioned Covariance Handling

```python
def _regularize_covariance(self, sigma: np.ndarray) -> np.ndarray:
    """Add small ridge to handle near-singular covariance."""
    min_eigenvalue = np.linalg.eigvalsh(sigma).min()
    if min_eigenvalue < 1e-8:
        ridge = max(1e-8 - min_eigenvalue, 1e-8)
        logger.warning(f"Regularizing covariance with ridge={ridge:.2e}")
        sigma = sigma + ridge * np.eye(sigma.shape[0])
    return sigma
```

### 1.7 Risk Parity Algorithm

Risk parity (Equal Risk Contribution) aims for equal marginal contribution to risk from each asset.

**ERC Condition:**
```
w_i * (Sigma @ w)_i = w_j * (Sigma @ w)_j  for all i, j
```

**Algorithm: Sequential Least Squares (Spinu 2013):**

```python
def optimize_risk_parity(
    self,
    universe: list[int],
    current_weights: dict[int, float] | None = None,
    max_iterations: int = 100,
    tolerance: float = 1e-6,
) -> OptimizationResult:
    """
    Risk parity optimization using convex reformulation.

    Algorithm: Solves the equivalent log-barrier problem:
        minimize    w' @ Sigma @ w - c * sum(log(w))
    where c controls the ERC condition.

    Then iteratively adjusts c until equal risk contribution is achieved.

    Convergence: Guaranteed for PSD covariance (convex problem).
    Typically converges in 10-30 iterations.
    """
    sigma, permnos = self._build_covariance(universe)
    n = len(permnos)

    # Initialize with equal weights
    w = np.ones(n) / n

    for iteration in range(max_iterations):
        # Compute marginal risk contributions
        sigma_w = sigma @ w
        total_risk = np.sqrt(w @ sigma_w)
        mrc = sigma_w / total_risk  # Marginal Risk Contribution

        # Risk contribution: RC_i = w_i * MRC_i
        rc = w * mrc
        rc_sum = rc.sum()

        # Target: equal risk contribution
        target_rc = rc_sum / n

        # Check convergence: all RC within tolerance of target
        rc_error = np.max(np.abs(rc - target_rc))
        if rc_error < tolerance:
            break

        # Update weights proportionally to target/actual RC ratio
        # This is the "inverse volatility" update
        w_new = w * (target_rc / rc)
        w_new = w_new / w_new.sum()  # Renormalize

        w = w_new

    # Verify equal risk contribution achieved
    sigma_w = sigma @ w
    total_risk = np.sqrt(w @ sigma_w)
    rc = w * (sigma_w / total_risk)
    rc_spread = rc.max() - rc.min()

    if rc_spread > tolerance * 10:
        logger.warning(
            f"Risk parity did not fully converge: RC spread = {rc_spread:.6f}"
        )

    return self._build_result(
        w, permnos, sigma, objective="risk_parity",
        status="optimal" if rc_spread < tolerance * 10 else "suboptimal",
        iterations=iteration + 1,
    )
```

**Alternative: cvxpy Convex Relaxation:**
```python
def _risk_parity_cvxpy(self, sigma: np.ndarray, n: int) -> np.ndarray:
    """
    Risk parity via log-barrier convex optimization.

    Reformulation by Maillard et al. (2010):
        minimize    w' @ Sigma @ w
        subject to  sum(log(w)) >= c  (controls diversification)
                    sum(w) = 1
                    w >= 0
    """
    w = cp.Variable(n)
    c = n * np.log(1/n)  # Target: log of equal-weight portfolio

    objective = cp.quad_form(w, sigma)
    constraints = [
        cp.sum(w) == 1,
        w >= 1e-6,  # Numerical stability
        cp.sum(cp.log(w)) >= c,
    ]

    problem = cp.Problem(cp.Minimize(objective), constraints)
    problem.solve(solver=self.config.solver)

    return w.value
```

**Test for Equal Risk Contribution:**
```python
def test_risk_parity_equal_contribution(self, sample_covariance_result):
    """Verify risk parity achieves equal risk contribution."""
    # ... setup ...
    result = optimizer.optimize_risk_parity(universe)

    # Compute risk contributions
    sigma = optimizer._build_covariance(universe)[0]
    w = result.optimal_weights["weight"].to_numpy()
    sigma_w = sigma @ w
    total_risk = np.sqrt(w @ sigma_w)
    rc = w * (sigma_w / total_risk)

    # All risk contributions should be approximately equal
    assert np.std(rc) < 0.01, f"RC not equal: std={np.std(rc)}"
    assert np.allclose(rc, rc.mean(), rtol=0.1), "RC spread too large"
```

---

## Part 2: Stress Testing (`libs/risk/stress_testing.py`)

### 2.1 Core Classes

```python
@dataclass
class StressScenario:
    """Definition of a stress scenario."""
    name: str  # 'GFC_2008', 'COVID_2020', 'rate_shock'
    scenario_type: str  # 'historical', 'hypothetical'
    description: str

    # Factor shocks (for hypothetical scenarios)
    factor_shocks: dict[str, float] | None = None  # factor_name -> shock (as return)

    # Historical period (for historical scenarios)
    start_date: date | None = None
    end_date: date | None = None

@dataclass
class StressTestResult:
    """Result of stress testing a portfolio."""
    test_id: str  # UUID
    portfolio_id: str
    scenario: StressScenario
    as_of_date: date

    # Impact metrics
    portfolio_pnl: float  # Estimated P&L under scenario
    portfolio_drawdown: float  # Peak-to-trough if historical
    factor_impacts: dict[str, float]  # factor -> contribution to P&L

    # Position-level detail
    worst_position: str | None
    worst_position_loss: float | None
    position_impacts: pl.DataFrame | None  # symbol, pnl, contribution

    # Provenance
    model_version: str
    dataset_version_ids: dict[str, str]
```

### 2.2 Stress Tester Class

```python
class StressTester:
    """
    Portfolio stress testing using factor model.

    Supports:
    - Historical scenarios (replay actual factor returns)
    - Hypothetical scenarios (user-defined factor shocks)
    - Position-level attribution
    """

    def __init__(
        self,
        risk_model: BarraRiskModel,
        historical_factor_returns: pl.DataFrame | None = None,  # date, factor_name, return
    ):
        self.risk_model = risk_model
        self.historical_returns = historical_factor_returns

        # Pre-defined scenarios (3 historical + 1 hypothetical)
        self._scenarios: dict[str, StressScenario] = {
            # Historical scenario 1: Global Financial Crisis
            "GFC_2008": StressScenario(
                name="GFC_2008",
                scenario_type="historical",
                description="Global Financial Crisis: Sep-Nov 2008",
                start_date=date(2008, 9, 1),
                end_date=date(2008, 11, 30),
            ),
            # Historical scenario 2: COVID-19 Market Crash
            "COVID_2020": StressScenario(
                name="COVID_2020",
                scenario_type="historical",
                description="COVID-19 Market Crash: Feb-Mar 2020",
                start_date=date(2020, 2, 19),
                end_date=date(2020, 3, 23),
            ),
            # Historical scenario 3: 2022 Rate Hike / Inflation Shock
            "RATE_HIKE_2022": StressScenario(
                name="RATE_HIKE_2022",
                scenario_type="historical",
                description="2022 Fed Rate Hikes & Inflation: Jan-Jun 2022",
                start_date=date(2022, 1, 3),
                end_date=date(2022, 6, 16),
            ),
            # Hypothetical scenario: Custom rate shock
            "RATE_SHOCK": StressScenario(
                name="RATE_SHOCK",
                scenario_type="hypothetical",
                description="Hypothetical rate shock: value up, growth down",
                factor_shocks={
                    "book_to_market": 0.05,  # Value up 5%
                    "momentum_12_1": -0.10,  # Momentum down 10%
                    "market_beta": -0.15,    # Market down 15%
                    "size": 0.03,            # Small cap up 3%
                    "volatility": -0.08,     # Low vol up (high vol down 8%)
                },
            ),
        }

    def run_stress_test(
        self,
        portfolio: pl.DataFrame,  # permno, weight
        scenario: str | StressScenario,
        portfolio_id: str | None = None,
    ) -> StressTestResult:
        """Run stress test for a single scenario."""

    def run_all_scenarios(
        self,
        portfolio: pl.DataFrame,
        portfolio_id: str | None = None,
    ) -> list[StressTestResult]:
        """Run all pre-defined scenarios."""

    def run_custom_scenario(
        self,
        portfolio: pl.DataFrame,
        factor_shocks: dict[str, float],
        scenario_name: str = "custom",
        portfolio_id: str | None = None,
    ) -> StressTestResult:
        """Run custom hypothetical scenario."""
```

### 2.3 Stress Test Computation

**P&L Components:**
```
Total P&L = Factor P&L + Specific P&L (optional)

Factor P&L = sum_k(exposure_k * factor_shock_k)
Specific P&L = estimated via sqrt(specific_variance) * z-score (optional, conservative)
```

**Design Decision:**
- Stress tests focus on **systematic (factor) risk** only
- Specific risk is NOT included by default because:
  1. Historical specific returns are stock-specific and not generalizable
  2. Hypothetical specific shocks are arbitrary
- Optional: Add `include_specific_risk=True` to add sqrt(specific_var) * 2 as conservative tail estimate

**Historical Scenario:**
```python
def _compute_historical_stress(
    self,
    portfolio: pl.DataFrame,
    scenario: StressScenario,
    include_specific_risk: bool = False,  # Optional specific risk estimate
) -> tuple[float, dict[str, float], float]:
    """
    Compute P&L using historical factor returns.

    P&L = sum over factors: exposure_k * cumulative_factor_return_k

    Scaling: Factor returns are compounded over the scenario period.
    No annualization - returns are as-realized over the period.

    Args:
        portfolio: DataFrame with permno, weight
        scenario: Historical scenario with date range
        include_specific_risk: If True, add conservative specific risk estimate

    Returns:
        Tuple of (total_factor_pnl, factor_pnl_dict, specific_risk_estimate)
    """
    if self.historical_returns is None:
        raise ValueError("Historical factor returns required for historical scenarios")

    # Get factor returns for scenario period
    period_returns = self.historical_returns.filter(
        (pl.col("date") >= scenario.start_date) &
        (pl.col("date") <= scenario.end_date)
    )

    if period_returns.height == 0:
        raise ValueError(
            f"No factor returns found for period {scenario.start_date} to {scenario.end_date}"
        )

    # Aggregate factor returns over period (compound returns)
    cumulative_factor_returns = period_returns.group_by("factor_name").agg(
        # Compound returns: prod(1 + r) - 1
        ((pl.col("return") + 1).product() - 1).alias("cumulative_return")
    )

    # Compute portfolio factor exposures: f = B' @ w
    exposures = self._compute_portfolio_exposures(portfolio)

    # Factor P&L contribution
    factor_pnl = {}
    total_factor_pnl = 0.0

    for factor_name in self.risk_model.factor_names:
        # Handle missing factor returns gracefully
        factor_ret_row = cumulative_factor_returns.filter(
            pl.col("factor_name") == factor_name
        )
        if factor_ret_row.height == 0:
            logger.warning(f"No historical returns for factor {factor_name}, assuming 0")
            factor_return = 0.0
        else:
            factor_return = factor_ret_row["cumulative_return"][0]

        exposure = exposures.get(factor_name, 0.0)
        contribution = exposure * factor_return
        factor_pnl[factor_name] = contribution
        total_factor_pnl += contribution

    # Optional: Add specific risk estimate (conservative 2-sigma tail)
    specific_estimate = 0.0
    if include_specific_risk:
        specific_estimate = self._estimate_specific_stress(portfolio, scenario)

    return total_factor_pnl, factor_pnl, specific_estimate


def _estimate_specific_stress(
    self,
    portfolio: pl.DataFrame,
    scenario: StressScenario,
) -> float:
    """
    Conservative estimate of specific risk contribution.

    Uses portfolio specific variance scaled by period length and 2-sigma multiplier.

    specific_stress = -2 * sqrt(sum(w_i^2 * spec_var_i) * n_days)
    """
    # Get specific variances
    weights, _, specific_vars = self.risk_model._build_aligned_arrays(portfolio)

    # Portfolio specific variance (daily)
    port_specific_var = np.sum(weights**2 * specific_vars)

    # Scale by period length
    n_days = (scenario.end_date - scenario.start_date).days
    period_specific_var = port_specific_var * n_days

    # 2-sigma tail estimate (conservative downside)
    return -2.0 * np.sqrt(period_specific_var)
```

**Hypothetical Scenario:**
```python
def _compute_hypothetical_stress(
    self,
    portfolio: pl.DataFrame,
    factor_shocks: dict[str, float],
    include_specific_risk: bool = False,
) -> tuple[float, dict[str, float], float]:
    """
    Compute P&L using hypothetical factor shocks.

    P&L = sum over factors: exposure_k * shock_k

    Missing factors: Factors not in factor_shocks are assumed to have 0 shock.
    This is logged as a warning.

    Args:
        portfolio: DataFrame with permno, weight
        factor_shocks: factor_name -> shock (as return, e.g., -0.10 = -10%)
        include_specific_risk: If True, add conservative specific risk estimate

    Returns:
        Tuple of (total_factor_pnl, factor_pnl_dict, specific_risk_estimate)
    """
    exposures = self._compute_portfolio_exposures(portfolio)

    # Warn about factors with exposure but no shock
    for factor_name in self.risk_model.factor_names:
        if factor_name not in factor_shocks and abs(exposures.get(factor_name, 0)) > 0.01:
            logger.warning(
                f"Factor {factor_name} has exposure {exposures.get(factor_name, 0):.3f} "
                f"but no shock defined, assuming 0"
            )

    factor_pnl = {}
    total_factor_pnl = 0.0

    for factor_name, shock in factor_shocks.items():
        if factor_name not in self.risk_model.factor_names:
            logger.warning(f"Unknown factor in shocks: {factor_name}, skipping")
            continue
        exposure = exposures.get(factor_name, 0.0)
        contribution = exposure * shock
        factor_pnl[factor_name] = contribution
        total_factor_pnl += contribution

    # Optional: Add specific risk estimate (hypothetical 2-sigma tail)
    specific_estimate = 0.0
    if include_specific_risk:
        # Assume 1-day shock for hypothetical, scaled by 2-sigma
        weights, _, specific_vars = self.risk_model._build_aligned_arrays(portfolio)
        port_specific_var = np.sum(weights**2 * specific_vars)
        specific_estimate = -2.0 * np.sqrt(port_specific_var)

    return total_factor_pnl, factor_pnl, specific_estimate
```

---

## Part 3: ADR-0021 Risk Model Implementation

### Structure

```markdown
# ADR-0021: Risk Model Implementation

## Status
Accepted

## Date
2025-12-07

## Context
- Need production-ready risk analytics for portfolio management
- Barra-style multi-factor model chosen for industry standard approach
- Must integrate with existing data warehouse (P4T1)

## Decision
1. **Architecture**: Layered design with T2.1 factors -> T2.2 covariance -> T2.3 risk model -> T2.4 optimizer
2. **Risk Model**: Barra-style with 5 canonical factors
3. **Optimizer**: cvxpy with CLARABEL/OSQP solvers
4. **Stress Testing**: Historical replay + hypothetical scenarios

## Consequences
- Positive: Industry-standard methodology, extensible framework
- Negative: Factor model assumes linear relationships
- Mitigation: Stress testing validates under extreme conditions
```

---

## Part 4: Test Plan

### 4.1 test_portfolio_optimizer.py

```python
class TestOptimizerConfig:
    """Test configuration validation."""
    def test_default_config(self): ...
    def test_custom_config(self): ...
    def test_transaction_cost_params_wired(self): ...  # Verify tc_linear_bps affects result

class TestMinVarianceOptimization:
    """Test minimum variance optimization."""
    def test_optimize_basic(self): ...  # Simple case, verify weights sum to 1
    def test_optimize_with_box_constraints(self): ...  # Min/max weights enforced
    def test_optimize_with_sector_constraints(self): ...  # Sector limits enforced
    def test_optimize_with_factor_constraints(self): ...  # Factor neutrality
    def test_optimize_with_turnover_constraint(self): ...  # Turnover limit
    def test_optimize_with_transaction_costs(self): ...  # Cost penalty affects result
    def test_optimize_with_budget_constraint(self): ...  # Net exposure = 1
    def test_optimize_with_gross_leverage(self): ...  # sum(|w|) <= max

class TestMaxSharpeOptimization:
    """Test maximum Sharpe optimization."""
    def test_optimize_basic(self): ...
    def test_optimize_tilts_toward_high_return(self): ...  # Verify alpha-seeking behavior
    def test_optimize_respects_risk_free_rate(self): ...  # r_f affects Sharpe calc
    def test_infeasible_return_target(self): ...  # Return target impossible -> graceful fail
    def test_all_negative_returns(self): ...  # Edge case: no positive alpha

class TestMeanVarianceCost:
    """Test mean-variance with costs (alternative to Max-Sharpe)."""
    def test_cost_aware_optimization(self): ...  # Transaction costs in objective
    def test_risk_aversion_parameter(self): ...  # Higher gamma = lower risk

class TestRiskParityOptimization:
    """Test risk parity optimization."""
    def test_equal_risk_contribution(self): ...  # Verify RC std < threshold
    def test_convergence_within_iterations(self): ...  # Converges in <100 iterations
    def test_risk_parity_vs_equal_weight(self): ...  # Different from 1/N when correlated

class TestSolverRobustness:
    """Test solver edge cases."""
    def test_solver_fallback(self): ...  # Primary fails, secondary succeeds
    def test_ill_conditioned_covariance(self): ...  # Near-singular handled via regularization
    def test_infeasible_constraints(self): ...  # Graceful infeasible result
    def test_unbounded_problem(self): ...  # Missing budget constraint -> detect
    def test_performance_500_stocks(self): ...  # <5s for 500 stocks (M1 Mac baseline)

class TestBarraModelIntegration:
    """Test integration with BarraRiskModel (T2.3)."""
    def test_covariance_assembly_from_barra(self): ...  # B @ F @ B.T + D
    def test_universe_coverage_validation(self): ...  # Partial coverage handled
    def test_missing_risk_data_raises(self): ...  # <80% coverage -> error
    def test_factor_alignment(self): ...  # Factor order matches risk model

class TestConstraintValidation:
    """Test constraint validation methods."""
    def test_box_constraint_validation(self): ...  # min > max -> error
    def test_sector_constraint_validation(self): ...  # Missing sector map -> warning
    def test_factor_constraint_validation(self): ...  # Unknown factor -> error
    def test_turnover_constraint_validation(self): ...  # Negative turnover -> error

class TestOptimizationResult:
    """Test result structure and validation."""
    def test_result_schema(self): ...
    def test_to_storage_format(self): ...
    def test_provenance_tracking(self): ...  # dataset_version_ids populated
```

### 4.2 test_stress_testing.py

```python
class TestStressScenario:
    """Test scenario definitions."""
    def test_historical_scenario_fields(self): ...
    def test_hypothetical_scenario_fields(self): ...
    def test_three_historical_scenarios_defined(self): ...  # GFC, COVID, Rate Hike 2022

class TestHistoricalStress:
    """Test historical scenario replay."""
    def test_gfc_2008_scenario(self): ...  # Known bad period
    def test_covid_2020_scenario(self): ...
    def test_rate_hike_2022_scenario(self): ...  # 3rd historical scenario
    def test_factor_attribution_sums(self): ...  # Factor contributions sum to total
    def test_missing_factor_returns_handled(self): ...  # Missing factor -> 0, warning
    def test_empty_period_raises(self): ...  # No data for period -> error

class TestHypotheticalStress:
    """Test hypothetical scenarios."""
    def test_rate_shock_scenario(self): ...
    def test_custom_scenario(self): ...
    def test_single_factor_shock(self): ...
    def test_missing_factor_shock_warning(self): ...  # Factor with exposure but no shock -> warning
    def test_unknown_factor_shock_skipped(self): ...  # Unknown factor in shocks -> skip + warning

class TestSpecificRiskEstimate:
    """Test optional specific risk estimation."""
    def test_specific_risk_disabled_by_default(self): ...
    def test_specific_risk_conservative_estimate(self): ...  # 2-sigma tail
    def test_specific_risk_scales_with_period(self): ...  # sqrt(n_days)

class TestBarraModelIntegration:
    """Test integration with BarraRiskModel."""
    def test_exposure_computation(self): ...  # f = B' @ w
    def test_coverage_handling(self): ...  # Partial coverage
    def test_weight_normalization(self): ...  # Weights sum to 1

class TestStressTestResult:
    """Test result structure."""
    def test_result_schema(self): ...
    def test_worst_position_identified(self): ...
    def test_to_storage_format(self): ...
    def test_provenance_tracking(self): ...  # dataset_version_ids populated
```

### 4.3 Performance Test Baseline

```python
# Performance tests run on: Apple M1 Pro, 16GB RAM
# Baseline established: 2025-12-07

@pytest.mark.performance
def test_optimizer_500_stocks_under_5s(sample_risk_model_500):
    """Optimization must complete in <5 seconds for 500 stocks."""
    import time
    optimizer = PortfolioOptimizer(sample_risk_model_500)

    start = time.perf_counter()
    result = optimizer.optimize_min_variance(universe_500)
    elapsed = time.perf_counter() - start

    assert elapsed < 5.0, f"Optimization took {elapsed:.2f}s, expected <5s"
    assert result.status == "optimal"

@pytest.mark.performance
def test_stress_test_under_1s(sample_risk_model, sample_portfolio):
    """Stress test must complete in <1 second."""
    import time
    tester = StressTester(sample_risk_model, historical_factor_returns)

    start = time.perf_counter()
    result = tester.run_stress_test(sample_portfolio, "GFC_2008")
    elapsed = time.perf_counter() - start

    assert elapsed < 1.0, f"Stress test took {elapsed:.2f}s, expected <1s"
```

---

## Part 5: Implementation Order

1. **OptimizerConfig and OptimizationResult dataclasses**
2. **Constraint classes (Box, Sector, Factor, Turnover)**
3. **PortfolioOptimizer._build_covariance_matrix()** - Extract from risk model
4. **PortfolioOptimizer.optimize_min_variance()** - Core QP formulation
5. **PortfolioOptimizer.optimize_max_sharpe()** - Extended QP
6. **PortfolioOptimizer.optimize_risk_parity()** - Iterative/convex relaxation
7. **StressScenario and StressTestResult dataclasses**
8. **StressTester with pre-defined scenarios**
9. **StressTester.run_stress_test()** for both historical and hypothetical
10. **Tests for all components**
11. **ADR-0021 documentation**

---

## Acceptance Criteria Checklist

- [ ] Mean-variance optimization with cvxpy solver (CLARABEL/OSQP)
- [ ] Box, sector, and factor exposure constraints with validation
- [ ] Budget and gross leverage constraints
- [ ] Transaction cost modeling wired into objective (linear + quadratic)
- [ ] Max-Sharpe via return-target efficient frontier search
- [ ] Risk parity with convergence verification (equal RC)
- [ ] 3 historical stress scenarios (GFC 2008, COVID 2020, Rate Hike 2022)
- [ ] 1 hypothetical stress scenario (Rate Shock)
- [ ] Custom scenario input capability
- [ ] Integration with BarraRiskModel (covariance assembly: B@F@B.T + D)
- [ ] **Performance test**: Optimization solves in <5 seconds for 500 stocks (M1 baseline)
- [ ] **Robustness test**: Optimizer handles ill-conditioned covariance via regularization
- [ ] **Robustness test**: Optimizer returns infeasible result gracefully
- [ ] **Edge case test**: Infeasible Sharpe targets handled
- [ ] **Edge case test**: Missing factor shocks logged with warning
- [ ] >90% test coverage
- [ ] ADR-0021 documenting architecture decisions
