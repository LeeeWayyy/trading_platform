# T1.5 Fama-French Local Provider - Implementation Plan

**Component:** T1.5-Fama-French-Local-Provider
**Status:** Plan Review
**Version:** 1.2 (Codex R2 feedback incorporated)
**Created:** 2025-12-04

---

## Overview

Implement a read-only local provider for Fama-French factor data, downloading from Ken French's Data Library website and storing in Parquet format with atomic writes.

**Key Differences from CRSP/Compustat:**
- Data source: Ken French website (not WRDS)
- No point-in-time lag concerns (factors are published, not filings)
- Multiple datasets with different schemas (3-factor, 5-factor, industry portfolios)
- Both daily and monthly frequencies

---

## Architecture

### Storage Layout

```
data/fama_french/
├── factors/
│   ├── factors_3_daily.parquet    # 3-factor daily (Mkt-RF, SMB, HML, RF)
│   ├── factors_3_monthly.parquet  # 3-factor monthly
│   ├── factors_5_daily.parquet    # 5-factor daily (adds RMW, CMA)
│   ├── factors_5_monthly.parquet  # 5-factor monthly
│   ├── factors_6_daily.parquet    # 6-factor daily (5-factor + UMD) [NEW - Codex R2]
│   ├── factors_6_monthly.parquet  # 6-factor monthly [NEW - Codex R2]
│   ├── momentum_daily.parquet     # UMD (momentum) factor daily
│   └── momentum_monthly.parquet   # UMD (momentum) factor monthly
├── industries/
│   ├── ind10_daily.parquet        # 10-industry portfolio returns daily
│   ├── ind10_monthly.parquet      # 10-industry portfolio returns monthly
│   ├── ind30_daily.parquet        # 30-industry portfolio returns daily
│   ├── ind30_monthly.parquet      # 30-industry portfolio returns monthly
│   ├── ind49_daily.parquet        # 49-industry portfolio returns daily
│   └── ind49_monthly.parquet      # 49-industry portfolio returns monthly
├── quarantine/                    # [NEW - Codex R1] Corrupted/failed files
│   └── <timestamp>_<filename>/
└── manifests/
    └── fama_french.json           # Manifest with per-file entries
```

**6-Factor Model (Codex R2):**
The 6-factor model (ff6) IS stored as a materialized Parquet file to satisfy the "local Parquet storage for all factor datasets" deliverable:
- `factors_6_daily.parquet`: Join of 5-factor + momentum on date
- `factors_6_monthly.parquet`: Join of 5-factor + momentum on date

This ensures complete manifest coverage and consistent downstream consumption.

### Data Source

Ken French Data Library: `https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/data_library.html`

Using `pandas_datareader.data.DataReader` with `data_source='famafrench'`:

**Factor Data:**
- `F-F_Research_Data_Factors_daily` → 3-factor daily
- `F-F_Research_Data_Factors` → 3-factor monthly
- `F-F_Research_Data_5_Factors_2x3_daily` → 5-factor daily
- `F-F_Research_Data_5_Factors_2x3` → 5-factor monthly
- `F-F_Momentum_Factor_daily` → Momentum daily
- `F-F_Momentum_Factor` → Momentum monthly [NEW - Codex R1]

**Industry Portfolios (Daily):**
- `10_Industry_Portfolios_daily` → 10-industry daily
- `30_Industry_Portfolios_daily` → 30-industry daily
- `49_Industry_Portfolios_daily` → 49-industry daily

**Industry Portfolios (Monthly) [NEW - Codex R1]:**
- `10_Industry_Portfolios` → 10-industry monthly
- `30_Industry_Portfolios` → 30-industry monthly
- `49_Industry_Portfolios` → 49-industry monthly

---

## Schema Definitions

### 3-Factor Schema (factors_daily, factors_monthly)
```python
FF3_SCHEMA = {
    "date": pl.Date,
    "mkt_rf": pl.Float64,  # Market minus risk-free
    "smb": pl.Float64,     # Small minus big
    "hml": pl.Float64,     # High minus low (value)
    "rf": pl.Float64,      # Risk-free rate
}
```

### 5-Factor Schema (factors_5_daily, factors_5_monthly)
```python
FF5_SCHEMA = {
    "date": pl.Date,
    "mkt_rf": pl.Float64,
    "smb": pl.Float64,
    "hml": pl.Float64,
    "rmw": pl.Float64,     # Robust minus weak (profitability)
    "cma": pl.Float64,     # Conservative minus aggressive (investment)
    "rf": pl.Float64,
}
```

### Momentum Schema (momentum_daily)
```python
MOM_SCHEMA = {
    "date": pl.Date,
    "umd": pl.Float64,     # Up minus down (momentum)
}
```

### Industry Portfolio Schema (ind10/30/49)
```python
INDUSTRY_SCHEMA = {
    "date": pl.Date,
    # Dynamic columns: industry_1, industry_2, ..., industry_N
    # Or named columns depending on source (e.g., "NoDur", "Durbl", etc.)
}
```

### Return Normalization (CRITICAL - Codex R2)

**Problem:** Ken French data is published in **percent** (e.g., 1.5 means 1.5%), but downstream consumers expect **decimal** returns (e.g., 0.015).

**Solution:** Normalize all factor and industry returns during sync:

```python
def _normalize_returns(self, df: pl.DataFrame) -> pl.DataFrame:
    """Convert percent returns to decimal returns.

    Ken French publishes returns as percent (1.5 = 1.5%).
    We store as decimal (0.015) for downstream consistency.

    Args:
        df: DataFrame with percent returns.

    Returns:
        DataFrame with decimal returns (divided by 100).
    """
    # Identify return columns (all except 'date')
    return_cols = [c for c in df.columns if c != "date"]

    return df.with_columns([
        (pl.col(c) / 100.0).alias(c)
        for c in return_cols
    ])
```

**Validation:** Add tests to verify scale is correct:
- Factor values should typically be in range [-0.10, 0.10] for daily
- Factor values should typically be in range [-0.50, 0.50] for monthly

---

## Implementation Details

### Class: FamaFrenchLocalProvider

```python
class FamaFrenchLocalProvider:
    """Read-only provider for Fama-French factor data.

    Features:
    - Download from Ken French Data Library
    - Local Parquet caching with atomic writes
    - Support for 3-factor, 5-factor, 6-factor (with momentum)
    - Industry portfolio returns (10, 30, 49 industries)
    - Daily and monthly frequencies
    - Manifest-aware snapshot consistency
    """

    DATASET_FACTORS = "fama_french"
    QUARANTINE_DIR = Path("data/fama_french/quarantine")  # [NEW - Codex R1]

    # Ken French data source identifiers
    FF3_DAILY = "F-F_Research_Data_Factors_daily"
    FF3_MONTHLY = "F-F_Research_Data_Factors"
    FF5_DAILY = "F-F_Research_Data_5_Factors_2x3_daily"
    FF5_MONTHLY = "F-F_Research_Data_5_Factors_2x3"
    MOM_DAILY = "F-F_Momentum_Factor_daily"
    MOM_MONTHLY = "F-F_Momentum_Factor"  # [NEW - Codex R1]

    # Industry portfolios - daily
    IND10_DAILY = "10_Industry_Portfolios_daily"
    IND30_DAILY = "30_Industry_Portfolios_daily"
    IND49_DAILY = "49_Industry_Portfolios_daily"

    # Industry portfolios - monthly [NEW - Codex R1]
    IND10_MONTHLY = "10_Industry_Portfolios"
    IND30_MONTHLY = "30_Industry_Portfolios"
    IND49_MONTHLY = "49_Industry_Portfolios"
```

### Key Methods

1. **`get_factors()`** - Retrieve factor returns
   ```python
   def get_factors(
       self,
       start_date: date,
       end_date: date,
       model: Literal["ff3", "ff5", "ff6"] = "ff3",
       frequency: Literal["daily", "monthly"] = "daily",
   ) -> pl.DataFrame:
       """Get Fama-French factor returns.

       Args:
           start_date: Start of date range.
           end_date: End of date range.
           model: Factor model (ff3=3-factor, ff5=5-factor, ff6=6-factor with momentum).
           frequency: Data frequency.

       Returns:
           DataFrame with date and factor columns.
       """
   ```

2. **`get_industry_returns()`** - Retrieve industry portfolio returns
   ```python
   def get_industry_returns(
       self,
       start_date: date,
       end_date: date,
       num_industries: Literal[10, 30, 49] = 49,
       frequency: Literal["daily", "monthly"] = "daily",
   ) -> pl.DataFrame:
       """Get industry portfolio returns.

       Args:
           start_date: Start of date range.
           end_date: End of date range.
           num_industries: Industry classification (10, 30, or 49).
           frequency: Data frequency.

       Returns:
           DataFrame with date and industry return columns.
       """
   ```

3. **`sync_data()`** - Download/update data from Ken French
   ```python
   def sync_data(
       self,
       datasets: list[str] | None = None,
       force: bool = False,
   ) -> SyncManifest:
       """Sync data from Ken French Data Library.

       Uses atomic writes (temp + rename + checksum).

       Args:
           datasets: Specific datasets to sync (None = all).
           force: Force re-download even if data exists.

       Returns:
           Updated manifest.
       """
   ```

---

## Atomic Write Pattern with Quarantine [UPDATED - Codex R1]

Following T1.2/T1.3/T1.4 patterns with full quarantine flow:

```python
def _atomic_write_parquet(
    self,
    df: pl.DataFrame,
    target_path: Path,
    expected_checksum: str | None = None,
) -> str:
    """Write Parquet atomically using temp file + rename + quarantine.

    Pattern:
    1. Write to temp path: target.parquet.tmp
    2. Compute checksum of temp file
    3. Validate: row count > 0, checksum matches (if expected)
    4. On validation failure: move to quarantine, raise error
    5. Atomic rename: temp -> target
    6. fsync directory for crash safety
    7. Update manifest with per-file entry
    8. Return checksum

    Readers NEVER see .tmp files (atomic rename).
    """
    temp_path = target_path.with_suffix(".parquet.tmp")
    df.write_parquet(temp_path)

    actual_checksum = self._compute_checksum(temp_path)

    # Validate row count
    if df.height == 0:
        self._quarantine_file(temp_path, "empty_dataframe")
        raise ValueError("Empty DataFrame, file quarantined")

    # Validate checksum if expected
    if expected_checksum and actual_checksum != expected_checksum:
        self._quarantine_file(temp_path, "checksum_mismatch")
        raise ChecksumError(
            f"Checksum mismatch: expected {expected_checksum}, got {actual_checksum}"
        )

    # Atomic rename (readers never see .tmp)
    temp_path.rename(target_path)

    # fsync directory for crash safety
    self._fsync_directory(target_path.parent)

    return actual_checksum

def _quarantine_file(self, file_path: Path, reason: str) -> Path:
    """Move failed file to quarantine directory.

    Args:
        file_path: File to quarantine.
        reason: Reason for quarantine.

    Returns:
        Quarantine destination path.
    """
    self.QUARANTINE_DIR.mkdir(parents=True, exist_ok=True)
    timestamp = datetime.now(UTC).strftime("%Y%m%d_%H%M%S")
    dest = self.QUARANTINE_DIR / f"{timestamp}_{reason}_{file_path.name}"
    file_path.rename(dest)
    logger.warning(
        "File quarantined",
        extra={"source": str(file_path), "destination": str(dest), "reason": reason},
    )
    return dest
```

---

## Manifest Integration with Per-File Entries [UPDATED - Codex R1]

Use existing ManifestManager with per-file checksum entries for granular validation:

```python
# Manifest structure with per-file entries
{
    "dataset": "fama_french",
    "sync_timestamp": "2024-12-04T10:30:00Z",
    "schema_version": "v1.0.0",
    "files": {
        "factors_3_daily.parquet": {
            "checksum": "abc123...",
            "row_count": 25000,
            "start_date": "1926-07-01",
            "end_date": "2024-11-29"
        },
        "factors_3_monthly.parquet": {
            "checksum": "def456...",
            "row_count": 1200,
            "start_date": "1926-07-01",
            "end_date": "2024-11-30"
        },
        # ... per-file entries for all 12 datasets
    },
    "total_row_count": 150000,
    "combined_checksum": "xyz789..."  # XOR of all file checksums
}

# Sync completion flow
def _update_manifest_for_file(
    self,
    filename: str,
    checksum: str,
    row_count: int,
    date_range: tuple[date, date],
) -> None:
    """Update manifest with per-file entry under lock."""
    with self._lock:  # AtomicFileLock wraps entire sync
        manifest = self._load_manifest()
        manifest["files"][filename] = {
            "checksum": checksum,
            "row_count": row_count,
            "start_date": date_range[0].isoformat(),
            "end_date": date_range[1].isoformat(),
        }
        manifest["sync_timestamp"] = datetime.now(UTC).isoformat()
        self._atomic_write_manifest(manifest)
```

**Lock Scope (Codex R1 Critical):**
The entire sync operation (download + all file writes + manifest update) MUST be wrapped in `AtomicFileLock` to ensure consistency between data files and manifest.

---

## Test Cases (from P4T1_TASK.md)

### Required by Task Document
- [ ] 3-factor model download and storage (daily + monthly)
- [ ] 5-factor model download and storage (daily + monthly)
- [ ] Industry portfolios download (10, 30, 49 industries)
- [ ] Daily vs monthly factor data handling
- [ ] **[Codex R2]** Atomic write: interrupted write leaves no partial files
- [ ] **[Codex R2]** Checksum mismatch triggers quarantine

### Factor Model Tests
- [ ] 3-factor daily returns correct columns (mkt_rf, smb, hml, rf)
- [ ] 3-factor monthly returns correct columns
- [ ] 5-factor daily adds rmw, cma columns
- [ ] 5-factor monthly adds rmw, cma columns
- [ ] Momentum daily download and storage
- [ ] Momentum monthly download and storage
- [ ] **[NEW - Codex R2]** 6-factor daily stored as materialized Parquet (5-factor + UMD joined)
- [ ] **[NEW - Codex R2]** 6-factor monthly stored as materialized Parquet
- [ ] **[NEW - Codex R2]** 6-factor files have manifest entries with checksums

### Industry Portfolio Tests
- [ ] 10-industry daily download and storage
- [ ] **[NEW - Codex R1]** 10-industry monthly download and storage
- [ ] 30-industry daily download and storage
- [ ] **[NEW - Codex R1]** 30-industry monthly download and storage
- [ ] 49-industry daily download and storage
- [ ] **[NEW - Codex R1]** 49-industry monthly download and storage
- [ ] Industry column names are normalized consistently

### Atomic Write & Quarantine Tests [NEW - Codex R1]
- [ ] Temp files (.tmp) never visible to readers
- [ ] Interrupted write cleans up temp file
- [ ] Checksum mismatch moves file to quarantine
- [ ] Empty DataFrame moves file to quarantine
- [ ] Quarantine directory created if not exists
- [ ] Per-file manifest entry updated after successful write

### Manifest & Validation Tests
- [ ] Per-file checksum entries in manifest
- [ ] Manifest version consistency check
- [ ] Lock wraps entire sync operation
- [ ] Concurrent sync attempts blocked by lock

### Return Normalization Tests [NEW - Codex R2]
- [ ] Factor returns converted from percent to decimal (÷100)
- [ ] Industry returns converted from percent to decimal (÷100)
- [ ] Daily factor values in expected range [-0.10, 0.10]
- [ ] Monthly factor values in expected range [-0.50, 0.50]
- [ ] RF (risk-free rate) correctly normalized
- [ ] Normalization applied to all datasets (daily + monthly)

### Error Handling Tests
- [ ] Empty date range returns empty DataFrame
- [ ] Date filtering works correctly
- [ ] Path traversal attack prevention (storage_path validation)
- [ ] Network timeout handling (with retry)
- [ ] Invalid model/frequency raises ValueError
- [ ] Force sync overwrites existing data
- [ ] Incremental sync skips up-to-date datasets

---

## Files to Create

1. **`libs/data_providers/fama_french_local_provider.py`**
   - Main provider implementation
   - ~400-500 lines

2. **`scripts/fama_french_sync.py`**
   - CLI for syncing Fama-French data
   - Commands: `--sync`, `--status`, `--verify`

3. **`tests/libs/data_providers/test_fama_french_local_provider.py`**
   - Comprehensive test suite
   - ~20+ test cases

4. **`docs/CONCEPTS/fama-french-factors.md`**
   - Documentation explaining FF factors
   - Usage examples

---

## Files to Modify

1. **`requirements.txt`**
   - Add: `pandas-datareader>=0.10.0,<1.0.0`

2. **`libs/data_providers/__init__.py`**
   - Export FamaFrenchLocalProvider

3. **`docs/INDEX.md`**
   - Add fama-french-factors.md entry
   - Add plan file entry

---

## Dependencies

- **pandas-datareader**: For Ken French data download
- **polars**: DataFrame operations
- **ManifestManager**: From libs/data_quality (existing)
- **AtomicFileLock**: From libs/data_providers/locking (existing)

---

## Error Handling

1. **NetworkError**: Retry with exponential backoff (3 attempts)
2. **DataNotFoundError**: Dataset not available from Ken French
3. **SchemaError**: Unexpected schema from source
4. **ChecksumError**: Quarantine corrupted files

---

## Considerations

### Why Not Use WRDS for FF Data?
- Ken French website is the primary source
- WRDS FF data is a mirror, may have lag
- Direct download is simpler, no credentials needed
- pandas-datareader handles all the complexity

### Rate Limiting
- Ken French website is not rate-limited
- No special handling needed
- Add small delay between datasets to be polite

### Data Currency
- Ken French updates monthly
- Sync can run weekly/monthly (not daily like CRSP)
- Document expected update frequency

---

## Implementation Order

1. Add pandas-datareader to requirements.txt
2. Create FamaFrenchLocalProvider class skeleton
3. Implement sync_data() with atomic writes
4. Implement get_factors()
5. Implement get_industry_returns()
6. Create tests
7. Create CLI script
8. Create documentation
9. Update exports and INDEX.md

---

## Review Checklist

- [ ] Follows atomic write pattern from T1.2/T1.3/T1.4 with quarantine
- [ ] Uses ManifestManager with per-file entries
- [ ] Lock wraps entire sync operation (manifest + data atomicity)
- [ ] Temp files (.tmp) never visible to readers
- [ ] Path traversal prevention
- [ ] Proper error handling with retries
- [ ] All 14 datasets covered (8 factor + 6 industry, daily + monthly)
- [ ] 6-factor stored as materialized Parquet (not on-the-fly)
- [ ] Return normalization: percent → decimal (÷100)
- [ ] Comprehensive test coverage (~35 test cases)
- [ ] Documentation complete
- [ ] Exports updated

---

## Review Log

### Review 1 (2025-12-04)

**Gemini:** APPROVED
- Noted implementation must ensure lock wraps entire sync operation

**Codex:** CHANGES_REQUESTED
- Missing monthly momentum dataset → FIXED in v1.1
- Missing monthly industry datasets → FIXED in v1.1
- Atomic write lacks quarantine flow → FIXED in v1.1
- Single manifest needs per-file entries → FIXED in v1.1
- Missing test cases for monthly/quarantine → FIXED in v1.1

### Review 2 (2025-12-04)

**Gemini:** APPROVED

**Codex:** CHANGES_REQUESTED
- 6-factor not stored as Parquet (was computed on-the-fly) → FIXED in v1.2
  - Added factors_6_daily.parquet and factors_6_monthly.parquet to storage layout
- Return normalization missing (percent → decimal) → FIXED in v1.2
  - Added _normalize_returns() method to convert ÷100
  - Added normalization tests for all datasets
