"""
Custom Qlib data provider for T1 adjusted Parquet data.

This module bridges T1's adjusted OHLCV data with Qlib's expected format,
enabling seamless integration without data duplication or conversion pipelines.

See /docs/CONCEPTS/qlib-data-providers.md for detailed explanation.
"""

from pathlib import Path
from datetime import date
from typing import List, Optional

import polars as pl
import pandas as pd


class T1DataProvider:
    """
    Load adjusted market data from T1 pipeline output for Qlib.

    This provider reads Parquet files generated by the T1 ETL pipeline
    and converts them to the format expected by Qlib (Pandas DataFrame
    with MultiIndex).

    The provider handles:
    - Loading multiple symbols from Parquet files
    - Date range filtering
    - Format conversion (Polars → Pandas)
    - MultiIndex creation (date, symbol)
    - Column name standardization (lowercase)

    Attributes:
        data_dir: Path to directory containing adjusted Parquet files
                  (default: data/adjusted/)

    Example:
        >>> provider = T1DataProvider()
        >>> data = provider.load_data(
        ...     symbols=["AAPL", "MSFT"],
        ...     start_date=date(2024, 1, 1),
        ...     end_date=date(2024, 12, 31)
        ... )
        >>> print(data.index.names)
        ['date', 'symbol']
        >>> print(data.columns.tolist())
        ['open', 'high', 'low', 'close', 'volume']

    Notes:
        - Reads from latest date partition (most recent adjusted data)
        - Automatically handles corporate action adjustments from T1
        - Returns empty DataFrame if no data found (graceful failure)
        - Sorts data by (symbol, date) for time-series correctness

    See Also:
        - /docs/CONCEPTS/qlib-data-providers.md
        - /docs/IMPLEMENTATION_GUIDES/t1-data-etl.md
        - /docs/IMPLEMENTATION_GUIDES/t2-baseline-strategy-qlib.md
    """

    def __init__(self, data_dir: Path = Path("data/adjusted")) -> None:
        """
        Initialize T1 data provider.

        Args:
            data_dir: Directory containing adjusted Parquet files.
                      Expected structure: data_dir/YYYY-MM-DD/{SYMBOL}.parquet
        """
        self.data_dir = Path(data_dir)

    def load_data(
        self,
        symbols: List[str],
        start_date: date,
        end_date: date,
        fields: Optional[List[str]] = None,
    ) -> pd.DataFrame:
        """
        Load adjusted OHLCV data for given symbols and date range.

        This method:
        1. Scans all date partitions for symbol Parquet files
        2. Loads and concatenates data
        3. Filters to requested date range
        4. Converts to Pandas with proper MultiIndex

        Args:
            symbols: List of stock symbols to load (e.g., ["AAPL", "MSFT"])
            start_date: Start date (inclusive)
            end_date: End date (inclusive)
            fields: Optional list of columns to return. If None, returns
                    all OHLCV columns. Valid: ["open", "high", "low",
                    "close", "volume"]

        Returns:
            DataFrame with (date, symbol) MultiIndex and OHLCV columns.
            Returns empty DataFrame with correct structure if no data found.

        Raises:
            ValueError: If data_dir doesn't exist
            ValueError: If no valid fields provided

        Example:
            >>> provider = T1DataProvider()
            >>> df = provider.load_data(
            ...     symbols=["AAPL"],
            ...     start_date=date(2024, 1, 1),
            ...     end_date=date(2024, 1, 31),
            ...     fields=["close", "volume"]
            ... )
            >>> print(df.head())
                                close    volume
            date       symbol
            2024-01-01 AAPL    150.0  1000000.0
            2024-01-02 AAPL    151.0  1100000.0

        Notes:
            - Uses latest partition if symbol appears in multiple dates
            - Missing symbols return no rows (not an error)
            - Date filtering happens after loading (in-memory)
            - Corporate actions already applied by T1 pipeline

        Performance:
            - Typical load time: ~100ms for 3 symbols × 252 days
            - Memory usage: ~1MB per symbol per year of daily data
        """
        # Validate data directory exists
        if not self.data_dir.exists():
            raise ValueError(f"Data directory not found: {self.data_dir}")

        # Set default fields if not provided
        if fields is None:
            fields = ["open", "high", "low", "close", "volume"]

        # Validate fields
        valid_fields = {"open", "high", "low", "close", "volume"}
        if not set(fields).issubset(valid_fields):
            invalid = set(fields) - valid_fields
            raise ValueError(f"Invalid fields: {invalid}. Must be subset of {valid_fields}")

        # Step 1: Load data for each symbol from Parquet files
        dfs_to_concat = []

        for symbol in symbols:
            # Find all Parquet files for this symbol across date partitions
            # Pattern: data/adjusted/YYYY-MM-DD/SYMBOL.parquet
            parquet_files = list(self.data_dir.rglob(f"*/{symbol}.parquet"))

            if not parquet_files:
                # Symbol not found, skip (will be absent from result)
                continue

            # Use the most recent partition (last in sorted order)
            # This ensures we get the latest adjusted data
            latest_file = sorted(parquet_files)[-1]

            # Load Parquet file using Polars (fast)
            try:
                df = pl.read_parquet(latest_file)
                dfs_to_concat.append(df)
            except Exception as e:
                # Log and skip corrupted files
                print(f"Warning: Failed to load {latest_file}: {e}")
                continue

        # Handle case where no data found
        if not dfs_to_concat:
            # Return empty DataFrame with correct structure
            return self._empty_dataframe(fields)

        # Step 2: Concatenate all symbol data
        combined = pl.concat(dfs_to_concat)

        # Step 3: Filter to requested date range
        # Convert dates to Polars date type for comparison
        combined = combined.filter(
            (pl.col("date") >= pl.lit(start_date)) & (pl.col("date") <= pl.lit(end_date))
        )

        # Check if filtering removed all data
        if combined.is_empty():
            return self._empty_dataframe(fields)

        # Step 4: Convert to Pandas (Qlib requires Pandas)
        pandas_df: pd.DataFrame = combined.to_pandas()

        # Step 5: Standardize column names to lowercase
        # Qlib expects lowercase: open, high, low, close, volume
        pandas_df.columns = [col.lower() for col in pandas_df.columns]

        # Step 6: Sort by symbol and date for time-series correctness
        # This is critical for Qlib's sequential feature computation
        pandas_df = pandas_df.sort_values(["symbol", "date"])

        # Step 7: Set MultiIndex (date, symbol)
        # This is Qlib's expected format for time-series data
        pandas_df = pandas_df.set_index(["date", "symbol"])

        # Step 8: Select only requested fields
        # Also ensures we don't pass extra columns to Qlib
        available_fields = [f for f in fields if f in pandas_df.columns]
        pandas_df = pandas_df[available_fields]

        return pandas_df

    def _empty_dataframe(self, fields: List[str]) -> pd.DataFrame:
        """
        Create empty DataFrame with correct Qlib structure.

        Used when no data is found to maintain consistent return type.

        Args:
            fields: Column names to include

        Returns:
            Empty DataFrame with (date, symbol) MultiIndex and field columns
        """
        # Create empty DataFrame with specified columns
        df = pd.DataFrame(columns=fields)

        # Create empty MultiIndex
        # Qlib expects (date, symbol) even when empty
        empty_index = pd.MultiIndex.from_tuples([], names=["date", "symbol"])
        df.index = empty_index

        return df

    def get_available_symbols(self) -> List[str]:
        """
        Get list of all symbols available in data directory.

        Scans all date partitions and returns unique symbols.

        Returns:
            Sorted list of symbol strings

        Example:
            >>> provider = T1DataProvider()
            >>> symbols = provider.get_available_symbols()
            >>> print(symbols)
            ['AAPL', 'GOOGL', 'MSFT']

        Notes:
            - Scans all date partitions
            - Deduplicates symbols
            - Returns empty list if data_dir is empty
        """
        if not self.data_dir.exists():
            return []

        # Find all Parquet files
        parquet_files = list(self.data_dir.rglob("*.parquet"))

        # Extract symbol names from filenames
        # Filename format: {SYMBOL}.parquet
        symbols = {f.stem for f in parquet_files}

        # Return sorted list
        return sorted(symbols)

    def get_date_range(self, symbol: str) -> tuple[Optional[date], Optional[date]]:
        """
        Get available date range for a symbol.

        Args:
            symbol: Stock symbol (e.g., "AAPL")

        Returns:
            Tuple of (min_date, max_date) or (None, None) if symbol not found

        Example:
            >>> provider = T1DataProvider()
            >>> start, end = provider.get_date_range("AAPL")
            >>> print(f"AAPL data: {start} to {end}")
            AAPL data: 2024-01-01 to 2024-12-31

        Notes:
            - Checks all date partitions for symbol
            - Uses latest partition's date range
            - Returns None if symbol not found
        """
        parquet_files = list(self.data_dir.rglob(f"*/{symbol}.parquet"))

        if not parquet_files:
            return (None, None)

        # Use latest partition
        latest_file = sorted(parquet_files)[-1]

        try:
            # Load just the date column to get range
            df = pl.read_parquet(latest_file, columns=["date"])

            # Polars .min() and .max() return Any, need type narrowing
            min_val = df["date"].min()
            max_val = df["date"].max()

            # Type narrow to date for mypy strict compliance
            if not isinstance(min_val, date) or not isinstance(max_val, date):
                return (None, None)

            return (min_val, max_val)

        except Exception:
            return (None, None)
