# T2.7 Factor Attribution Analysis - Implementation Plan

**Task:** T2.7 Factor Attribution Analysis
**Effort:** 3-4 days
**Status:** Planning (v11 - Added currency_version to composite hash)
**Created:** 2025-12-08
**Updated:** 2025-12-08 (v3)

---

## Overview

Implement Fama-French factor attribution analysis with robust standard errors, rolling exposure tracking, data filters, and comprehensive diagnostics. This task leverages the existing FamaFrenchLocalProvider (T1.5) and DatasetVersionManager (T1.6) for PIT-correct analysis.

---

## Dependencies (All Complete)

| Dependency | Status | Location |
|------------|--------|----------|
| T2.1 Multi-Factor Model | Complete | `libs/factors/` |
| T1.5 Fama-French Provider | Complete | `libs/data_providers/fama_french_local_provider.py` |
| T1.6 DatasetVersionManager | Complete | `libs/data_quality/versioning.py` |

---

## Deliverables

### Core (Required)
1. Fama-French regression (3/5/6-factor)
2. Rolling factor exposure tracking (252-day window)
3. Performance attribution output with alpha, betas, R²
4. Robust standard errors (Newey-West default)
5. Multicollinearity check (VIF)
6. **Microcap filter** (exclude stocks below 20th percentile market cap or $100M)
7. **Currency filter** (USD-denominated only or specify currency)
8. Output schema for dashboard and registry (with schema versioning)
9. Integration with FamaFrenchLocalProvider
10. PIT compliance via DatasetVersionManager
11. >90% test coverage (measured via `pytest --cov`)

### Stretch (If Time Permits)
- Conditional attribution (up/down market regimes)
- Regime detector for market state classification

---

## Architecture

### Module Structure

**File naming aligned with P4T2 task spec:**
```
libs/analytics/
├── __init__.py        # Update exports
├── attribution.py     # NEW: Main implementation (per spec naming)
├── event_study.py     # Existing (reference patterns)
├── microstructure.py  # Existing
├── volatility.py      # Existing
└── execution_quality.py # Existing

tests/libs/analytics/
├── __init__.py
├── test_attribution.py  # NEW: Comprehensive tests (per spec naming)
└── test_event_study.py  # Existing
```

### Class Hierarchy

```python
# Configuration with filters
@dataclass(frozen=True)
class FactorAttributionConfig:
    """Configuration for factor attribution analysis."""
    model: Literal["ff3", "ff5", "ff6"] = "ff5"
    window_trading_days: int = 252  # TRADING days, not calendar days
    rebalance_freq: Literal["daily", "weekly", "monthly"] = "monthly"
    std_errors: Literal["ols", "hc3", "newey_west"] = "newey_west"
    newey_west_lags: int = 5
    min_observations: int = 60
    vif_threshold: float = 5.0
    annualization_factor: int = 252
    # Data filters (from P4T2 spec)
    min_market_cap_usd: float | None = 100_000_000  # $100M default, None=no filter
    market_cap_percentile: float | None = 0.20     # Exclude below 20th percentile
    currency: str | None = "USD"                   # USD-denominated only, None=no filter
    # Portfolio aggregation (when input has per-permno returns)
    aggregation_method: Literal["equal_weight", "value_weight"] = "equal_weight"
    rebalance_on_filter: bool = True  # Re-weight after filter removes stocks

# Result dataclass with schema versioning
@dataclass(frozen=True)
class AttributionResult:
    """Factor attribution output for dashboard and registry."""
    # Schema version for forward compatibility
    schema_version: str = "1.0.0"

    # Identification
    portfolio_id: str
    as_of_date: date
    dataset_version_id: str
    dataset_versions: dict[str, str]  # {"fama_french": "v1.2", "crsp": "v2.0"}
    snapshot_id: str | None  # DatasetVersionManager snapshot ID
    regression_config: dict

    # Core metrics
    alpha_annualized_bps: float
    alpha_t_stat: float
    alpha_p_value: float
    r_squared_adj: float
    residual_vol_annualized: float

    # Factor loadings
    betas: dict[str, float]
    beta_t_stats: dict[str, float]
    beta_p_values: dict[str, float]

    # Diagnostics
    n_observations: int
    multicollinearity_warnings: list[str]
    durbin_watson: float
    filter_stats: dict[str, int]  # {"total": 500, "after_microcap": 450, "after_currency": 450}

    # Reproducibility
    computation_timestamp: datetime

    # Serialization methods
    def to_registry_dict(self) -> dict[str, Any]: ...
    def to_dashboard_dict(self) -> dict[str, Any]: ...

# Rolling exposure result with schema versioning
@dataclass(frozen=True)
class RollingExposureResult:
    """Rolling factor exposure output."""
    schema_version: str = "1.0.0"
    portfolio_id: str
    exposures: pl.DataFrame  # [date, factor_name, beta, t_stat, p_value] - NaN for skipped
    skipped_windows: list[dict]  # [{"date": date, "n_obs": int, "reason": str}, ...]
    config: dict  # Serialized config
    dataset_version_id: str
    dataset_versions: dict[str, str]
    snapshot_id: str | None
    computation_timestamp: datetime

    def to_registry_dict(self) -> dict[str, Any]: ...
    def to_dashboard_dict(self) -> dict[str, Any]: ...

# Return decomposition result
@dataclass(frozen=True)
class ReturnDecompositionResult:
    """Return decomposition output."""
    schema_version: str = "1.0.0"
    portfolio_id: str
    decomposition: pl.DataFrame  # [date, portfolio_return, risk_free, excess_return, mkt_contrib, ...]
    attribution_result: AttributionResult
    dataset_version_id: str
    computation_timestamp: datetime

    def to_registry_dict(self) -> dict[str, Any]: ...
    def to_dashboard_dict(self) -> dict[str, Any]: ...

# Main analyzer class
class FactorAttribution:
    """Fama-French factor attribution with robust standard errors."""

    def __init__(
        self,
        ff_provider: FamaFrenchLocalProvider,
        crsp_provider: CRSPLocalProvider | None = None,  # For market cap + currency data
        version_manager: DatasetVersionManager | None = None,
        config: FactorAttributionConfig | None = None,
    ):
        """Initialize factor attribution analyzer.

        Args:
            ff_provider: Fama-French factor data provider (required)
            crsp_provider: CRSP data provider for market caps and currency (optional)
                          Required if microcap/currency filters are enabled.
            version_manager: Dataset version manager for PIT compliance (optional)
                            Required for reproducible backtests.
            config: Attribution configuration (uses defaults if None)
        """
        ...

    def fit(
        self,
        portfolio_returns: pl.DataFrame,
        start_date: date,
        end_date: date,
        portfolio_id: str = "portfolio",
        as_of_date: date | None = None,  # For PIT queries
        portfolio_version: str | None = None,  # Version ID if from registry, else content hash
        market_caps: pl.DataFrame | None = None,  # [date, permno, market_cap] override
        currencies: pl.DataFrame | None = None,   # [permno, currency] override
    ) -> AttributionResult:
        """Run factor attribution regression.

        Args:
            portfolio_returns: [date, return] for aggregated portfolio OR
                              [date, permno, return] for individual stock filtering
            start_date: Start of analysis period
            end_date: End of analysis period
            portfolio_id: Identifier for this portfolio
            as_of_date: Point-in-time date for PIT compliance (optional)
            portfolio_version: Version ID if portfolio from versioned registry;
                              if None, content hash computed for reproducibility
            market_caps: Market cap data override (if None, uses crsp_provider)
            currencies: Currency data override (if None, uses crsp_provider)

        Returns:
            AttributionResult with regression coefficients and diagnostics

        Raises:
            InsufficientObservationsError: If < min_observations after filtering
            DataMismatchError: If portfolio and factor dates don't overlap
            PITViolationError: If data extends beyond as_of_date
            ValueError: If filters enabled but no data source available
        """
        ...

    def compute_rolling_exposures(
        self,
        portfolio_returns: pl.DataFrame,
        start_date: date,
        end_date: date,
        portfolio_id: str = "portfolio",
        as_of_date: date | None = None,  # For PIT queries (same as fit)
        market_caps: pl.DataFrame | None = None,
        currencies: pl.DataFrame | None = None,
    ) -> RollingExposureResult:
        """Compute rolling factor exposures over time.

        Args:
            portfolio_returns: [date, return] or [date, permno, return]
            start_date: Start of rolling analysis
            end_date: End of rolling analysis
            portfolio_id: Identifier for this portfolio
            as_of_date: Point-in-time date for PIT compliance
            market_caps: Market cap data override
            currencies: Currency data override

        Returns:
            RollingExposureResult with exposures DataFrame

        Note:
            PIT compliance: If as_of_date provided:
            - Global bound: All windows end <= as_of_date (no future data)
            - Per-window bound: Window[i] uses data up to window_end[i]
            - Dataset versioning: ONE query at as_of_date, reused for all windows
              (simulates "what we would have known at as_of_date")
            - version_id/snapshot_id in result reflects the as_of_date snapshot

            PIT Window Rule (TRADING DAYS, not calendar):
            - window_end[i] = min(rebalance_date[i], as_of_date)
            - window_start[i] = trading_day_offset(window_end[i], -window_trading_days)
              (look back exactly 252 TRADING days from the trading calendar)
            - If window_end[i] > as_of_date → window SKIPPED (PIT violation)

            Trading Day Enforcement:
            - Use factor data dates as trading calendar (FF factors have trading days)
            - Window contains exactly `window_trading_days` observations (or fewer if insufficient data)
            - NOT calendar day subtraction: window_end - timedelta(days=252) is WRONG

        Insufficient Observations Handling:
            When a rolling window has fewer than min_observations after filters:
            - Output exposures DataFrame INCLUDES the date with NaN for all metrics
              (beta=NaN, t_stat=NaN, p_value=NaN)
            - A warning is logged with window date and observation count
            - The result includes a `skipped_windows: list[dict]` with details:
              [{"date": date, "n_obs": int, "reason": str}, ...]
            - If ALL windows are skipped, raises InsufficientObservationsError

            Rationale: Including NaN rows preserves time series alignment for
            downstream consumers (dashboards, charts) while clearly indicating
            missing data. The `skipped_windows` metadata provides debugging info.
        """
        ...

    def check_multicollinearity(
        self,
        ff_factors: pl.DataFrame,
    ) -> list[str]: ...

    def decompose_returns(
        self,
        portfolio_returns: pl.DataFrame,
        attribution_result: AttributionResult,
    ) -> ReturnDecompositionResult:
        """Decompose portfolio returns into factor contributions.

        Algorithm:
        For each date t:
            portfolio_return[t] = rf[t] + alpha + sum(beta_i * factor_i[t]) + residual[t]

        Where:
            - rf[t]: Risk-free rate for date t
            - alpha: Intercept from regression (daily, not annualized)
            - beta_i: Factor loading from regression
            - factor_i[t]: Factor return for date t (already excess returns)
            - residual[t]: portfolio_return[t] - (rf[t] + alpha + sum(beta_i * factor_i[t]))

        Output columns:
            - date: Trading date
            - portfolio_return: Raw portfolio return
            - risk_free: rf[t]
            - excess_return: portfolio_return - rf
            - alpha_contrib: alpha (constant, daily)
            - mkt_contrib: beta_mkt * mkt_rf[t]
            - smb_contrib: beta_smb * smb[t]
            - hml_contrib: beta_hml * hml[t]
            - (rmw_contrib, cma_contrib, umd_contrib for ff5/ff6)
            - residual: Unexplained return
            - total_factor_contrib: sum of all factor contributions

        Reconciliation check:
            excess_return ≈ alpha_contrib + total_factor_contrib + residual
            (within floating-point tolerance)

        Annualization:
            - alpha_contrib is DAILY (not annualized)
            - For annualized alpha, use attribution_result.alpha_annualized_bps
        """
        ...

    # Filter methods (private)
    def _apply_microcap_filter(
        self,
        portfolio_returns: pl.DataFrame,
        market_caps: pl.DataFrame,
    ) -> tuple[pl.DataFrame, dict[str, int]]: ...

    def _apply_currency_filter(
        self,
        portfolio_returns: pl.DataFrame,
        currencies: pl.DataFrame,
    ) -> tuple[pl.DataFrame, dict[str, int]]: ...

    def _get_filter_data(
        self,
        start_date: date,
        end_date: date,
        as_of_date: date | None,
    ) -> tuple[pl.DataFrame | None, pl.DataFrame | None]:
        """Get market cap and currency data from crsp_provider or version_manager.

        If as_of_date provided, queries PIT-correct data via version_manager.
        Returns (market_caps, currencies) DataFrames or None if no provider.
        """
        ...
```

---

## PIT/Versioning Integration (Detailed)

### Data Flow with PIT Compliance

**Key Principle:** PIT versioning applies to ALL data sources:
- Fama-French factors (via FamaFrenchLocalProvider)
- Portfolio universe/returns (via user input or DatasetVersionManager)
- CRSP market caps and currencies for filters (via CRSPLocalProvider)

```
1. User calls fit(portfolio_returns, start_date, end_date, as_of_date)
                    │
                    ▼
2. If version_manager provided AND as_of_date specified:
   ┌─────────────────────────────────────────────────────────────────┐
   │  # Query PIT snapshots for ALL datasets                        │
   │  ff_path, ff_manifest = version_manager.query_as_of(           │
   │      "fama_french", as_of_date)                                │
   │  crsp_path, crsp_manifest = version_manager.query_as_of(       │
   │      "crsp_daily", as_of_date)                                 │
   │                                                                 │
   │  ff_version_id = ff_manifest.datasets["fama_french"].version   │
   │  crsp_version_id = crsp_manifest.datasets["crsp_daily"].version│
   └─────────────────────────────────────────────────────────────────┘
                    │
                    ▼
3. Apply PIT universe filter to portfolio_returns:
   ┌─────────────────────────────────────────────────────────────────┐
   │  # If portfolio has permno column, filter to PIT universe      │
   │  if "permno" in portfolio_returns.columns:                     │
   │      pit_universe = crsp_provider.get_universe(                │
   │          as_of_date=as_of_date,  # Only stocks existing at PIT │
   │          data_path=crsp_path     # From versioned snapshot     │
   │      )                                                         │
   │      portfolio_returns = portfolio_returns.filter(             │
   │          pl.col("permno").is_in(pit_universe["permno"])       │
   │      )                                                         │
   │  # This prevents survivorship bias from future delistings      │
   └─────────────────────────────────────────────────────────────────┘
                    │
                    ▼
4. Query FF factors from SNAPSHOT-SCOPED provider (PIT-compliant):
   ┌─────────────────────────────────────────────────────────────────┐
   │  # CRITICAL: Instantiate NEW provider with snapshot path       │
   │  # Providers are initialized with storage_path, not per-call   │
   │  snapshot_ff_provider = FamaFrenchLocalProvider(               │
   │      storage_path=ff_path  # FROM VERSIONED SNAPSHOT (step 2)  │
   │  )                                                             │
   │  ff_factors = snapshot_ff_provider.get_factors(                │
   │      start_date, min(end_date, as_of_date),                    │
   │      model=config.model,                                       │
   │      frequency="daily"                                         │
   │  )                                                             │
   └─────────────────────────────────────────────────────────────────┘
                    │
                    ▼
5. Get filter data with PIT compliance:
   ┌─────────────────────────────────────────────────────────────────┐
   │  # CRSP doesn't have get_market_caps - calculate from prices   │
   │  # Instantiate snapshot-scoped CRSP provider                   │
   │  snapshot_crsp = CRSPLocalProvider(                            │
   │      storage_path=crsp_path,                                   │
   │      manifest_manager=ManifestManager(crsp_path)               │
   │  )                                                             │
   │  prices = snapshot_crsp.get_daily_prices(                      │
   │      start_date, min(end_date, as_of_date),                    │
   │      columns=["date", "permno", "prc", "shrout"],              │
   │      adjust_prices=True  # abs(prc)                            │
   │  )                                                             │
   │  # Calculate market cap: abs(price) * shares outstanding       │
   │  # CRITICAL: CRSP shrout is in THOUSANDS, must multiply by 1000│
   │  market_caps = prices.with_columns(                            │
   │      (pl.col("prc").abs() * pl.col("shrout") * 1000).alias("market_cap")│
   │  ).select(["date", "permno", "market_cap"])                    │
   │                                                                 │
   │  # CRSP is implicitly USD-denominated (US equities only)       │
   │  # Currency filter for CRSP data is a no-op; filter only       │
   │  # applies when portfolio contains non-CRSP securities.        │
   │  # Currency data must come from user input if filtering needed.│
   └─────────────────────────────────────────────────────────────────┘
                    │
                    ▼
6. Validate no look-ahead in ANY dataset:
   ┌─────────────────────────────────────────────────┐
   │  if any(ff_factors["date"] > as_of_date):      │
   │      raise PITViolationError("FF look-ahead")  │
   │  if any(portfolio_returns["date"] > as_of_date)│
   │      raise PITViolationError("Portfolio look-ahead")│
   │  if any(market_caps["date"] > as_of_date):     │
   │      raise PITViolationError("MarketCap look-ahead")│
   └─────────────────────────────────────────────────┘
                    │
                    ▼
7. Apply filters, run regression, build result:
   ┌─────────────────────────────────────────────────┐
   │  result = AttributionResult(                   │
   │      dataset_version_id=composite_hash,        │
   │      dataset_versions={                        │
   │          "fama_french": ff_version_id,         │
   │          "crsp_daily": crsp_version_id,        │
   │          "portfolio": portfolio_hash           │
   │      },                                        │
   │      snapshot_id=manifest.version_tag,         │
   │      ...                                       │
   │  )                                             │
   └─────────────────────────────────────────────────┘
```

### Version ID Construction

```python
def _build_dataset_version_id(
    self,
    ff_version: str,
    crsp_version: str | None,
    portfolio_version: str,  # Hash of portfolio input OR version from registry
    currency_version: str | None,  # Currency version if filter used
    config_hash: str,
) -> str:
    """Build composite version ID for reproducibility.

    Portfolio versioning:
    - If portfolio comes from a versioned registry/database, use that version
    - If portfolio is ad-hoc user input, compute SHA256 hash of contents

    Currency versioning:
    - Included when currency filter is enabled
    - None if filter disabled (currency_version="none" in hash)
    """
    crsp_part = f"crsp:{crsp_version}" if crsp_version else "crsp:none"
    currency_part = f"currency:{currency_version}" if currency_version else "currency:none"
    components = f"ff:{ff_version}|{crsp_part}|portfolio:{portfolio_version}|{currency_part}|config:{config_hash}"
    return hashlib.sha256(components.encode()).hexdigest()[:16]

def _compute_portfolio_version(
    self,
    portfolio_returns: pl.DataFrame,
    portfolio_version_hint: str | None = None,
) -> str:
    """Compute or retrieve portfolio version identifier.

    Args:
        portfolio_returns: Portfolio data
        portfolio_version_hint: If portfolio came from a versioned source,
                               pass the version ID here. If None, computes
                               content hash.

    Returns:
        Version identifier string
    """
    if portfolio_version_hint is not None:
        return portfolio_version_hint

    # Compute content hash for ad-hoc portfolios
    # Sort for determinism, then hash
    sorted_df = portfolio_returns.sort(["date"] + (
        ["permno"] if "permno" in portfolio_returns.columns else []
    ))
    content = sorted_df.to_pandas().to_csv(index=False)
    return hashlib.sha256(content.encode()).hexdigest()[:16]
```

**Portfolio PIT Handling:**
The portfolio input is versioned differently from FF/CRSP:
1. **Versioned portfolio** (from registry): Pass `portfolio_version` to `fit()` for tracking
2. **Ad-hoc portfolio** (user-provided): Content hash computed for reproducibility
3. **PIT constraint**: Portfolio dates are bounded by `as_of_date` same as other datasets
4. All version components included in `dataset_versions` dict in result

**Portfolio Snapshot Retrieval (for versioned portfolios):**
```python
# If user provides portfolio_version, they're responsible for loading correct snapshot
# This is documented in fit() Args - portfolio_version is the tracking ID, not a path
# For full PIT workflow, user should:
# 1. Query their portfolio registry for as_of_date snapshot
# 2. Pass that data + version ID to fit()

# For aggregated portfolios (no permno):
# - No survivorship filter possible; user must ensure pre-filtered
# - PIT validation: all dates <= as_of_date

# For per-permno portfolios:
# - Apply PIT universe filter via crsp_provider.get_universe()
# - Filter removes securities not existing at as_of_date
```

**Currency Data PIT Source:**
Currency data is NOT available from CRSP (US equities only, implicitly USD).
For portfolios with non-USD securities:

**Option 1: User-provided with explicit version (recommended for PIT):**
```python
# User queries their currency registry with as_of_date
currencies, currency_version = user_registry.get_currencies(as_of_date=as_of_date)

# Pass to fit() with version for tracking
result = attribution.fit(
    portfolio_returns=...,
    currencies=currencies,
    currency_version=currency_version,  # Tracked in dataset_versions
)
```

**Option 2: Content hash for ad-hoc (less rigorous PIT):**
```python
# User passes currencies without version; content hash computed
result = attribution.fit(
    portfolio_returns=...,
    currencies=currencies,  # No version → hash computed
)
```

**PIT Validation for currencies:**
```python
def _validate_currency_pit(
    self,
    currencies: pl.DataFrame,
    as_of_date: date | None,
) -> None:
    """Validate currency data respects as_of_date.

    Raises:
        PITViolationError: If currency data has dates beyond as_of_date
    """
    if as_of_date is None:
        return  # No PIT constraint

    if "effective_date" in currencies.columns:
        max_date = currencies["effective_date"].max()
        if max_date > as_of_date:
            raise PITViolationError(
                f"Currency data extends beyond as_of_date: {max_date} > {as_of_date}"
            )
```

**fit() signature update:**
```python
def fit(
    ...
    currencies: pl.DataFrame | None = None,
    currency_version: str | None = None,  # NEW: For PIT tracking
) -> AttributionResult:
```

**dataset_versions includes all sources:**
```python
dataset_versions = {
    "fama_french": ff_version,
    "crsp": crsp_version,
    "portfolio": portfolio_version,
    "currencies": currency_version or self._compute_content_hash(currencies),
}
```

### PIT Universe Enforcement

```python
def _apply_pit_universe_filter(
    self,
    portfolio_returns: pl.DataFrame,
    as_of_date: date,
) -> pl.DataFrame:
    """Filter portfolio to PIT-correct universe.

    Removes any securities that:
    - Were delisted before as_of_date
    - Did not exist (IPO after as_of_date)
    - Are not in the versioned CRSP snapshot

    This prevents survivorship bias from including/excluding
    securities based on future information.
    """
    if "permno" not in portfolio_returns.columns:
        # Aggregated portfolio, no filtering possible
        return portfolio_returns

    if self.version_manager is None or self.crsp_provider is None:
        logger.warning("PIT universe filter skipped: no version_manager or crsp_provider")
        return portfolio_returns

    # Get PIT universe from versioned snapshot
    crsp_path, manifest = self.version_manager.query_as_of("crsp_daily", as_of_date)
    pit_universe = self.crsp_provider.get_universe(
        as_of_date=as_of_date,
        data_path=crsp_path
    )

    original_count = portfolio_returns.n_unique("permno")
    filtered = portfolio_returns.filter(
        pl.col("permno").is_in(pit_universe["permno"])
    )
    filtered_count = filtered.n_unique("permno")

    if filtered_count < original_count:
        logger.info(
            "PIT universe filter applied",
            extra={
                "original_securities": original_count,
                "filtered_securities": filtered_count,
                "removed": original_count - filtered_count,
                "as_of_date": as_of_date.isoformat(),
            }
        )

    return filtered
```

---

## Portfolio Return Aggregation

**When `fit()` receives per-permno returns** (columns: `date`, `permno`, `return`), the filtered
returns must be aggregated to a portfolio return series for regression. The plan supports:

```python
@dataclass(frozen=True)
class FactorAttributionConfig:
    # ... existing fields ...
    # Portfolio aggregation method (when input has per-permno returns)
    aggregation_method: Literal["equal_weight", "value_weight"] = "equal_weight"
    rebalance_on_filter: bool = True  # Re-weight after filter removes stocks
```

**Aggregation Methods:**

1. **Equal Weight** (default):
   ```python
   # Daily cross-sectional average
   portfolio_return = filtered_returns.group_by("date").agg(
       pl.col("return").mean().alias("return")
   )
   ```

2. **Value Weight** (market cap weighted):
   ```python
   # Join market caps, compute weighted average
   # LEFT join to detect missing caps; then handle per policy
   weighted = filtered_returns.join(
       market_caps, on=["date", "permno"], how="left"
   )

   # Missing market cap policy:
   # - Default: forward-fill from last known cap (up to 5 days)
   # - If still null: drop that (date, permno) observation
   # - Log warning if > 5% of observations dropped
   weighted = weighted.with_columns(
       pl.col("market_cap").forward_fill().over("permno").alias("market_cap")
   ).filter(pl.col("market_cap").is_not_null())

   portfolio_return = weighted.group_by("date").agg(
       (pl.col("return") * pl.col("market_cap")).sum() / pl.col("market_cap").sum()
   ).rename({"literal": "return"})
   ```

**Missing Market Cap Handling:**
- Forward-fill last known cap per permno (max 5 trading days)
- If still missing after forward-fill, drop observation with warning
- If >5% of (date, permno) pairs dropped, log warning with count
- If ALL observations for a date dropped, that date excluded from regression

**Rebalance Behavior:**
- `rebalance_on_filter=True`: Weights recalculated after each filter application
- `rebalance_on_filter=False`: Original weights preserved, only filter masks applied

**Edge Cases:**
- If all stocks filtered on a date → that date is dropped from regression
- If < `min_observations` dates after aggregation → raise `InsufficientObservationsError`

---

## Data Filters Implementation

### Microcap Filter

**Filter Logic (INCLUSION rule):**
A stock is INCLUDED if and only if:
- Market Cap >= 20th percentile for that date, AND
- Market Cap >= $100M

Equivalently (EXCLUSION rule): Exclude if (market_cap < percentile) OR (market_cap < $100M).

```python
def _apply_microcap_filter(
    self,
    portfolio_returns: pl.DataFrame,
    market_caps: pl.DataFrame | None,
) -> tuple[pl.DataFrame, dict[str, int]]:
    """Filter out microcap stocks.

    Exclusion rule: Exclude if (market_cap < 20th percentile) OR (market_cap < $100M)
    Equivalently: Keep if (market_cap >= 20th percentile) AND (market_cap >= $100M)

    Args:
        portfolio_returns: [date, permno, return] for individual stocks
        market_caps: [date, permno, market_cap] market cap data

    Returns:
        Filtered returns and filter statistics.

    Raises:
        ValueError: If filter enabled but market_caps is None and crsp_provider
                   is not configured.
    """
    stats = {"total": len(portfolio_returns)}

    # Skip filter for aggregated portfolios (no permno)
    if "permno" not in portfolio_returns.columns:
        stats["after_microcap"] = stats["total"]
        stats["microcap_filter_applied"] = False
        stats["reason"] = "aggregated_portfolio"
        return portfolio_returns, stats

    # Require market cap data if filter is enabled
    if market_caps is None:
        if self.config.min_market_cap_usd is not None or self.config.market_cap_percentile is not None:
            raise ValueError(
                "Microcap filter enabled but no market_caps data provided. "
                "Either pass market_caps to fit() or configure crsp_provider."
            )
        stats["after_microcap"] = stats["total"]
        stats["microcap_filter_applied"] = False
        return portfolio_returns, stats

    # PIT-SAFE: Compute percentile PER DATE, not over entire sample
    # This prevents look-ahead bias from future market cap distribution
    if self.config.market_cap_percentile is not None:
        # Compute daily percentile threshold for each date
        daily_thresholds = market_caps.group_by("date").agg(
            pl.col("market_cap").quantile(self.config.market_cap_percentile).alias("percentile_threshold")
        )
        # Join threshold back to market_caps
        market_caps_with_threshold = market_caps.join(daily_thresholds, on="date")
        # Apply percentile filter (per-date)
        market_caps_filtered = market_caps_with_threshold.filter(
            pl.col("market_cap") >= pl.col("percentile_threshold")
        )
        stats["percentile_filter_applied"] = True
        stats["percentile_value"] = self.config.market_cap_percentile
    else:
        market_caps_filtered = market_caps
        stats["percentile_filter_applied"] = False

    # Apply absolute threshold (static, same for all dates)
    if self.config.min_market_cap_usd is not None:
        market_caps_filtered = market_caps_filtered.filter(
            pl.col("market_cap") >= self.config.min_market_cap_usd
        )
        stats["absolute_threshold"] = self.config.min_market_cap_usd
        stats["absolute_filter_applied"] = True
    else:
        stats["absolute_filter_applied"] = False

    # Join filtered market caps back to portfolio returns
    filtered = portfolio_returns.join(
        market_caps_filtered.select(["date", "permno"]),
        on=["date", "permno"],
        how="inner"
    )

    stats["after_microcap"] = len(filtered)
    stats["microcap_filter_applied"] = True
    stats["removed"] = stats["total"] - stats["after_microcap"]
    return filtered, stats
```

### Currency Filter

```python
def _apply_currency_filter(
    self,
    portfolio_returns: pl.DataFrame,
    currencies: pl.DataFrame | None,
) -> tuple[pl.DataFrame, dict[str, int]]:
    """Filter to specified currency-denominated securities only.

    Args:
        portfolio_returns: [date, permno, return] for individual stocks
        currencies: [permno, currency] mapping

    Returns:
        Filtered returns and filter statistics.

    Raises:
        ValueError: If currency filter enabled but currencies is None and
                   crsp_provider is not configured.
    """
    stats = {}

    # Skip filter if disabled in config
    if self.config.currency is None:
        stats["after_currency"] = len(portfolio_returns)
        stats["currency_filter_applied"] = False
        stats["reason"] = "filter_disabled"
        return portfolio_returns, stats

    # Skip filter for aggregated portfolios (no permno)
    if "permno" not in portfolio_returns.columns:
        stats["after_currency"] = len(portfolio_returns)
        stats["currency_filter_applied"] = False
        stats["reason"] = "aggregated_portfolio"
        return portfolio_returns, stats

    # Require currency data - fail fast if not available
    # Do NOT silently assume USD; require explicit currency verification
    if currencies is None:
        raise ValueError(
            f"Currency filter for '{self.config.currency}' enabled but no "
            "currencies data provided. Either pass currencies to fit(), "
            "configure crsp_provider, or set config.currency=None to disable filter."
        )

    filtered = portfolio_returns.join(
        currencies.filter(pl.col("currency") == self.config.currency),
        on="permno",
        how="inner"
    )
    stats["after_currency"] = len(filtered)
    stats["currency_filter_applied"] = True
    stats["removed"] = len(portfolio_returns) - stats["after_currency"]
    return filtered, stats
```

---

## Output Schema Specifications

### AttributionResult JSON Schema (v1.0.0)

```json
{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "type": "object",
  "required": ["schema_version", "portfolio_id", "as_of_date", "alpha_annualized_bps", "betas"],
  "properties": {
    "schema_version": {"type": "string", "pattern": "^\\d+\\.\\d+\\.\\d+$"},
    "portfolio_id": {"type": "string"},
    "as_of_date": {"type": "string", "format": "date"},
    "dataset_version_id": {"type": "string"},
    "dataset_versions": {
      "type": "object",
      "additionalProperties": {"type": "string"}
    },
    "snapshot_id": {"type": ["string", "null"]},
    "regression_config": {
      "type": "object",
      "properties": {
        "model": {"enum": ["ff3", "ff5", "ff6"]},
        "window_days": {"type": "integer"},
        "std_errors": {"enum": ["ols", "hc3", "newey_west"]},
        "newey_west_lags": {"type": "integer"}
      }
    },
    "alpha_annualized_bps": {"type": "number"},
    "alpha_t_stat": {"type": "number"},
    "alpha_p_value": {"type": "number"},
    "r_squared_adj": {"type": "number", "minimum": 0, "maximum": 1},
    "residual_vol_annualized": {"type": "number", "minimum": 0},
    "betas": {
      "type": "object",
      "additionalProperties": {"type": "number"}
    },
    "beta_t_stats": {
      "type": "object",
      "additionalProperties": {"type": "number"}
    },
    "beta_p_values": {
      "type": "object",
      "additionalProperties": {"type": "number", "minimum": 0, "maximum": 1}
    },
    "n_observations": {"type": "integer", "minimum": 1},
    "multicollinearity_warnings": {
      "type": "array",
      "items": {"type": "string"}
    },
    "durbin_watson": {"type": "number", "minimum": 0, "maximum": 4},
    "filter_stats": {
      "type": "object",
      "properties": {
        "total": {"type": "integer"},
        "after_microcap": {"type": "integer"},
        "after_currency": {"type": "integer"}
      }
    },
    "computation_timestamp": {"type": "string", "format": "date-time"}
  }
}
```

### RollingExposureResult JSON Schema (v1.0.0)

```json
{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "type": "object",
  "required": ["schema_version", "portfolio_id", "exposures", "dataset_version_id"],
  "properties": {
    "schema_version": {"type": "string", "pattern": "^\\d+\\.\\d+\\.\\d+$"},
    "portfolio_id": {"type": "string"},
    "exposures": {
      "type": "array",
      "items": {
        "type": "object",
        "properties": {
          "date": {"type": "string", "format": "date"},
          "factor_name": {"type": "string"},
          "beta": {"type": ["number", "null"]},
          "t_stat": {"type": ["number", "null"]},
          "p_value": {"type": ["number", "null"]}
        }
      }
    },
    "skipped_windows": {
      "type": "array",
      "items": {
        "type": "object",
        "properties": {
          "date": {"type": "string", "format": "date"},
          "n_obs": {"type": "integer"},
          "reason": {"type": "string"}
        }
      }
    },
    "config": {"type": "object"},
    "dataset_version_id": {"type": "string"},
    "dataset_versions": {
      "type": "object",
      "additionalProperties": {"type": "string"}
    },
    "snapshot_id": {"type": ["string", "null"]},
    "computation_timestamp": {"type": "string", "format": "date-time"}
  }
}
```

---

## Serialization Implementation

**Polars DataFrame to JSON Conversion:**
```python
def to_registry_dict(self) -> dict[str, Any]:
    """Serialize result for registry storage.

    Converts Polars DataFrames to list of dicts, handles NaN→null,
    ensures deterministic ordering for reproducibility.
    """
    exposures_list = None
    if hasattr(self, "exposures") and self.exposures is not None:
        # Convert DataFrame to records, replace NaN with None
        exposures_list = self.exposures.to_dicts()
        for row in exposures_list:
            for k, v in row.items():
                if isinstance(v, float) and (v != v):  # NaN check
                    row[k] = None

    return {
        "schema_version": self.schema_version,
        "portfolio_id": self.portfolio_id,
        "as_of_date": self.as_of_date.isoformat() if hasattr(self, "as_of_date") else None,
        "dataset_version_id": self.dataset_version_id,
        "dataset_versions": self.dataset_versions,
        "snapshot_id": self.snapshot_id,
        "alpha_annualized_bps": self._nan_to_none(getattr(self, "alpha_annualized_bps", None)),
        "betas": {k: self._nan_to_none(v) for k, v in getattr(self, "betas", {}).items()},
        # ... other fields ...
        "exposures": exposures_list,
        "skipped_windows": getattr(self, "skipped_windows", []),
        "computation_timestamp": self.computation_timestamp.isoformat(),
    }

@staticmethod
def _nan_to_none(val: float | None) -> float | None:
    """Convert NaN to None for JSON serialization."""
    if val is None:
        return None
    if isinstance(val, float) and val != val:  # NaN check
        return None
    return val

def to_dashboard_dict(self) -> dict[str, Any]:
    """Serialize result for dashboard display.

    Same as registry but may include additional computed fields
    or formatting for UI consumption.
    """
    base = self.to_registry_dict()
    # Add display-friendly fields if needed
    if hasattr(self, "alpha_annualized_bps") and self.alpha_annualized_bps is not None:
        base["alpha_display"] = f"{self.alpha_annualized_bps:.1f} bps"
    return base
```

**Deterministic Ordering:**
- Dict keys sorted alphabetically for reproducibility
- DataFrame rows sorted by date before serialization
- Use `json.dumps(data, sort_keys=True)` for final output

---

## Implementation Details

### 1. Fama-French Regression

**Factor Column Specifications (from FamaFrenchLocalProvider):**

| Model | Required Columns | Description |
|-------|-----------------|-------------|
| **FF3** | `date`, `mkt_rf`, `smb`, `hml`, `rf` | 3-factor model |
| **FF5** | `date`, `mkt_rf`, `smb`, `hml`, `rmw`, `cma`, `rf` | 5-factor model |
| **FF6** | `date`, `mkt_rf`, `smb`, `hml`, `rmw`, `cma`, `umd`, `rf` | 6-factor model (momentum) |

**Column Name Constants:**
```python
# Factor column constants (from FamaFrenchLocalProvider)
FF3_FACTOR_COLS = ("mkt_rf", "smb", "hml")
FF5_FACTOR_COLS = ("mkt_rf", "smb", "hml", "rmw", "cma")
FF6_FACTOR_COLS = ("mkt_rf", "smb", "hml", "rmw", "cma", "umd")
RISK_FREE_COL = "rf"

# Map model to factor columns
FACTOR_COLS_BY_MODEL = {
    "ff3": FF3_FACTOR_COLS,
    "ff5": FF5_FACTOR_COLS,
    "ff6": FF6_FACTOR_COLS,
}
```

**Excess Return Construction:**
```python
# Portfolio excess return = portfolio return - risk-free rate
# Factors from provider are already excess returns (mkt_rf = mkt - rf)
excess_returns = portfolio_returns["return"] - ff_factors["rf"]
```

**Regression Equation:**
- FF3: `R_p - R_f = alpha + beta_mkt * MKT_RF + beta_smb * SMB + beta_hml * HML + epsilon`
- FF5: FF3 + `beta_rmw * RMW + beta_cma * CMA`
- FF6: FF5 + `beta_umd * UMD`

**Standard Error Options:**
- `ols`: Homoskedastic standard errors (fastest, assumes constant variance)
- `hc3`: Heteroskedasticity-consistent (White, 1980) - robust to varying variance
- `newey_west`: HAC standard errors (Newey-West, 1987) - robust to heteroskedasticity AND autocorrelation

**Regression Implementation (statsmodels):**
```python
import statsmodels.api as sm

def _run_regression(
    self,
    excess_returns: np.ndarray,  # Portfolio excess returns (T,)
    factors: np.ndarray,          # Factor returns (T, K)
    factor_names: tuple[str, ...],
) -> tuple[RegressionResult, dict]:
    """Run OLS regression with specified standard errors.

    Uses statsmodels OLS with cov_type parameter for robust SEs.

    Returns:
        (model_result, diagnostics_dict)
    """
    # Add constant for intercept (alpha)
    X = sm.add_constant(factors)

    # Fit OLS model
    model = sm.OLS(excess_returns, X)

    # Choose covariance estimator
    if self.config.std_errors == "ols":
        result = model.fit()
    elif self.config.std_errors == "hc3":
        result = model.fit(cov_type="HC3")
    else:  # newey_west
        # Newey-West HAC: cov_type="HAC", cov_kwds={"maxlags": lags}
        # RESPECT config override; fall back to rule-of-thumb if not set
        if self.config.newey_west_lags > 0:
            nw_lags = self.config.newey_west_lags  # User-specified
        else:
            nw_lags = self._compute_nw_lags(len(excess_returns))  # Auto
        result = model.fit(
            cov_type="HAC",
            cov_kwds={"maxlags": nw_lags, "use_correction": True}
        )

    # Extract results
    alpha = result.params[0]  # Intercept
    betas = dict(zip(factor_names, result.params[1:]))
    t_stats = dict(zip(["alpha"] + list(factor_names), result.tvalues))
    p_values = dict(zip(["alpha"] + list(factor_names), result.pvalues))

    diagnostics = {
        "r_squared_adj": result.rsquared_adj,
        "durbin_watson": sm.stats.durbin_watson(result.resid),
        "residual_std": result.resid.std(),
        "n_observations": result.nobs,
    }

    return result, diagnostics
```

### 2. Newey-West Lag Selection

**Rule of thumb:** `lags = floor(4 * (T/100)^(2/9))` where T = sample size

**Edge cases:**
- T < 60: Warning, NW may be unreliable
- T > 1000: Cap lags at 10 to avoid over-fitting
- Config override: Always respect `config.newey_west_lags` if > 0

**Frequency-aware lag policy:**
- Daily data (default): Use rule-of-thumb `floor(4 * (T/100)^(2/9))`
- Weekly rebalance: Consider ~1/5 of daily lags (autocorrelation decays faster)
- Monthly rebalance: Typically 1-3 lags sufficient
- Note: Current implementation uses sample-size rule; frequency adjustment is
  optional enhancement. Users can explicitly set lags via config if needed.

### 3. VIF Check with Singular Matrix Handling

**Multicollinearity Detection:**
```python
def check_multicollinearity(
    self,
    ff_factors: pl.DataFrame,
) -> list[str]:
    """Check for multicollinearity using Variance Inflation Factor (VIF).

    Returns:
        List of warning messages for factors exceeding VIF threshold.

    Raises:
        MulticollinearityError: If matrix is perfectly singular (cannot compute VIF).

    Note:
        For near-singular matrices (condition number > 1e10), VIF computation
        may be numerically unstable. In this case, we skip VIF and return
        a warning about potential multicollinearity issues.
    """
    factor_cols = FACTOR_COLS_BY_MODEL[self.config.model]
    X = ff_factors.select(list(factor_cols)).to_numpy()

    warnings = []

    # Check condition number first for numerical stability
    cond_number = np.linalg.cond(X)
    if cond_number > 1e10:
        warnings.append(
            f"Factor matrix near-singular (condition number: {cond_number:.2e}). "
            "VIF computation skipped. Review factor data for constant columns or identical factors."
        )
        return warnings

    # Compute VIF for each factor
    try:
        for i, factor_name in enumerate(factor_cols):
            # VIF_i = 1 / (1 - R²_i)
            # where R²_i is from regressing X_i on all other X_j
            X_subset = np.delete(X, i, axis=1)
            y = X[:, i]

            # Add constant for regression
            X_with_const = np.column_stack([np.ones(len(y)), X_subset])

            # Check if this subset is also singular
            subset_cond = np.linalg.cond(X_with_const)
            if subset_cond > 1e10:
                warnings.append(
                    f"Factor '{factor_name}': VIF computation skipped (near-singular subset)"
                )
                continue

            # OLS: beta = (X'X)^-1 X'y
            try:
                beta = np.linalg.lstsq(X_with_const, y, rcond=None)[0]
                y_pred = X_with_const @ beta
                ss_res = np.sum((y - y_pred) ** 2)
                ss_tot = np.sum((y - np.mean(y)) ** 2)

                if ss_tot < 1e-10:
                    # Constant column - perfect multicollinearity
                    warnings.append(
                        f"Factor '{factor_name}': Constant column detected (VIF=∞). "
                        "Remove this factor from the model."
                    )
                    continue

                r_squared = 1 - (ss_res / ss_tot)
                r_squared = min(r_squared, 0.9999999)  # Cap to avoid div by zero

                vif = 1 / (1 - r_squared)

                if vif > self.config.vif_threshold:
                    warnings.append(
                        f"Factor '{factor_name}': VIF={vif:.2f} exceeds threshold "
                        f"({self.config.vif_threshold}). High multicollinearity detected."
                    )

            except np.linalg.LinAlgError as e:
                warnings.append(
                    f"Factor '{factor_name}': VIF computation failed ({e}). "
                    "Check for linear dependencies in factor data."
                )

    except Exception as e:
        warnings.append(
            f"VIF computation encountered error: {e}. "
            "Multicollinearity status unknown - review factor data manually."
        )

    return warnings
```

**Key Guardrails:**
1. **Condition number check**: Before VIF, check if matrix is near-singular (`cond > 1e10`)
2. **Constant column detection**: Check `ss_tot < 1e-10` for constant factors
3. **R² capping**: Cap R² at 0.9999999 to prevent division by zero
4. **Try/except**: Graceful degradation with warning instead of crash
5. **Subset singularity**: Check each factor subset for singularity before regression

### 4. Rolling Rebalance Calendar

**End-of-Month Logic:**
```python
def _get_rebalance_dates(
    self,
    start_date: date,
    end_date: date,
    trading_calendar: pl.DataFrame,  # [date] of trading days
) -> list[date]:
    """Get rebalance dates based on frequency."""
    if self.config.rebalance_freq == "daily":
        return trading_calendar.filter(
            (pl.col("date") >= start_date) & (pl.col("date") <= end_date)
        )["date"].to_list()

    elif self.config.rebalance_freq == "weekly":
        # Last trading day of each week
        return trading_calendar.filter(
            (pl.col("date") >= start_date) & (pl.col("date") <= end_date)
        ).with_columns(
            pl.col("date").dt.week().alias("week"),
            pl.col("date").dt.year().alias("year")
        ).group_by(["year", "week"]).agg(
            pl.col("date").max()
        )["date"].sort().to_list()

    else:  # monthly
        # Last trading day of each month
        return trading_calendar.filter(
            (pl.col("date") >= start_date) & (pl.col("date") <= end_date)
        ).with_columns(
            pl.col("date").dt.month().alias("month"),
            pl.col("date").dt.year().alias("year")
        ).group_by(["year", "month"]).agg(
            pl.col("date").max()
        )["date"].sort().to_list()
```

---

## Error Handling

### Exceptions

```python
class FactorAttributionError(AlphaResearchError):
    """Base exception for factor attribution."""
    pass

class InsufficientObservationsError(FactorAttributionError):
    """Raised when n_observations < min_observations."""
    pass

class DataMismatchError(FactorAttributionError):
    """Raised when portfolio dates don't overlap with factor dates."""
    pass

class PITViolationError(FactorAttributionError):
    """Raised when data extends beyond as_of_date (look-ahead bias)."""
    pass
```

### Validation Rules
1. `n_observations >= min_observations` (default 60)
2. Portfolio returns in decimal form (|return| < 1 in most cases)
3. Date ranges must overlap between portfolio and factors
4. No NaN/Inf values in returns or factors
5. PIT: No factor data beyond `as_of_date`

### Date Alignment Contract

**Deterministic alignment of portfolio returns and factor rows:**
```python
def _align_data(
    self,
    portfolio_returns: pl.DataFrame,  # [date, return] aggregated
    ff_factors: pl.DataFrame,          # [date, mkt_rf, smb, ...]
) -> tuple[pl.DataFrame, pl.DataFrame]:
    """Align portfolio and factor data on dates.

    Steps:
    1. Inner join on date column (only keep matching dates)
    2. Sort by date ascending (deterministic order)
    3. Remove duplicate dates (keep first if any)
    4. Validate lengths match

    Returns:
        (aligned_portfolio, aligned_factors) with matching row order

    Raises:
        DataMismatchError: If inner join produces empty result
        ValueError: If duplicate dates found after join
    """
    # Inner join - only overlapping dates
    aligned = portfolio_returns.join(
        ff_factors,
        on="date",
        how="inner"
    )

    if aligned.height == 0:
        raise DataMismatchError(
            f"No overlapping dates between portfolio "
            f"({portfolio_returns['date'].min()} to {portfolio_returns['date'].max()}) "
            f"and factors ({ff_factors['date'].min()} to {ff_factors['date'].max()})"
        )

    # Check for duplicates
    if aligned.n_unique("date") != aligned.height:
        duplicates = aligned.group_by("date").count().filter(pl.col("count") > 1)
        raise ValueError(
            f"Duplicate dates found after alignment: {duplicates['date'].to_list()[:5]}"
        )

    # Sort for deterministic order
    aligned = aligned.sort("date")

    # Split back into portfolio and factor DataFrames
    aligned_portfolio = aligned.select(["date", "return"])
    factor_cols = [c for c in aligned.columns if c not in ["date", "return"]]
    aligned_factors = aligned.select(["date"] + factor_cols)

    return aligned_portfolio, aligned_factors
```

**Alignment is applied:**
1. After portfolio aggregation (if per-permno input)
2. After all filters applied
3. Before regression

### Date Overlap Validation (Concrete Steps)
```python
def _validate_date_overlap(
    self,
    portfolio_dates: set[date],
    factor_dates: set[date],
) -> tuple[set[date], dict]:
    """Validate portfolio and factor date overlap.

    Returns:
        (overlapping_dates, validation_stats)

    Raises:
        DataMismatchError: If no overlap or insufficient overlap.
    """
    overlap = portfolio_dates & factor_dates
    portfolio_only = portfolio_dates - factor_dates
    factor_only = factor_dates - portfolio_dates

    stats = {
        "portfolio_dates": len(portfolio_dates),
        "factor_dates": len(factor_dates),
        "overlap_dates": len(overlap),
        "portfolio_only": len(portfolio_only),
        "factor_only": len(factor_only),
    }

    if len(overlap) == 0:
        raise DataMismatchError(
            f"No date overlap between portfolio ({min(portfolio_dates)} to "
            f"{max(portfolio_dates)}) and factors ({min(factor_dates)} to "
            f"{max(factor_dates)})"
        )

    if len(overlap) < self.config.min_observations:
        raise InsufficientObservationsError(
            f"Only {len(overlap)} overlapping dates, need {self.config.min_observations}"
        )

    if portfolio_only:
        logger.warning(
            f"Portfolio has {len(portfolio_only)} dates with no factor data; "
            f"range: {min(portfolio_only)} to {max(portfolio_only)}"
        )

    return overlap, stats

def _validate_no_nan(
    self,
    data: pl.DataFrame,
    columns: list[str],
    source: str,
) -> None:
    """Validate no NaN/Inf values in specified columns.

    Raises:
        ValueError: If NaN or Inf found.
    """
    for col in columns:
        null_count = data[col].null_count()
        if null_count > 0:
            raise ValueError(
                f"{source} has {null_count} null values in column '{col}'"
            )
        # Check for Inf
        inf_count = data.filter(pl.col(col).is_infinite()).height
        if inf_count > 0:
            raise ValueError(
                f"{source} has {inf_count} infinite values in column '{col}'"
            )
```

---

## Test Plan (Expanded)

### Unit Tests (~55 tests)

| Category | Tests | Description |
|----------|-------|-------------|
| **Config Validation** | 5 | Invalid params, bounds, frozen dataclass |
| **Regression** | 12 | FF3/FF5/FF6, all std_error types, edge cases |
| **Rolling Exposure** | 10 | Daily/weekly/monthly, EOM calendar, gap handling |
| **Multicollinearity** | 10 | VIF calculation, warnings, threshold edge cases, singular matrix handling |
| **Diagnostics** | 5 | DW test, R² adj, residuals |
| **Return Decomposition** | 5 | Contributions sum correctly |
| **Filters** | 12 | Microcap filter, currency filter, combined, PIT currency versioning |
| **PIT/Versioning** | 8 | Version tracking, PITViolation, snapshot linkage |
| **Error Handling** | 8 | InsufficientObs, DataMismatch, missing overlap |
| **Serialization** | 8 | to_registry_dict, to_dashboard_dict, schema validation |

### Integration Tests (~12 tests)
- FamaFrenchLocalProvider integration
- DatasetVersionManager PIT queries (positive and negative tests)
- Full pipeline with real data fixtures
- Version mismatch detection
- Rolling exposure with PIT boundaries

### Edge Case Tests
- **NW lag sensitivity**: Compare different lag values
- **Missing date overlap**: Portfolio and factors have gaps
- **VIF threshold edge**: Exact boundary values
- **Zero observations after filter**: Should raise InsufficientObservationsError
- **VIF singular matrix**: Identical factors (e.g., SMB = HML) returns warning, not crash
- **VIF constant column**: Factor with zero variance returns warning about removal
- **VIF near-singular**: High condition number (>1e10) skips VIF with warning
- **VIF subset singularity**: One factor perfectly predicts another
- **NW SE correctness**: Compare output with statsmodels reference (known data)
- **252 trading days enforcement**: Verify window uses trading days not calendar days
- **Value-weight missing caps**: Forward-fill and drop behavior
- **Serialization round-trip**: to_registry_dict() output validates against JSON schema
- **NaN in output**: Verify NaN→null in serialized output
- **NW lag override**: config.newey_west_lags=10 used instead of auto-computed
- **NW lag auto**: config.newey_west_lags=0 triggers rule-of-thumb computation
- **Currency filter PIT**: Currency data with as_of_date filtering
- **Currency version tracking**: currency_version in dataset_versions output
- **Mixed currency portfolio**: Filter retains only matching currency
- **Missing currency data**: ValueError when filter enabled but no data
- **Date alignment empty**: DataMismatchError when no overlap
- **Date alignment duplicates**: ValueError on duplicate dates

### Coverage Measurement
```bash
pytest tests/libs/analytics/test_attribution.py --cov=libs/analytics/attribution --cov-report=term-missing --cov-fail-under=90
```

---

## Acceptance Criteria Checklist

### Core (Required)
- [ ] Fama-French 3/5/6-factor regression
- [ ] Rolling factor exposure (252-day window)
- [ ] Performance attribution output with alpha, betas, R²
- [ ] Robust standard errors (Newey-West default)
- [ ] Multicollinearity check (VIF)
- [ ] **Microcap filter** (20th percentile or $100M)
- [ ] **Currency filter** (USD default)
- [ ] Output schema for dashboard and registry (with schema_version)
- [ ] Uses T1.5 FamaFrenchLocalProvider
- [ ] Uses DatasetVersionManager for PIT universe
- [ ] **dataset_version_id propagated to all outputs**
- [ ] **snapshot_id tracked in results**
- [ ] >90% test coverage (measured via pytest --cov)

### Stretch (If Time)
- [ ] Conditional attribution (up/down market regimes)
- [ ] Regime detector for market state classification

---

## Implementation Order

1. **Core Data Classes** (Day 1 morning)
   - `FactorAttributionConfig` with filter params
   - `AttributionResult` with schema_version
   - `RollingExposureResult` with schema_version
   - `ReturnDecompositionResult`
   - Custom exceptions

2. **Filter Methods** (Day 1 morning)
   - `_apply_microcap_filter()`
   - `_apply_currency_filter()`
   - Unit tests for filters

3. **Regression Engine** (Day 1 afternoon)
   - `_run_regression()` with all std_error types
   - NW lag edge case handling
   - `fit()` method with PIT integration
   - Unit tests for regression

4. **Rolling Exposure** (Day 2 morning)
   - `compute_rolling_exposures()`
   - Rebalance calendar logic (EOM, EOW)
   - Unit tests for rolling

5. **Diagnostics & VIF** (Day 2 afternoon)
   - `check_multicollinearity()`
   - Durbin-Watson test
   - `decompose_returns()`
   - Unit tests for diagnostics

6. **PIT Integration** (Day 3 morning)
   - DatasetVersionManager integration
   - Version ID construction
   - PITViolationError checks
   - Integration tests

7. **Serialization & Polish** (Day 3 afternoon)
   - `to_registry_dict()` methods
   - `to_dashboard_dict()` methods
   - JSON schema validation tests

8. **Documentation** (Day 3 afternoon)
   - Create `docs/CONCEPTS/performance-attribution.md` (required by spec)
   - Update `docs/INDEX.md` with plan and concept references

9. **Review & Commit** (Day 3-4)
   - Code review (Gemini + Codex)
   - Address feedback
   - CI and commit

---

## Files to Create/Modify

### New Files
- `libs/analytics/attribution.py` (~700 lines) - Main implementation
- `tests/libs/analytics/test_attribution.py` (~1000 lines) - Comprehensive tests
- `docs/CONCEPTS/performance-attribution.md` - Concept documentation (required by spec)

### Modified Files
- `libs/analytics/__init__.py` - Add exports
- `docs/INDEX.md` - Add plan and concept doc references

---

## Risk Mitigation

| Risk | Mitigation |
|------|------------|
| statsmodels API changes | Pin version, use stable API |
| Numerical instability | Use robust standard errors, check condition number |
| Insufficient overlap dates | Clear error message with date ranges |
| Large memory for rolling | Process in chunks if > 10 years |
| Filter removes all data | Raise InsufficientObservationsError with filter stats |

---

## Notes

- All returns must be in **decimal form** (0.01 = 1%)
- Fama-French factors from provider are already normalized to decimal
- Alpha is annualized: `alpha_daily * 252 * 10000` for bps
- Use frozen dataclasses for immutability
- Follow existing patterns from `libs/alpha/` and `libs/analytics/event_study.py`
- **File naming:** Use `attribution.py` per P4T2 spec (not `factor_attribution.py`)
