# T1.6 Dataset Versioning Implementation Plan

**Component:** T1.6-Dataset-Versioning
**Task:** P4T1 Phase 1 Foundation - Data Infrastructure
**Date:** 2025-12-04
**Status:** Plan Review Iteration 3
**Review History:**
- Iteration 1: Gemini APPROVED, Codex CHANGES_REQUESTED (10 issues)
- Iteration 2: Gemini APPROVED, Codex CHANGES_REQUESTED (2 issues: test count mismatch, deletion protocol)
- Iteration 3: All issues addressed (11 total resolutions documented below)

---

## Overview

Implement Dataset Versioning & Reproducibility system for the local data warehouse. This enables:
- Git-like versioning for dataset snapshots
- Time-travel queries ("give me data as of 2024-01-15")
- Manifest files linking backtest results to data versions
- Reproducible research by ensuring backtests can reference exact data states

---

## Dependencies

**Required (already implemented):**
- T1.1 Data Quality & Validation Framework (`libs/data_quality/`)
- T1.2 WRDS Connection & Bulk Sync Manager (`libs/data_providers/sync_manager.py`)

**Uses:**
- `ManifestManager` from `libs/data_quality/manifest.py` (has basic `create_snapshot`)
- `LockToken` from `libs/data_quality/types.py`
- `DataValidator` from `libs/data_quality/validation.py` (checksum computation)
- Atomic write patterns from existing codebase

---

## Architecture

### Snapshot Storage Strategy

```
data/
├── wrds/                          # Live data (mutable)
│   ├── crsp/daily/*.parquet
│   └── compustat/*.parquet
├── snapshots/                     # Immutable snapshots
│   ├── 2024-01-15/               # Version tag (date-based)
│   │   ├── manifest.json         # Snapshot metadata
│   │   └── files/                # Per-file entries (hardlink/copy/cas)
│   │       └── crsp_daily_2024.parquet
│   └── backtest-abc123/          # Named version (backtest reference)
│       └── ...
├── cas/                           # Content-addressable storage
│   ├── <sha256hash>.parquet      # Deduplicated files
│   └── _refcount.json            # Reference counting index
├── diffs/                         # Compressed diffs for recovery
│   └── <from>_<to>.json.zst      # Compressed diff files
├── backtests/                     # Backtest → snapshot index
│   └── <backtest_id>.json        # Backtest linkage manifest
└── manifests/                     # Current sync manifests
```

---

## ISSUE RESOLUTIONS (Codex Review)

### Issue 1: Consistency Locks (BLOCKER)

**Problem:** Snapshot creation must coordinate with sync locks to capture consistent state.

**Solution:** Implement manifest-version gating with read-lock pattern:

```python
SNAPSHOT CONSISTENCY PROTOCOL:
1. Before snapshot creation:
   - Read manifest_version for each target dataset
   - Store versions in snapshot_request.pinned_versions

2. During snapshot:
   - For each file copy/hardlink operation:
     - Re-read manifest_version
     - If changed: abort, cleanup, raise SnapshotInconsistentError
   - This is "optimistic concurrency" - no blocking of sync writers

3. Alternative (for long snapshots):
   - Acquire shared "snapshot lock" via ManifestManager
   - Sync writers check for active snapshot locks before writing
   - Lock file: data/locks/snapshot_active.lock

IMPLEMENTATION:
def create_snapshot(...) -> SnapshotManifest:
    # Pin manifest versions at start
    pinned_versions = {
        ds: self.manifest_manager.load_manifest(ds).manifest_version
        for ds in datasets
    }

    try:
        # ... create snapshot files ...

        # Verify no manifest changes during snapshot
        for ds, pinned_v in pinned_versions.items():
            current = self.manifest_manager.load_manifest(ds)
            if current.manifest_version != pinned_v:
                raise SnapshotInconsistentError(
                    f"Dataset {ds} was modified during snapshot"
                )
    except:
        self._cleanup_partial_snapshot(version_tag)
        raise
```

**Tests:**
- `test_snapshot_aborts_if_manifest_changes_during_creation`
- `test_snapshot_pinned_versions_match_final_state`

---

### Issue 2: Backtest Linkage Model (MAJOR)

**Problem:** Need durable backtest→snapshot index for many-to-many mapping.

**Solution:** Add dedicated backtest index at `data/backtests/<id>.json`:

```python
# data/backtests/<backtest_id>.json
class BacktestLinkage(BaseModel):
    """Durable backtest → snapshot mapping."""
    backtest_id: str
    created_at: datetime
    snapshot_version: str              # Primary snapshot reference
    dataset_versions: dict[str, int]   # dataset → manifest_version at link time
    checksum: str                      # Integrity checksum of this linkage

BACKTEST INDEX STORAGE:
data/backtests/
├── backtest-abc123.json
├── backtest-def456.json
└── _index.json                       # Quick lookup: backtest_id -> version_tag

ATOMIC UPDATE PROTOCOL:
1. Write linkage to temp file
2. Verify checksum
3. Atomic rename to final path
4. Update _index.json atomically
5. Update snapshot's referenced_by list atomically
```

**API:**
```python
def link_backtest(
    self,
    backtest_id: str,
    version_tag: str,
    datasets: list[str] | None = None,  # Specific datasets (default: all in snapshot)
) -> BacktestLinkage:
    """Atomically link backtest to snapshot."""

def get_backtests_for_snapshot(self, version_tag: str) -> list[str]:
    """Get all backtest IDs referencing a snapshot."""

def get_snapshot_for_backtest(self, backtest_id: str) -> SnapshotManifest | None:
    """Get snapshot linked to backtest."""
```

**Tests:**
- `test_backtest_linkage_atomic_update`
- `test_backtest_many_to_many_mapping`
- `test_backtest_index_quick_lookup`
- `test_backtest_linkage_survives_crash`

---

### Issue 3: CAS Accounting & GC (MAJOR)

**Problem:** CAS reference counting and GC design unspecified.

**Solution:** Implement reference-counted CAS with atomic updates:

```python
# data/cas/_refcount.json
class CASIndex(BaseModel):
    """CAS reference counting index."""
    files: dict[str, CASEntry]  # hash -> entry

class CASEntry(BaseModel):
    hash: str
    size_bytes: int
    original_path: str          # First path that stored this hash
    created_at: datetime
    ref_count: int              # Number of snapshots referencing this
    referencing_snapshots: list[str]  # Version tags for audit

CAS OPERATIONS:
1. _store_in_cas(file_path) -> hash:
   - Compute SHA-256
   - If hash exists in index: increment ref_count, add to referencing_snapshots
   - If new: copy file to cas/<hash>.parquet, create entry with ref_count=1
   - Return hash

2. _release_cas_ref(hash, version_tag) -> None:
   - Decrement ref_count
   - Remove version_tag from referencing_snapshots
   - (Does NOT delete file - GC handles that)

3. _gc_cas() -> int:
   - Acquire exclusive CAS lock
   - For each entry with ref_count == 0:
     - Delete cas/<hash>.parquet
     - Remove from index
   - Return bytes freed

ATOMIC UPDATE:
- Use temp file + rename for _refcount.json updates
- Acquire CAS lock (data/locks/cas.lock) for modifications
- Read operations don't need lock (eventual consistency OK for ref lists)
```

**Tests:**
- `test_cas_ref_count_increment_on_duplicate`
- `test_cas_gc_removes_zero_ref_files`
- `test_cas_gc_preserves_referenced_files`
- `test_cas_cross_filesystem_fallback_updates_refs`
- `test_cas_atomic_index_update`

---

### Issue 4: Diff Storage & Recovery (MAJOR)

**Problem:** Diff storage location and recovery algorithm undefined.

**Solution:** Define diff storage at `data/diffs/` with recovery workflow:

```python
# data/diffs/<from>_<to>.json.zst (zstandard compressed)
class SnapshotDiff(BaseModel):
    """Compressed diff between snapshots."""
    from_version: str
    to_version: str
    created_at: datetime
    added_files: list[DiffFileEntry]
    removed_files: list[str]          # Paths only (content not stored)
    changed_files: list[DiffFileEntry]
    checksum: str                     # Diff integrity checksum

class DiffFileEntry(BaseModel):
    path: str
    old_hash: str | None              # None for added files
    new_hash: str
    storage: Literal["inline", "cas"]
    inline_data: bytes | None         # For small diffs (< 1MB)
    cas_hash: str | None              # For large diffs

DIFF STORAGE POLICY:
- Location: data/diffs/<from_tag>_<to_tag>.json.zst
- Compression: zstandard (zstd) for space efficiency
- Inline threshold: Files < 1MB stored inline in diff
- Large files: Reference CAS hash (requires CAS entry to exist)

RECOVERY ALGORITHM:
def recover_corrupted_snapshot(version_tag: str, source_version: str | None) -> bool:
    1. If source_version not specified:
       - Find nearest valid snapshot by date
       - Preference: older snapshot (proven stable)

    2. Load diff chain: source -> target
       - If direct diff exists: use it
       - Else: construct chain through intermediate versions

    3. For each diff in chain:
       - Apply additions: restore from inline_data or CAS
       - Apply changes: restore from inline_data or CAS
       - Skip removals (target shouldn't have them)

    4. Verify recovered snapshot checksums

    5. If verification fails: raise SnapshotRecoveryError

DIFF CHAIN EXAMPLE:
v1 -> v2 (diff exists)
v2 -> v3 (diff exists)
v1 -> v3 (no direct diff, but can apply v1->v2 then v2->v3)
```

**Tests:**
- `test_diff_generation_captures_all_changes`
- `test_diff_inline_storage_for_small_files`
- `test_diff_cas_reference_for_large_files`
- `test_recovery_from_nearest_valid_snapshot`
- `test_recovery_chain_through_intermediate_versions`
- `test_recovery_fails_cleanly_on_missing_diff`

---

### Issue 5: Time-Travel Semantics (MAJOR)

**Problem:** query_as_of behavior underspecified.

**Solution:** Define explicit ordering and selection rules:

```python
TIME-TRAVEL QUERY SEMANTICS:

1. SNAPSHOT ORDERING:
   - Snapshots ordered by created_at timestamp (UTC)
   - Date-based tags (YYYY-MM-DD) normalized to midnight UTC
   - Named tags (backtest-xxx) use actual created_at

2. QUERY RESOLUTION:
   query_as_of(dataset, as_of_date) returns:
   - Latest snapshot where created_at <= as_of_date 23:59:59 UTC
   - Among candidates, prefer snapshots containing requested dataset

3. TIE-BREAKING (multiple snapshots same day):
   - Use created_at timestamp (not just date)
   - Later timestamp wins (latest state for that day)

4. FALLBACK BEHAVIOR:
   - If no snapshot exists <= as_of_date: raise SnapshotNotFoundError
   - If snapshot exists but doesn't contain dataset: raise DatasetNotInSnapshotError

5. NON-DATE TAGS:
   - Named tags (backtest-xxx) are NOT included in time-travel queries
   - Use get_data_at_version(dataset, version_tag) for named tags

def query_as_of(
    self,
    dataset: str,
    as_of_date: date,
) -> tuple[Path, SnapshotManifest]:
    """
    Get dataset path as it existed on given date.

    Returns:
        Tuple of (data_path, snapshot_manifest)

    Raises:
        SnapshotNotFoundError: No snapshot exists <= as_of_date
        DatasetNotInSnapshotError: Snapshot exists but lacks dataset
    """
    # Get all date-based snapshots
    candidates = [
        s for s in self.list_snapshots()
        if self._is_date_based_tag(s.version_tag)
        and s.created_at.date() <= as_of_date
    ]

    if not candidates:
        raise SnapshotNotFoundError(f"No snapshot exists on or before {as_of_date}")

    # Sort by created_at descending, take first
    candidates.sort(key=lambda s: s.created_at, reverse=True)
    best = candidates[0]

    if dataset not in best.datasets:
        raise DatasetNotInSnapshotError(f"Snapshot {best.version_tag} lacks {dataset}")

    return self.get_data_at_version(dataset, best.version_tag), best
```

**Tests:**
- `test_time_travel_returns_latest_snapshot_before_date`
- `test_time_travel_tie_breaking_same_day`
- `test_time_travel_fallback_raises_not_found`
- `test_time_travel_excludes_named_tags`
- `test_time_travel_date_gap_handling`
- `test_time_travel_multiple_snapshots_same_day`

---

### Issue 6: Storage Metadata Granularity (MAJOR)

**Problem:** storage_type is per-snapshot but real snapshots may mix storage modes.

**Solution:** Per-file storage metadata:

```python
class FileStorageInfo(BaseModel):
    """Per-file storage metadata."""
    path: str                         # Relative path within snapshot
    original_path: str                # Original source path
    storage_mode: Literal["hardlink", "copy", "cas"]
    target: str                       # Hardlink/copy path OR CAS hash
    size_bytes: int
    checksum: str                     # SHA-256 of file content

class DatasetSnapshot(BaseModel):
    """Snapshot info for a single dataset (UPDATED)."""
    dataset: str
    sync_manifest_version: int
    files: list[FileStorageInfo]      # Per-file storage info (replaces file_checksums)
    row_count: int
    date_range: tuple[date, date]

class SnapshotManifest(BaseModel):
    """Metadata for a dataset snapshot (UPDATED)."""
    version_tag: str
    created_at: datetime
    datasets: dict[str, DatasetSnapshot]
    total_size_bytes: int
    aggregate_checksum: str           # Hash of all file checksums (for tamper detection)
    referenced_by: list[str]
    # REMOVED: storage_type (now per-file in FileStorageInfo)
```

**Tests:**
- `test_snapshot_mixed_storage_modes`
- `test_per_file_storage_verification`
- `test_storage_mode_recorded_correctly`

---

### Issue 7: Atomicity & Error Handling (MAJOR)

**Problem:** No rollback/cleanup for partial snapshot builds.

**Solution:** Staging directory with atomic commit:

```python
ATOMIC SNAPSHOT CREATION:

1. STAGING:
   staging_dir = data/snapshots/.staging_<version_tag>_<pid>

2. BUILD PHASE (in staging):
   - Create staging directory
   - For each file:
     - Try hardlink first
     - Fallback to copy if OSError (cross-filesystem)
     - On ANY error: goto CLEANUP
   - Write manifest.json to staging
   - Verify all checksums match

3. COMMIT PHASE:
   - Atomic rename: staging_dir -> final_dir
   - If rename fails: goto CLEANUP
   - fsync parent directory
   - Update CAS refcounts (already committed, so safe)

4. CLEANUP (on any failure):
   - Remove staging directory recursively
   - Release any CAS refs that were incremented
   - Log failure with full context
   - Raise appropriate exception

FAILURE SCENARIOS:
- Hardlink fails mid-run: Fallback to copy, if copy fails -> cleanup
- Checksum mismatch: Cleanup, raise ChecksumMismatchError
- ENOSPC: Cleanup, quarantine temp files, raise DiskSpaceError
- Manifest change detected: Cleanup, raise SnapshotInconsistentError

def _cleanup_partial_snapshot(self, version_tag: str) -> None:
    """Remove staging directory and release CAS refs."""
    staging_path = self.snapshots_dir / f".staging_{version_tag}_{os.getpid()}"
    if staging_path.exists():
        # Release CAS refs for any files we added
        manifest_path = staging_path / "manifest.json"
        if manifest_path.exists():
            manifest = SnapshotManifest.model_validate_json(manifest_path.read_text())
            for ds in manifest.datasets.values():
                for f in ds.files:
                    if f.storage_mode == "cas":
                        self._release_cas_ref(f.target, version_tag)

        # Remove staging directory
        shutil.rmtree(staging_path, ignore_errors=True)
```

**Tests:**
- `test_partial_failure_leaves_no_visible_snapshot`
- `test_enospc_cleanup_and_quarantine`
- `test_checksum_mismatch_triggers_cleanup`
- `test_cas_refs_released_on_failure`
- `test_staging_directory_removed_on_success`

---

### Issue 8: Checksum Scope (MINOR)

**Problem:** Per-file and aggregate checksum computation unclear.

**Solution:** Document checksum chain:

```python
CHECKSUM COMPUTATION:

1. PER-FILE CHECKSUM:
   - Algorithm: SHA-256
   - Computed via DataValidator.compute_checksum(file_path)
   - Stored in FileStorageInfo.checksum

2. AGGREGATE CHECKSUM (tamper detection):
   - Purpose: Detect if any file was modified after snapshot creation
   - Algorithm:
     a. Sort all file checksums alphabetically by path
     b. Concatenate: "path1:checksum1\npath2:checksum2\n..."
     c. SHA-256 hash of concatenated string
   - Stored in SnapshotManifest.aggregate_checksum
   - Verification: Recompute aggregate from individual checksums, compare

3. HASH CHAIN (optional tamper detection):
   - Each snapshot includes prev_snapshot_checksum (like git commits)
   - Enables detection of retroactive manifest tampering
   - Implementation:
     hash_chain = SHA-256(prev_aggregate_checksum + current_aggregate_checksum)

def compute_aggregate_checksum(files: list[FileStorageInfo]) -> str:
    """Compute aggregate checksum for tamper detection."""
    sorted_files = sorted(files, key=lambda f: f.path)
    manifest_str = "\n".join(f"{f.path}:{f.checksum}" for f in sorted_files)
    return hashlib.sha256(manifest_str.encode()).hexdigest()
```

**Tests:**
- `test_aggregate_checksum_computation`
- `test_tamper_detection_via_aggregate_checksum`
- `test_hash_chain_detects_retroactive_tampering`

---

### Issue 9: Retention for Diffs (MINOR)

**Problem:** Diffs kept indefinitely with no guardrails.

**Solution:** Add retention guardrails:

```python
DIFF RETENTION POLICY:

1. DEFAULT: Keep diffs indefinitely (required for recovery)

2. GUARDRAILS:
   - Max total diff storage: 10 GB (configurable)
   - Max single diff size: 500 MB (larger diffs rejected, use full snapshot)
   - Age-based cleanup: Diffs for deleted snapshots removed after 30 days

3. MONITORING HOOKS:
   - Log warning at 80% of max storage
   - Log critical at 90% of max storage
   - Expose metrics: diff_storage_bytes, diff_count, oldest_diff_age

4. MANUAL CLEANUP:
   - prune_diffs(older_than_days: int) method for manual cleanup
   - Respects referenced snapshots (won't delete diffs needed for recovery)

DIFF_MAX_STORAGE_BYTES = 10 * 1024**3  # 10 GB
DIFF_MAX_SINGLE_SIZE_BYTES = 500 * 1024**2  # 500 MB
DIFF_CLEANUP_AFTER_DAYS = 30  # For orphaned diffs
```

**Tests:**
- `test_diff_storage_respects_max_limit`
- `test_large_diff_rejected`
- `test_orphaned_diff_cleanup`

---

### Issue 10: Provider Integration Surface (MINOR)

**Problem:** Provider contract undefined.

**Solution:** Explicit provider contract:

```python
PROVIDER VERSION_TAG CONTRACT:

1. DEFAULT BEHAVIOR (version_tag=None):
   - Query latest live data (data/wrds/...)
   - No snapshot involvement
   - Backward compatible with existing code

2. VERSIONED BEHAVIOR (version_tag="2024-01-15"):
   - Query snapshot data (data/snapshots/<tag>/...)
   - Raises SnapshotNotFoundError if tag doesn't exist
   - Raises DatasetNotInSnapshotError if dataset not in snapshot

3. VALIDATION:
   - Provider validates version_tag format
   - Invalid format: raise ValueError("Invalid version tag format")

4. BACKWARD COMPATIBILITY:
   - All existing call sites continue to work (version_tag defaults to None)
   - No changes required to existing code
   - New functionality opt-in via version_tag parameter

PROVIDER INTERFACE UPDATE:
class CRSPLocalProvider:
    def get_daily_returns(
        self,
        permnos: list[int] | None = None,
        start_date: date | None = None,
        end_date: date | None = None,
        version_tag: str | None = None,  # NEW PARAMETER
    ) -> pl.DataFrame:
        """
        Get daily returns.

        Args:
            version_tag: Optional snapshot version. If provided, queries
                        historical snapshot instead of live data.
        """
        if version_tag:
            # Delegate to versioning system
            path, _ = self.version_manager.get_data_at_version("crsp_daily", version_tag)
            return self._query_from_path(path, permnos, start_date, end_date)
        else:
            # Query live data (existing behavior)
            return self._query_live(permnos, start_date, end_date)
```

**Tests:**
- `test_provider_default_queries_live_data`
- `test_provider_version_tag_queries_snapshot`
- `test_provider_invalid_version_tag_raises`
- `test_provider_backward_compatible`

---

### Issue 11: Snapshot Deletion Protocol (NEW - Codex Review 2)

**Problem:** Deletion flow, locks, and CAS ref-release not specified.

**Solution:** Define complete deletion protocol:

```python
SNAPSHOT DELETION PROTOCOL:

1. REQUEST VALIDATION:
   def delete_snapshot(version_tag: str, force: bool = False) -> bool:
       # Check if snapshot exists
       snapshot = self.get_snapshot(version_tag)
       if not snapshot:
           raise SnapshotNotFoundError(version_tag)

       # Check if referenced (BLOCKING)
       if snapshot.referenced_by and not force:
           raise SnapshotReferencedError(
               f"Snapshot {version_tag} is referenced by: {snapshot.referenced_by}"
           )

2. LOCK ACQUISITION:
   - Acquire exclusive snapshot lock: data/locks/snapshots.lock
   - This prevents concurrent deletion/creation
   - Timeout: 30 seconds

3. DELETION ORDER (atomicity):
   a. Decrement CAS refcounts for all files in snapshot
      - For each file with storage_mode="cas":
        - Call _release_cas_ref(file.target, version_tag)
        - Do NOT delete CAS files yet (GC handles that)

   b. Remove backtest linkages pointing to this snapshot
      - Update data/backtests/_index.json atomically
      - Remove individual backtest linkage files? NO - keep for audit
      - Instead: mark linkage as "orphaned" with deleted_snapshot_at timestamp

   c. Archive or delete diff files
      - If diff references this snapshot (from_version or to_version):
        - Keep diff for 30 days (orphan retention)
        - After 30 days: prune_diffs() removes them

   d. Delete snapshot directory
      - shutil.rmtree(data/snapshots/<version_tag>)

   e. Release lock

4. FAILURE HANDLING:
   - If failure after CAS refcount decrement:
     - Log warning, but snapshot may be partially deleted
     - Next GC run will clean up orphaned CAS files
   - If failure during directory deletion:
     - Retry up to 3 times with backoff
     - If still fails: mark snapshot as "deletion_pending" in index

5. FORCE DELETE (force=True):
   - Bypasses referenced_by check
   - Logs warning: "Force-deleting referenced snapshot"
   - Updates backtest linkages to mark them orphaned
   - Used only for emergency cleanup

IMPLEMENTATION:
def delete_snapshot(self, version_tag: str, force: bool = False) -> bool:
    """Delete snapshot with full cleanup."""
    snapshot = self.get_snapshot(version_tag)
    if not snapshot:
        raise SnapshotNotFoundError(version_tag)

    # Block deletion if referenced (unless force)
    if snapshot.referenced_by and not force:
        raise SnapshotReferencedError(
            f"Cannot delete {version_tag}: referenced by {snapshot.referenced_by}"
        )

    with self._acquire_snapshot_lock():
        try:
            # 1. Release CAS refs
            for ds in snapshot.datasets.values():
                for f in ds.files:
                    if f.storage_mode == "cas":
                        self._release_cas_ref(f.target, version_tag)

            # 2. Orphan backtest linkages (don't delete - audit trail)
            for backtest_id in snapshot.referenced_by:
                self._mark_backtest_orphaned(backtest_id, version_tag)

            # 3. Mark related diffs as orphaned
            self._mark_diffs_orphaned(version_tag)

            # 4. Delete snapshot directory
            snapshot_path = self.snapshots_dir / version_tag
            shutil.rmtree(snapshot_path)

            logger.info("Deleted snapshot", extra={"version_tag": version_tag})
            return True

        except Exception as e:
            logger.error(
                "Snapshot deletion failed",
                extra={"version_tag": version_tag, "error": str(e)}
            )
            raise
```

**Tests:**
- `test_delete_snapshot_releases_cas_refs`
- `test_delete_snapshot_blocked_if_referenced`
- `test_delete_snapshot_force_bypasses_reference_check`
- `test_delete_snapshot_orphans_backtest_linkages`
- `test_delete_snapshot_marks_diffs_orphaned`

---

## Updated Data Models

```python
# libs/data_quality/versioning.py

class FileStorageInfo(BaseModel):
    """Per-file storage metadata."""
    path: str
    original_path: str
    storage_mode: Literal["hardlink", "copy", "cas"]
    target: str
    size_bytes: int
    checksum: str

class DatasetSnapshot(BaseModel):
    """Snapshot info for a single dataset."""
    dataset: str
    sync_manifest_version: int
    files: list[FileStorageInfo]
    row_count: int
    date_range: tuple[date, date]

class SnapshotManifest(BaseModel):
    """Metadata for a dataset snapshot."""
    version_tag: str
    created_at: datetime
    datasets: dict[str, DatasetSnapshot]
    total_size_bytes: int
    aggregate_checksum: str
    referenced_by: list[str]
    prev_snapshot_checksum: str | None = None  # Hash chain for tamper detection

class BacktestLinkage(BaseModel):
    """Durable backtest → snapshot mapping."""
    backtest_id: str
    created_at: datetime
    snapshot_version: str
    dataset_versions: dict[str, int]
    checksum: str

class CASEntry(BaseModel):
    """CAS reference counting entry."""
    hash: str
    size_bytes: int
    original_path: str
    created_at: datetime
    ref_count: int
    referencing_snapshots: list[str]

class CASIndex(BaseModel):
    """CAS reference counting index."""
    files: dict[str, CASEntry]
    total_size_bytes: int
    last_gc_at: datetime | None = None

class SnapshotDiff(BaseModel):
    """Compressed diff between snapshots."""
    from_version: str
    to_version: str
    created_at: datetime
    added_files: list[DiffFileEntry]
    removed_files: list[str]
    changed_files: list[DiffFileEntry]
    checksum: str

class DiffFileEntry(BaseModel):
    """File entry in a diff."""
    path: str
    old_hash: str | None
    new_hash: str
    storage: Literal["inline", "cas"]
    inline_data: bytes | None
    cas_hash: str | None
```

---

## Updated Test Cases (38 total)

### Core Functionality (5)
1. `test_snapshot_creation_with_version_tag`
2. `test_snapshot_retrieval_by_version`
3. `test_backtest_to_version_manifest_linkage`
4. `test_snapshot_retention_policy_enforcement`
5. `test_list_snapshots_ordered_by_date`

### Consistency & Locking (3)
6. `test_snapshot_aborts_if_manifest_changes_during_creation`
7. `test_snapshot_pinned_versions_match_final_state`
8. `test_snapshot_consistency_under_concurrent_sync`

### Time-Travel (6)
9. `test_time_travel_returns_latest_snapshot_before_date`
10. `test_time_travel_tie_breaking_same_day`
11. `test_time_travel_fallback_raises_not_found`
12. `test_time_travel_excludes_named_tags`
13. `test_time_travel_date_gap_handling`
14. `test_time_travel_multiple_snapshots_same_day`

### Backtest Linkage (4)
15. `test_backtest_linkage_atomic_update`
16. `test_backtest_many_to_many_mapping`
17. `test_referenced_snapshot_deletion_blocked`
18. `test_backtest_linkage_survives_crash`

### Storage & CAS (6)
19. `test_hardlink_creation_for_unchanged_partitions`
20. `test_cross_filesystem_fallback_to_copy`
21. `test_cas_ref_count_increment_on_duplicate`
22. `test_cas_gc_removes_zero_ref_files`
23. `test_per_file_storage_verification`
24. `test_snapshot_mixed_storage_modes`

### Checksums & Integrity (3)
25. `test_checksum_validation_on_snapshot_creation`
26. `test_aggregate_checksum_computation`
27. `test_tamper_detection_via_aggregate_checksum`

### Error Handling & Recovery (4)
28. `test_partial_failure_leaves_no_visible_snapshot`
29. `test_diff_based_recovery_from_retained_snapshot`
30. `test_corrupted_snapshot_detection_and_recovery`
31. `test_cas_refs_released_on_failure`

### Snapshot Deletion (5) - NEW
32. `test_delete_snapshot_releases_cas_refs`
33. `test_delete_snapshot_blocked_if_referenced`
34. `test_delete_snapshot_force_bypasses_reference_check`
35. `test_delete_snapshot_orphans_backtest_linkages`
36. `test_delete_snapshot_marks_diffs_orphaned`

### Edge Cases (2)
37. `test_empty_dataset_snapshot`
38. `test_concurrent_snapshot_creation`

---

## New Exceptions

```python
class SnapshotNotFoundError(DataQualityError):
    """Raised when requested snapshot version doesn't exist."""

class SnapshotReferencedError(DataQualityError):
    """Raised when attempting to delete a referenced snapshot."""

class SnapshotCorruptedError(DataQualityError):
    """Raised when snapshot integrity verification fails."""

class SnapshotInconsistentError(DataQualityError):
    """Raised when source data changed during snapshot creation."""

class DatasetNotInSnapshotError(DataQualityError):
    """Raised when snapshot exists but doesn't contain requested dataset."""

class SnapshotRecoveryError(DataQualityError):
    """Raised when snapshot recovery fails."""
```

---

## Files to Create/Modify

### New Files
1. `libs/data_quality/versioning.py` - Main implementation (~800 lines)
2. `tests/libs/data_quality/test_versioning.py` - Test suite (~1200 lines)
3. `docs/CONCEPTS/dataset-versioning.md` - Concept documentation
4. `docs/ADRs/0020-dataset-versioning.md` - Architecture decision record

### Modified Files
1. `libs/data_quality/__init__.py` - Export new classes
2. `libs/data_quality/exceptions.py` - Add 6 new exceptions

---

## Acceptance Criteria

1. All 38 test cases pass
2. Code coverage > 85%
3. mypy --strict passes
4. No breaking changes to existing APIs (backward compatible)
5. Documentation complete (ADR + CONCEPTS)
6. Review approval from Gemini + Codex (both APPROVED, zero issues)

---

## De-scoped (Future Work)

Per task document:
- Iceberg/DuckDB time-travel integration (follow-up PR)
- Automatic snapshot scheduling (manual for v1)
- Remote backup integration (local only for v1)
- Provider updates (separate PR after versioning core is merged)
