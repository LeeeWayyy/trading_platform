# P4T1.9 Implementation Plan: Data Storage & ETL Pipeline

## Overview

Create a historical ETL pipeline that orchestrates data fetching via the UnifiedDataFetcher (P4T1.8), stores data in partitioned Parquet format, manages a DuckDB catalog for SQL queries, and supports incremental updates with atomic writes.

## Analysis Summary

### Existing Infrastructure

1. **UnifiedDataFetcher** (P4T1.8) - Entry point for data fetching:
   - Supports `get_daily_prices(symbols, start_date, end_date)` → pl.DataFrame
   - Supports `get_universe(as_of_date)` → list[str]
   - Unified schema: date, symbol, close, volume, ret, open, high, low, adj_close

2. **libs/data_quality/manifest.py** - ManifestManager with atomic writes:
   - `ManifestManager.acquire_lock()` - O_EXCL atomic lock
   - `ManifestManager.save_manifest()` - Atomic write + fsync
   - `ManifestManager.check_disk_space()` - Disk space watermarks
   - `ManifestManager.quarantine_data()` - Quarantine on failure
   - `SyncManifest` model with required fields

3. **libs/data_quality/validation.py** - DataValidator:
   - `validate_null_percentage()` - Check null thresholds
   - `validate_schema()` - Schema validation
   - `compute_checksum()` - SHA-256 file checksum

### What We Need to Build

**HistoricalETL** - New component that:
1. Uses UnifiedDataFetcher to fetch historical data
2. Partitions data by year (like sync_manager.py)
3. Writes to Parquet with atomic pattern + lock coupling
4. Maintains DuckDB catalog for SQL queries
5. Supports incremental updates with deduplication
6. Tracks progress with manifest + resume capability

## Implementation Details

### 1. ETL-Specific Manifest Extension (HIGH Priority Fix)

**Problem:** SyncManifest lacks `metadata` field for per-symbol tracking.

**Solution:** Use a separate ETL progress manifest (companion file) that tracks per-symbol state, rather than extending SyncManifest schema. This avoids migration issues and keeps SyncManifest compatible.

```python
class ETLProgressManifest(BaseModel):
    """ETL-specific progress tracking (separate from SyncManifest).

    Stored at: data/sync_progress/historical_daily_progress.json
    SyncManifest stored at: data/manifests/historical_daily.json (standard)
    """
    dataset: str
    last_updated: datetime
    symbol_last_dates: dict[str, date]  # Per-symbol last sync date
    years_completed: list[int]
    years_remaining: list[int]
    status: Literal["running", "paused", "completed", "failed"]

class HistoricalETL:
    DATASET_ID = "historical_daily"
    PROGRESS_DIR = Path("data/sync_progress")

    def _load_etl_progress(self) -> ETLProgressManifest | None:
        """Load ETL progress with corruption recovery.

        MEDIUM Priority Fix: Handle missing/corrupted progress files.

        Recovery behavior:
        - Missing file: Return None (fresh start)
        - Corrupted file: Log warning, backup corrupted, return None
        - Valid file: Parse and return
        """
        path = self.PROGRESS_DIR / f"{self.DATASET_ID}_progress.json"
        if not path.exists():
            return None

        try:
            content = path.read_text()
            return ETLProgressManifest.model_validate_json(content)
        except (json.JSONDecodeError, ValidationError) as e:
            # Corrupted progress file - backup and start fresh
            backup_path = path.with_suffix(f".json.corrupted.{int(time.time())}")
            shutil.copy2(path, backup_path)
            logger.warning(
                "Corrupted ETL progress file backed up, starting fresh",
                extra={
                    "event": "etl.progress.corrupted",
                    "original": str(path),
                    "backup": str(backup_path),
                    "error": str(e),
                },
            )
            return None

    def _save_etl_progress(self, progress: ETLProgressManifest) -> None:
        """Atomic save of ETL progress with fsync and backup.

        MEDIUM Priority Fix: Durable progress writes with rollback capability.

        Durability guarantees:
        1. Backup previous progress before overwrite
        2. Write to temp file
        3. Compute checksum
        4. fsync temp file
        5. Atomic rename
        6. fsync directory

        On failure: previous progress file remains intact.
        """
        path = self.PROGRESS_DIR / f"{self.DATASET_ID}_progress.json"
        temp_path = path.with_suffix(".json.tmp")
        backup_path = path.with_suffix(".json.backup")

        # Step 1: Backup previous progress (if exists)
        if path.exists():
            shutil.copy2(path, backup_path)

        try:
            # Step 2: Write to temp
            content = progress.model_dump_json(indent=2)
            temp_path.write_text(content)

            # Step 3: fsync temp file
            with open(temp_path, "rb") as f:
                os.fsync(f.fileno())

            # Step 4: Atomic rename
            temp_path.rename(path)

            # Step 5: fsync directory
            dir_fd = os.open(self.PROGRESS_DIR, os.O_RDONLY | os.O_DIRECTORY)
            try:
                os.fsync(dir_fd)
            finally:
                os.close(dir_fd)

        except OSError as e:
            # Clean up temp on failure
            if temp_path.exists():
                temp_path.unlink()
            # Previous progress file (or backup) remains intact
            raise ETLProgressError(f"Failed to save ETL progress: {e}") from e
```

### 2. Complete Manifest Generation (MEDIUM Priority Fix)

**Problem:** Plan didn't specify how SyncManifest fields are produced.

**Solution:** Explicit manifest generation with all required fields:

```python
def _create_sync_manifest(
    self,
    file_paths: list[Path],
    start_date: date,
    end_date: date,
) -> SyncManifest:
    """Create SyncManifest with all required fields.

    Computes:
    - row_count: Sum of rows across all partitions
    - checksum: Combined SHA-256 of all partition files
    - file_paths: List of partition paths
    - start_date/end_date: Date range
    - schema_version: From schema registry or default
    """
    # Compute combined row count
    total_rows = 0
    for path in file_paths:
        df = pl.scan_parquet(path).select(pl.len()).collect()
        total_rows += df.item()

    # Compute combined checksum (sorted order for determinism)
    hasher = hashlib.sha256()
    for path in sorted(file_paths):
        file_checksum = self.validator.compute_checksum(path)
        hasher.update(file_checksum.encode())
    combined_checksum = hasher.hexdigest()

    # Compute query hash (for reproducibility)
    query_hash = hashlib.sha256(
        f"{self.DATASET_ID}:{start_date}:{end_date}".encode()
    ).hexdigest()

    return SyncManifest(
        dataset=self.DATASET_ID,
        sync_timestamp=datetime.now(UTC),
        start_date=start_date,
        end_date=end_date,
        row_count=total_rows,
        checksum=combined_checksum,
        checksum_algorithm="sha256",
        schema_version="v1.0.0",
        wrds_query_hash=query_hash,
        file_paths=[str(p) for p in file_paths],
        validation_status="passed",
    )

def run_full_etl(self, ...) -> ETLResult:
    """Full ETL with complete manifest generation."""
    with self.manifest_manager.acquire_lock(
        dataset=self.DATASET_ID,
        writer_id=f"etl_{os.getpid()}",
    ) as lock_token:
        # ... process partitions ...

        # Create manifest with all required fields
        manifest = self._create_sync_manifest(
            file_paths=written_paths,
            start_date=start_date,
            end_date=end_date,
        )

        # Save manifest (includes backup of previous under lock)
        self.manifest_manager.save_manifest(manifest, lock_token)
```

### 3. Data Validation Hook (MEDIUM Priority Fix)

**Problem:** No explicit data quality validation step before committing.

**Solution:** Add validation gate using DataValidator before atomic write:

```python
def _validate_partition(self, df: pl.DataFrame, year: int) -> list[str]:
    """Validate partition data quality before writing.

    Checks:
    1. Primary key uniqueness (no duplicates)
    2. No nulls in primary key columns
    3. Schema matches expected (date, symbol, etc.)

    Returns:
        List of validation errors (empty = passed).

    Raises:
        DataQualityError: If validation fails and quarantine triggered.
    """
    errors: list[str] = []

    # 1. Primary key uniqueness
    pk_cols = self.PRIMARY_KEYS
    dup_count = df.height - df.unique(subset=pk_cols).height
    if dup_count > 0:
        errors.append(f"Duplicate primary keys: {dup_count} rows")

    # 2. No nulls in primary keys
    for col in pk_cols:
        null_count = df.filter(pl.col(col).is_null()).height
        if null_count > 0:
            errors.append(f"Null values in {col}: {null_count} rows")

    # 3. Schema validation (required columns exist)
    required_cols = ["date", "symbol", "close", "volume"]
    missing = [c for c in required_cols if c not in df.columns]
    if missing:
        errors.append(f"Missing required columns: {missing}")

    return errors

def _atomic_write_with_quarantine(
    self, df: pl.DataFrame, target_path: Path
) -> str:
    """Atomic write with complete validation + quarantine coupling.

    HIGH Priority Fix: Full integration with DataValidator and ManifestManager.quarantine_data.

    Failure Path:
    1. Validation fails → quarantine temp file → mark manifest failed → raise
    2. ENOSPC during write → quarantine temp → leave manifest unchanged → raise
    3. Checksum mismatch → quarantine target → rollback → raise

    Returns:
        Checksum of written file.
    """
    temp_path = target_path.with_suffix(".parquet.tmp")
    year = int(target_path.stem)

    try:
        # 1. Validate BEFORE writing (using DataValidator)
        validation_errors = self._validate_partition(df, year)
        if validation_errors:
            logger.error(
                "Partition validation failed",
                extra={"event": "etl.validation.failed", "year": year, "errors": validation_errors},
            )
            raise DataQualityError(f"Validation failed: {validation_errors}")

        # 2. Disk space check on DATA volume (not manifest volume)
        estimated_size = self._estimate_parquet_size(df)
        self._check_disk_space_on_path(target_path.parent, estimated_size * 2)

        # 3. Write to temp
        df.write_parquet(temp_path)

        # 4. Compute checksum
        checksum = self.validator.compute_checksum(temp_path)

        # 5. fsync temp file
        with open(temp_path, "rb") as f:
            os.fsync(f.fileno())

        # 6. Atomic rename
        temp_path.rename(target_path)

        # 7. fsync directory
        dir_fd = os.open(target_path.parent, os.O_RDONLY | os.O_DIRECTORY)
        try:
            os.fsync(dir_fd)
        finally:
            os.close(dir_fd)

        # 8. Verify checksum after rename
        verify_checksum = self.validator.compute_checksum(target_path)
        if verify_checksum != checksum:
            raise ChecksumMismatchError(
                f"Checksum mismatch after write: {checksum} vs {verify_checksum}"
            )

        return checksum

    except OSError as e:
        # ENOSPC or other I/O error
        if temp_path.exists():
            self.manifest_manager.quarantine_data(
                source_path=temp_path,
                reason=f"Write failed: {e}",
                dataset=self.DATASET_ID,
            )
        raise

    except (DataQualityError, ChecksumMismatchError) as e:
        # Quarantine on validation/checksum failure
        quarantine_path = temp_path if temp_path.exists() else target_path
        if quarantine_path.exists():
            self.manifest_manager.quarantine_data(
                source_path=quarantine_path,
                reason=str(e),
                dataset=self.DATASET_ID,
            )
        raise

def _check_disk_space_on_path(self, path: Path, required_bytes: int) -> None:
    """Check disk space on the SPECIFIC volume containing path.

    LOW Priority Fix: Ensures we check data volume, not just manifest volume.
    """
    stat = os.statvfs(path)
    available = stat.f_bavail * stat.f_frsize
    if available < required_bytes:
        raise DiskSpaceError(
            f"Insufficient disk space on {path}: need {required_bytes}, have {available}"
        )

def _check_merge_disk_space(
    self, affected_years: list[int], new_df: pl.DataFrame
) -> None:
    """Check disk space for merge operations.

    MEDIUM Priority Fix: Explicit sizing formula for merge rewrites.

    Formula:
    required = existing_partitions + new_data + temp_files + manifest_overhead

    Components:
    - existing_partitions: Sum of sizes of partitions being rewritten
    - new_data: Estimated size of new data (using estimated_size)
    - temp_files: 1x buffer for temp files during atomic write
    - manifest_overhead: 10KB for manifest + progress files
    """
    existing_size = 0
    for year in affected_years:
        partition_path = self.storage_path / "daily" / f"{year}.parquet"
        if partition_path.exists():
            existing_size += partition_path.stat().st_size

    # New data estimate
    new_data_size = self._estimate_parquet_size(new_df)

    # Total required: existing + new + temp (1x) + manifest (10KB)
    # Merged file ~= existing + new (may be smaller due to dedup)
    merged_estimate = existing_size + new_data_size
    temp_buffer = merged_estimate  # 1x for temp during atomic write
    manifest_overhead = 10 * 1024  # 10KB

    required = merged_estimate + temp_buffer + manifest_overhead

    # Add 20% safety margin
    required_with_margin = int(required * 1.2)

    self._check_disk_space_on_path(self.storage_path, required_with_margin)

    logger.debug(
        "Disk space check passed for merge",
        extra={
            "event": "etl.disk_check",
            "existing_size": existing_size,
            "new_data_size": new_data_size,
            "required_with_margin": required_with_margin,
        },
    )

def _handle_validation_failure(
    self, partition_path: Path, df: pl.DataFrame, errors: list[str]
) -> None:
    """Handle validation failure with quarantine and manifest update.

    HIGH Priority Fix: Complete failure path with quarantine.
    """
    # Write failed data to temp for quarantine
    temp_path = partition_path.with_suffix(".parquet.failed")
    df.write_parquet(temp_path)

    # Quarantine the failed data
    self.manifest_manager.quarantine_data(
        source_path=temp_path,
        reason=f"Validation errors: {errors}",
        dataset=self.DATASET_ID,
    )

    logger.error(
        "Partition quarantined due to validation failure",
        extra={
            "event": "etl.quarantine",
            "partition": str(partition_path),
            "errors": errors,
        },
    )
```

### 4. Accurate Disk Space Estimation (LOW Priority Fix)

**Problem:** `len(df) * 200` is ad-hoc and may mis-estimate.

**Solution:** Use Polars `estimated_size()` for accurate estimation:

```python
def _estimate_parquet_size(self, df: pl.DataFrame) -> int:
    """Estimate Parquet file size from DataFrame.

    Uses Polars estimated_size() which accounts for:
    - Column data types
    - Actual data distribution
    - Compression factor (Parquet typically 2-5x smaller than memory)

    Returns:
        Estimated bytes (conservative: 1.5x estimated_size for safety margin).
    """
    # Polars estimated_size() returns in-memory size
    memory_size = df.estimated_size()

    # Parquet is typically 2-5x smaller than in-memory
    # Use conservative 0.5x factor (memory → parquet)
    parquet_estimate = int(memory_size * 0.5)

    # Add 50% safety margin for compression variance
    return int(parquet_estimate * 1.5)

def _atomic_write_parquet(self, df: pl.DataFrame, target_path: Path) -> str:
    """Atomic write with accurate disk space check."""
    temp_path = target_path.with_suffix(".parquet.tmp")

    try:
        # 1. Accurate disk space check using estimated_size
        estimated_size = self._estimate_parquet_size(df)
        required_space = estimated_size * 2  # 2x for temp + final
        self.manifest_manager.check_disk_space(required_space)

        # 2. Write to temp
        df.write_parquet(temp_path)

        # 3-7. Rest of atomic write pattern...
```

## Complete Implementation Details

### Single-Writer Locking + Manifest Coupling

```python
class HistoricalETL:
    """Uses ManifestManager for lock + manifest coupling."""

    DATASET_ID = "historical_daily"
    PRIMARY_KEYS = ["date", "symbol"]

    def __init__(
        self,
        fetcher: UnifiedDataFetcher,
        storage_path: Path = Path("data/historical"),
        catalog_path: Path = Path("data/duckdb/historical_catalog.duckdb"),
        manifest_manager: ManifestManager | None = None,
        validator: DataValidator | None = None,
    ) -> None:
        self.fetcher = fetcher
        self.storage_path = storage_path
        self.catalog_path = catalog_path
        self.manifest_manager = manifest_manager or ManifestManager()
        self.validator = validator or DataValidator()

        # Ensure directories exist
        self.storage_path.mkdir(parents=True, exist_ok=True)
        self.catalog_path.parent.mkdir(parents=True, exist_ok=True)
        self.PROGRESS_DIR.mkdir(parents=True, exist_ok=True)

    def run_full_etl(
        self,
        symbols: list[str],
        start_date: date,
        end_date: date,
        resume: bool = True,
    ) -> ETLResult:
        """Full ETL with lock + manifest coupling."""
        with self.manifest_manager.acquire_lock(
            dataset=self.DATASET_ID,
            writer_id=f"etl_{os.getpid()}",
        ) as lock_token:
            # Disk space check
            estimated_bytes = self._estimate_total_size(symbols, start_date, end_date)
            self.manifest_manager.check_disk_space(estimated_bytes * 2)

            # Check for resume
            etl_progress = self._load_etl_progress() if resume else None
            if etl_progress and etl_progress.status in ("running", "paused"):
                years = etl_progress.years_remaining
            else:
                years = list(range(start_date.year, end_date.year + 1))
                etl_progress = ETLProgressManifest(
                    dataset=self.DATASET_ID,
                    last_updated=datetime.now(UTC),
                    symbol_last_dates={},
                    years_completed=[],
                    years_remaining=years.copy(),
                    status="running",
                )

            written_paths: list[Path] = []
            total_rows = 0

            for year in years:
                # Fetch data for year
                year_start = max(date(year, 1, 1), start_date)
                year_end = min(date(year, 12, 31), end_date)

                df = self.fetcher.get_daily_prices(
                    symbols=symbols,
                    start_date=year_start,
                    end_date=year_end,
                )

                if df.is_empty():
                    continue

                # Validate + atomic write
                partition_path = self.storage_path / "daily" / f"{year}.parquet"
                self._atomic_write_parquet(df, partition_path)
                written_paths.append(partition_path)
                total_rows += df.height

                # Update progress checkpoint
                etl_progress.years_completed.append(year)
                etl_progress.years_remaining.remove(year)
                etl_progress.last_updated = datetime.now(UTC)
                self._save_etl_progress(etl_progress)

            # Create and save manifest
            manifest = self._create_sync_manifest(
                file_paths=written_paths,
                start_date=start_date,
                end_date=end_date,
            )
            self.manifest_manager.save_manifest(manifest, lock_token)

            # Update DuckDB catalog
            self._update_catalog()

            # Mark complete
            etl_progress.status = "completed"
            self._save_etl_progress(etl_progress)

            return ETLResult(
                total_rows=total_rows,
                partitions_written=[str(p) for p in written_paths],
                symbols_processed=symbols,
                start_date=start_date,
                end_date=end_date,
                duration_seconds=0.0,  # Set by caller
                manifest_checksum=manifest.checksum,
            )
```

### DuckDB Catalog Safety (MEDIUM Priority Fix)

**Problem:** DROP VIEW then CREATE VIEW leaves readers failing; object cache may hide new partitions.

**Solution:** Use CREATE OR REPLACE VIEW + disable_object_cache on BOTH connections.

```python
def _get_writer_connection(self) -> duckdb.DuckDBPyConnection:
    """Writer-only connection (called under exclusive lock)."""
    conn = duckdb.connect(str(self.catalog_path))
    conn.execute("PRAGMA threads=4")
    conn.execute("PRAGMA memory_limit='4GB'")
    # Disable object cache for writer too (consistent behavior)
    conn.execute("PRAGMA disable_object_cache")
    return conn

def _get_reader_connection(self) -> duckdb.DuckDBPyConnection:
    """Read-only connection for queries."""
    conn = duckdb.connect(str(self.catalog_path), read_only=True)
    # CRITICAL: Disable object cache so new partitions are visible
    conn.execute("PRAGMA disable_object_cache")
    return conn

def _update_catalog(self) -> None:
    """Update catalog views after successful write (under lock).

    MEDIUM Priority Fix: Use CREATE OR REPLACE to avoid reader failures.
    """
    with self._get_writer_connection() as conn:
        daily_path = str(self.storage_path / "daily" / "*.parquet")
        # CREATE OR REPLACE is atomic - no window where view is missing
        conn.execute(f"""
            CREATE OR REPLACE VIEW daily_prices AS
            SELECT * FROM read_parquet('{daily_path}')
        """)

def query_sql(self, sql: str) -> pl.DataFrame:
    """Query via DuckDB with read-only connection.

    Note: disable_object_cache ensures new partitions are visible.
    """
    with self._get_reader_connection() as conn:
        return conn.execute(sql).pl()
```

### Incremental Update with Batched Deduplication (MEDIUM Priority Fix)

**Problem:** Original design did O(N) partition rewrites per symbol - excessive I/O.

**Solution:** Batch all fetches, collect in memory, then single merge per affected year.

```python
def run_incremental_etl(self, symbols: list[str]) -> ETLResult:
    """Incremental ETL with BATCHED deduplication.

    Performance optimization:
    1. Group symbols by last_updated date for batched fetching
    2. Collect ALL new data in memory
    3. Group by year
    4. Single read-merge-write per affected year partition
    """
    with self.manifest_manager.acquire_lock(
        dataset=self.DATASET_ID,
        writer_id=f"etl_incr_{os.getpid()}",
    ) as lock_token:
        etl_progress = self._load_etl_progress()
        symbol_last_dates = etl_progress.symbol_last_dates if etl_progress else {}
        today = date.today()

        # Step 1: Group symbols by last_updated for batched fetching
        date_to_symbols: dict[date, list[str]] = {}
        for symbol in symbols:
            last = symbol_last_dates.get(symbol, self.DEFAULT_START_DATE)
            if last >= today:
                continue
            fetch_start = last + timedelta(days=1)
            date_to_symbols.setdefault(fetch_start, []).append(symbol)

        # Step 2: Fetch in batches, collect ALL new data
        all_new_data: list[pl.DataFrame] = []
        for fetch_start, batch_symbols in date_to_symbols.items():
            batch_df = self.fetcher.get_daily_prices(
                symbols=batch_symbols,
                start_date=fetch_start,
                end_date=today,
            )
            if not batch_df.is_empty():
                all_new_data.append(batch_df)

        if not all_new_data:
            return ETLResult(total_rows=0, ...)

        # Step 3: Concat all new data
        combined_df = pl.concat(all_new_data)

        # Step 4: Group by year, single merge per partition
        affected_years = combined_df["date"].dt.year().unique().to_list()

        # Disk space check for merge (existing + new + temp + manifest)
        # MEDIUM Priority Fix: Explicit sizing for merge operations
        self._check_merge_disk_space(affected_years, combined_df)

        written_paths: list[Path] = []
        for year in affected_years:
            year_df = combined_df.filter(pl.col("date").dt.year() == year)
            partition_path = self._merge_partition_deterministic(
                year, year_df, lock_token
            )
            written_paths.append(partition_path)

        # Update symbol_last_dates for ALL processed symbols
        for symbol in symbols:
            if symbol in [s for batch in date_to_symbols.values() for s in batch]:
                symbol_last_dates[symbol] = today

        # Update manifest and progress...
```

### Deterministic Merge Algorithm (HIGH Priority Fix)

**Problem:** `_merge_partition` lacked algorithm for deterministic union.

**Solution:** Explicit read → concat → sort → dedup → atomic rewrite with checksum alignment.

```python
def _merge_partition_deterministic(
    self,
    year: int,
    new_df: pl.DataFrame,
    lock_token: str,
) -> Path:
    """Deterministic merge: read existing → concat → sort → dedup → atomic rewrite.

    Guarantees:
    - Reruns produce IDENTICAL checksums (deterministic sort order)
    - No duplicates across partition (PRIMARY_KEYS uniqueness)
    - Atomic rewrite with quarantine on failure

    Returns:
        Path to written partition file.
    """
    partition_path = self.storage_path / "daily" / f"{year}.parquet"

    # Step 1: Read existing partition (if exists)
    if partition_path.exists():
        existing_df = pl.read_parquet(partition_path)
        # Concat existing + new
        combined = pl.concat([existing_df, new_df])
    else:
        combined = new_df

    # Step 2: Sort by PRIMARY_KEYS for deterministic order
    combined = combined.sort(self.PRIMARY_KEYS)

    # Step 3: Deduplicate (keep LAST occurrence = new data wins)
    combined = combined.unique(subset=self.PRIMARY_KEYS, keep="last")

    # Step 4: Re-sort after dedup for deterministic output
    combined = combined.sort(self.PRIMARY_KEYS)

    # Step 5: Validate before write
    validation_errors = self._validate_partition(combined, year)
    if validation_errors:
        self._handle_validation_failure(partition_path, combined, validation_errors)
        raise DataQualityError(f"Validation failed: {validation_errors}")

    # Step 6: Atomic rewrite with quarantine on failure
    checksum = self._atomic_write_with_quarantine(combined, partition_path)

    return partition_path
```

## Test Cases (19 Total)

```python
class TestHistoricalETL:
    # From task doc (6 core tests)
    def test_full_etl_pipeline_execution(self): ...
    def test_incremental_updates_append_correctly(self): ...
    def test_duckdb_catalog_reflects_all_tables(self): ...
    def test_partition_pruning_in_queries(self): ...
    def test_atomic_write_no_partial_files(self): ...
    def test_checksum_mismatch_triggers_quarantine(self): ...

    # Added from Codex review (round 1)
    def test_dedup_across_reruns_produces_identical_checksums(self): ...
    def test_multi_year_incremental_handles_boundary(self): ...
    def test_reader_cache_invalidation_sees_new_data(self): ...
    def test_stale_lock_recovery(self): ...
    def test_disk_full_quarantines_temp(self): ...
    def test_manifest_corruption_triggers_rollback(self): ...

    # Added from Codex review (round 2)
    def test_validation_gate_rejects_invalid_data(self): ...
    def test_etl_progress_manifest_separate_from_sync_manifest(self): ...

    # Added from Codex/Gemini review (round 3)
    def test_validation_failure_triggers_quarantine_and_manifest_not_saved(self):
        """Verify validation failure → quarantine → manifest unchanged."""
        ...

    def test_deterministic_incremental_merge_identical_checksum(self):
        """Rerun incremental merge produces identical checksum/file size."""
        ...

    def test_duckdb_view_hot_swap_no_missing_view_errors(self):
        """CREATE OR REPLACE VIEW doesn't cause reader failures."""
        ...

    def test_resume_with_corrupted_progress_manifest(self):
        """Corrupted progress manifest is backed up, fresh start works."""
        ...

    def test_enospc_during_merge_cleans_temp_preserves_manifest(self):
        """ENOSPC during merge: temp cleaned, manifest unchanged."""
        ...
```

### Test Implementation Notes

```python
# test_deterministic_incremental_merge_identical_checksum
def test_deterministic_incremental_merge_identical_checksum(etl, mock_fetcher):
    """Verify reruns produce identical checksums (deterministic sort order)."""
    # Run 1: Initial data
    etl.run_full_etl(symbols=["AAPL"], start_date=..., end_date=...)
    checksum_1 = etl._get_partition_checksum(2024)

    # Run 2: Same data, rerun
    etl.run_incremental_etl(symbols=["AAPL"])
    checksum_2 = etl._get_partition_checksum(2024)

    assert checksum_1 == checksum_2, "Reruns must produce identical checksums"

# test_duckdb_view_hot_swap_no_missing_view_errors
def test_duckdb_view_hot_swap_no_missing_view_errors(etl):
    """Verify CREATE OR REPLACE is atomic (no window where view missing)."""
    import threading
    import time

    errors = []

    def reader_thread():
        for _ in range(100):
            try:
                etl.query_sql("SELECT COUNT(*) FROM daily_prices")
            except duckdb.CatalogException as e:
                errors.append(str(e))
            time.sleep(0.01)

    # Start reader thread
    reader = threading.Thread(target=reader_thread)
    reader.start()

    # Trigger catalog update while reader is active
    etl._update_catalog()

    reader.join()
    assert not errors, f"Reader saw missing view: {errors}"
```

## Storage Schema

```
data/
├── manifests/
│   └── historical_daily.json         # SyncManifest (standard fields)
├── sync_progress/
│   └── historical_daily_progress.json # ETLProgressManifest (per-symbol tracking)
├── historical/
│   └── daily/
│       ├── 2020.parquet
│       ├── 2021.parquet
│       └── ...
├── duckdb/
│   └── historical_catalog.duckdb
└── quarantine/
    └── {timestamp}_{file}/            # Failed writes quarantined here
```

## Dependencies

- `libs/data_providers/unified_fetcher.py` (P4T1.8) - ✅ Complete
- `libs/data_quality/manifest.py` - ✅ Exists (ManifestManager, SyncManifest)
- `libs/data_quality/validation.py` - ✅ Exists (DataValidator)
- `duckdb` - Python package (already installed)

## Files to Create/Modify

| File | Action | Description |
|------|--------|-------------|
| `libs/data_pipeline/historical_etl.py` | CREATE | Main ETL module with all safety patterns |
| `libs/data_pipeline/__init__.py` | MODIFY | Add exports |
| `tests/libs/data_pipeline/test_historical_etl.py` | CREATE | 19 comprehensive tests |
| `docs/CONCEPTS/historical-etl-pipeline.md` | CREATE | Documentation |

## Estimated Effort

| Component | Estimate |
|-----------|----------|
| Core HistoricalETL + lock coupling | 1 day |
| Manifest generation + validation | 0.5 day |
| Atomic writes + quarantine | 0.5 day |
| Incremental update + batched dedup | 0.5 day |
| DuckDB catalog + reader/writer | 0.5 day |
| Progress checkpoints + durability | 0.25 day |
| Tests (19 test cases) | 1.25 day |
| Documentation | 0.25 day |
| **Total** | **4.75 days** |

## Success Criteria

- [ ] Full ETL completes without error
- [ ] Incremental updates work correctly with deduplication
- [ ] DuckDB SQL queries work with partition pruning
- [ ] Atomic writes verified (no partial files)
- [ ] Lock + manifest coupling verified
- [ ] Validation gate rejects invalid data
- [ ] Progress resume works after interruption
- [ ] All 19 test cases pass
- [ ] 85%+ test coverage

## Review History

- **v1.0** - Initial plan
- **v1.1** - Incorporated Codex feedback (round 1):
  - HIGH: Added lock + manifest coupling, complete atomic write pattern, incremental dedup
  - MEDIUM: Added DuckDB safety, error handling, extended tests
  - LOW: Aligned manifest location, added progress checkpoints
- **v1.2** - Incorporated Codex feedback (round 2):
  - HIGH: Separate ETLProgressManifest for per-symbol tracking (avoid SyncManifest schema change)
  - MEDIUM: Explicit manifest generation with all required fields
  - MEDIUM: Added validation gate using DataValidator before commit
  - LOW: Use df.estimated_size() for accurate disk space estimation
- **v1.3** - Incorporated Gemini + Codex feedback (round 3):
  - HIGH: Deterministic merge algorithm (read → concat → sort → dedup → atomic rewrite)
  - HIGH: Complete validation/quarantine coupling with DataValidator + ManifestManager.quarantine_data
  - MEDIUM: Batched incremental ETL (single write per partition instead of O(N))
  - MEDIUM: DuckDB CREATE OR REPLACE VIEW (no reader failures during update)
  - MEDIUM: ETLProgressManifest durability (fsync, backup, corruption recovery)
  - MEDIUM: Explicit disk space sizing formula for merge operations
  - LOW: Batch symbol fetches by last_updated date
  - LOW: Check disk space on data volume (not just manifest volume)
  - Added 5 new tests (total: 19 tests)
