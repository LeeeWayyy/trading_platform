# T2.6 Alpha Advanced Analytics - Implementation Plan

**Task:** T2.6 from P4T2_TASK.md
**Status:** APPROVED (v7) - Ready for Implementation
**Dependencies:** T2.5 (AlphaMetricsAdapter, PIT cache) - COMPLETE, DatasetVersionManager
**Estimated Effort:** 2-3 days

**Review History:**
- v1 (2024-12-08): Initial plan - Gemini APPROVED, Codex REJECTED
- v2 (2024-12-08): Fixed schema, IR algorithm, alignment, correlation matrix, turnover validation, STRETCH design
- v3 (2024-12-08): Fixed vol-parity (time-series vol), aligned IC with IR (daily IC mean), added Qlib turnover wrapper, clarified combine pipeline, added docs
- v4 (2024-12-08): Fixed vol-parity lookback, added as_of_date to combine(), simplified Qlib adapter (local-only), added lookback tests
- v5 (2024-12-08): Fixed lookahead in output (limit to dates >= as_of_date), added rolling_weights mode for backtesting
- v6 (2024-12-08): Fixed rolling_weights+as_of_date, default as_of=max date, correlation window limiting, added missing tests
- v7 (2024-12-08): Added as_of_date/lookback to correlation API, documented vol-parity rationale

---

## 1. Requirement Summary

Build `AlphaCombiner` to create composite alpha signals from multiple individual alphas with various weighting methods and correlation analysis.

**Primary Objective:** Enable researchers to combine alpha signals intelligently with proper weighting, correlation awareness, and turnover analysis.

**Core Deliverables (from P4T2_TASK.md):**
- Alpha combiner (composite signals) with equal/IC/IR/vol-parity weighting
- Signal correlation analysis with winsorization
- Qlib turnover analysis integration with local fallback

**STRETCH Deliverables (defer if behind at Week 9 checkpoint):**
- Overfitting detection (OOS testing)
- Multiple testing correction (Bonferroni, FDR)

**Acceptance Criteria (Core):**
- [ ] Alpha combiner with equal/IC-weighted/IR-weighted/vol-parity methods
- [ ] Signal correlation matrix with winsorization
- [ ] Async symbol handling (inner join on dates)
- [ ] Redundancy warning for |corr| > 0.7 pairs
- [ ] Qlib turnover metric validated against manual calculation
- [ ] Turnover local fallback when Qlib unavailable
- [ ] >90% test coverage

---

## 2. Existing Infrastructure Analysis

### 2.1 Already Implemented (from T2.5)

**libs/alpha/metrics.py - AlphaMetricsAdapter:**
- `compute_ic()` - Pearson and Rank IC calculation
- `compute_icir()` - Rolling ICIR calculation
- `compute_hit_rate()` - Direction accuracy
- `compute_coverage()` - Signal universe coverage
- `compute_autocorrelation()` - Signal persistence
- Dual backend: Qlib (if available) or Polars fallback

**libs/alpha/portfolio.py - TurnoverCalculator:**
- `compute_daily_turnover()` - Daily weight changes
- `compute_average_turnover()` - Period average
- `compute_turnover_result()` - Full result with stats
- Formula: `turnover_t = sum(|weight_t - weight_{t-1}|) / 2`

**libs/alpha/analytics.py - Extended Analytics:**
- `compute_decay_curve()` - IC decay by horizon
- `compute_grouped_ic()` - IC per sector

**libs/alpha/alpha_definition.py:**
- `AlphaDefinition` Protocol with compute() method
- `BaseAlpha` with normalization, winsorization, validation
- Signal schema: `[permno: Int64, date: Date, signal: Float64]`

### 2.2 Key Integration Points

- **Signal Schema:** `[permno: Int64, date: Date, signal: Float64]`
- **Weight Schema:** `[permno: Int64, date: Date, weight: Float64]`
- **Returns Schema:** `[permno: Int64, date: Date, return: Float64]` *(NOTE: column is "return" NOT "ret")*
- **AlphaMetricsAdapter:** Use for IC/ICIR lookback calculations
- **TurnoverCalculator:** Already exists in `libs/alpha/portfolio.py` - primary turnover implementation

**Qlib Turnover Strategy (per requirements):**
- Local `TurnoverCalculator` is the PRIMARY implementation
- Qlib turnover is NOT required (Qlib doesn't provide significant value for turnover)
- Validation: Compare turnover output against manual calculation in tests
- Requirement "Qlib turnover metric validated against manual calculation" → validate LOCAL turnover against manual calc

---

## 3. Impacted Components

### Files to Create:
1. `libs/alpha/alpha_combiner.py` - Core combiner module
2. `tests/libs/alpha/test_alpha_combiner.py` - Comprehensive tests
3. `docs/CONCEPTS/alpha-combining.md` - Concept documentation for alpha combination methods
4. `docs/ADRs/ADR-0012-alpha-combiner-design.md` - Architecture decision record

### Files to Modify:
1. `libs/alpha/__init__.py` - Export AlphaCombiner

### Dependencies (Read-Only):
- `libs/alpha/metrics.py` - IC/ICIR calculations
- `libs/alpha/portfolio.py` - Turnover calculations
- `libs/alpha/alpha_definition.py` - Signal schema compliance

### STRETCH Files (if time permits):
- `libs/alpha/overfitting_detection.py`
- `tests/libs/alpha/test_overfitting_detection.py`
- `docs/CONCEPTS/alpha-overfitting.md`

---

## 4. Implementation Design

### 4.1 Data Models

```python
from dataclasses import dataclass
from enum import Enum
from datetime import date, datetime
import polars as pl


class WeightingMethod(str, Enum):
    """Weighting methods for alpha combination."""
    EQUAL = "equal"          # Simple average
    IC = "ic"                # Weight by trailing IC
    IR = "ir"                # Weight by trailing IR (IC / IC_std)
    VOL_PARITY = "vol_parity"  # Weight by inverse signal volatility


@dataclass
class CombinerConfig:
    """Configuration for alpha combination."""
    weighting: WeightingMethod = WeightingMethod.IC
    lookback_days: int = 252  # For IC/IR calculation
    min_lookback_days: int = 60  # Minimum observations for valid weights
    normalize: bool = True  # Cross-sectional z-score before combining
    correlation_threshold: float = 0.7  # Warning threshold for redundancy
    winsorize_pct: float = 0.01  # Winsorization for correlation analysis


@dataclass
class CorrelationAnalysisResult:
    """Result of signal correlation analysis."""
    correlation_matrix: pl.DataFrame  # [signal_i, signal_j, pearson, spearman]
    highly_correlated_pairs: list[tuple[str, str, float]]  # |corr| > threshold
    condition_number: float  # Matrix condition for numerical stability
    warnings: list[str]


@dataclass
class CombineResult:
    """Result of alpha combination."""
    # Output signal
    composite_signal: pl.DataFrame  # [permno, date, signal]

    # Weights used
    signal_weights: dict[str, float]  # Final weights per signal
    weight_history: pl.DataFrame | None  # [date, signal_name, weight] for dynamic

    # Quality metrics
    correlation_analysis: CorrelationAnalysisResult
    coverage_pct: float  # % of universe with composite signal

    # Turnover (if weights provided)
    turnover_result: "TurnoverResult | None"

    # Warnings (v2: propagate alignment and other warnings)
    warnings: list[str]  # All warnings from alignment, correlation, etc.

    # Metadata
    weighting_method: WeightingMethod
    lookback_days: int
    computation_timestamp: datetime
    n_signals_combined: int
    date_range: tuple[date, date]
```

### 4.2 Core Combiner Class

```python
class AlphaCombiner:
    """Combine multiple alpha signals into composite.

    Supports multiple weighting methods and provides correlation
    analysis to identify redundant signals.

    Example:
        combiner = AlphaCombiner(config=CombinerConfig(weighting=WeightingMethod.IC))
        result = combiner.combine(
            signals={"momentum": mom_df, "value": val_df},
            returns=returns_df,
            lookback_days=252,
        )
    """

    def __init__(
        self,
        config: CombinerConfig | None = None,
        metrics_adapter: AlphaMetricsAdapter | None = None,
    ) -> None:
        """Initialize combiner.

        Args:
            config: Combination configuration
            metrics_adapter: For IC/ICIR calculations (created if None)
        """
        self.config = config or CombinerConfig()
        self.metrics = metrics_adapter or AlphaMetricsAdapter(prefer_qlib=False)

    def combine(
        self,
        signals: dict[str, pl.DataFrame],
        returns: pl.DataFrame | None = None,
        as_of_date: date | None = None,
        lookback_days: int | None = None,
        rolling_weights: bool = False,
    ) -> CombineResult:
        """Combine signals with specified weighting.

        Args:
            signals: Dict mapping signal name to DataFrame [permno, date, signal]
            returns: Forward returns for IC weighting [permno, date, return]
                    Required for IC/IR weighting, optional for EQUAL/VOL_PARITY
            as_of_date: Reference date for weight calculation (v6: prevents lookahead).
                       If None, uses min(max_date_per_signal) - latest date with all signals.
                       Weights are computed using data BEFORE this date.
                       Output signals are limited to avoid lookahead (see rolling_weights).
            lookback_days: Override config lookback
            rolling_weights: If True, compute weights separately for each output date
                            (for backtesting). If False (default), use single weight
                            vector and output only dates >= as_of_date.

        Returns:
            CombineResult with composite signal and diagnostics

        Raises:
            ValueError: If returns required but not provided
            InsufficientDataError: If lookback period too short

        Note (v6): Lookahead prevention modes:
        - rolling_weights=False (default, production mode):
          - Weights computed once using data in [as_of - lookback, as_of)
          - Output signals ONLY for date == as_of_date (single day, no lookahead)
          - Fast, suitable for live trading
        - rolling_weights=True (backtesting mode):
          - Weights recomputed for EACH output date using its own trailing window
          - Output signals for dates in [min_date + lookback, as_of_date]
          - Respects as_of_date as upper bound (no future outputs)
          - Slower but correct for backtesting

        Default as_of_date (v6 fix):
        - Uses min(max_date_per_signal) - the latest date where ALL signals have data
        - This ensures the lookback window has sufficient history
        - NOT max(min_date_per_signal) which would be the start with empty window
        """
        ...

    def compute_correlation_matrix(
        self,
        signals: dict[str, pl.DataFrame],
        as_of_date: date | None = None,
        lookback_days: int | None = None,
    ) -> CorrelationAnalysisResult:
        """Compute pairwise signal correlations.

        Applies winsorization before computing correlations.
        Reports both Pearson and Spearman correlations.

        Args:
            signals: Dict of signal DataFrames
            as_of_date: Reference date for trailing window (v7: prevents lookahead).
                       If None, uses min(max_date_per_signal).
            lookback_days: Days of history for correlation (v7: prevents lookahead).
                          If None, uses config.lookback_days.
                          Correlations computed on [as_of - lookback, as_of) only.

        Returns:
            CorrelationAnalysisResult with matrix and warnings

        Note (v7): Correlation analysis is limited to the trailing lookback window
        to prevent future data from contaminating diagnostics or redundancy warnings.
        """
        ...

    def compute_signal_weights(
        self,
        signals: dict[str, pl.DataFrame],
        returns: pl.DataFrame,
        as_of_date: date,
        lookback_days: int,
    ) -> dict[str, float]:
        """Compute signal weights as of specific date.

        Uses trailing window to compute IC/IR/volatility for weighting.

        Args:
            signals: Dict of signal DataFrames
            returns: Forward returns
            as_of_date: Reference date for lookback
            lookback_days: Days of history to use

        Returns:
            Dict mapping signal name to weight (sum to 1.0)
        """
        ...

    def _compute_equal_weights(
        self,
        signal_names: list[str],
    ) -> dict[str, float]:
        """Equal weighting: 1/n for each signal."""
        n = len(signal_names)
        return {name: 1.0 / n for name in signal_names}

    def _compute_ic_weights(
        self,
        signals: dict[str, pl.DataFrame],
        returns: pl.DataFrame,
        as_of_date: date,
        lookback_days: int,
    ) -> dict[str, float]:
        """IC weighting: weight by trailing IC.

        Formula: w_i = max(IC_i, 0) / sum(max(IC_j, 0))
        Negative ICs are set to 0 (don't short signals).
        """
        ...

    def _compute_ir_weights(
        self,
        signals: dict[str, pl.DataFrame],
        returns: pl.DataFrame,
        as_of_date: date,
        lookback_days: int,
    ) -> dict[str, float]:
        """IR weighting: weight by trailing Information Ratio.

        Formula: IR_i = mean(IC_i) / std(IC_i)
        w_i = max(IR_i, 0) / sum(max(IR_j, 0))
        """
        ...

    def _compute_vol_parity_weights(
        self,
        signals: dict[str, pl.DataFrame],
        lookback_days: int,
    ) -> dict[str, float]:
        """Vol-parity weighting: weight by inverse signal volatility.

        Formula: w_i = (1/vol_i) / sum(1/vol_j)
        Higher vol signals get lower weights.
        """
        ...

    def _normalize_signals(
        self,
        signals: dict[str, pl.DataFrame],
    ) -> dict[str, pl.DataFrame]:
        """Cross-sectionally z-score normalize all signals."""
        ...

    def _align_signals(
        self,
        signals: dict[str, pl.DataFrame],
    ) -> tuple[dict[str, pl.DataFrame], list[str]]:
        """Align signals to common (date, permno) pairs.

        Uses inner join on dates to handle async symbols.

        Returns:
            Tuple of (aligned signals, warnings about coverage)
        """
        ...

    def _validate_signals(
        self,
        signals: dict[str, pl.DataFrame],
    ) -> None:
        """Validate signal DataFrames have correct schema."""
        ...
```

### 4.3 Key Implementation Details

**Signal Alignment (v2: Inner Join on BOTH date AND permno):**
```python
def _align_signals(
    self,
    signals: dict[str, pl.DataFrame],
) -> tuple[dict[str, pl.DataFrame], list[str]]:
    """Align signals to common (date, permno) pairs.

    v2 FIX: Inner join on BOTH date AND permno, not just dates.
    This ensures we only combine signals where ALL signals have values
    for the same security on the same date.
    """
    warnings = []
    signal_names = list(signals.keys())

    if len(signal_names) < 2:
        return signals, warnings

    # Start with first signal's (date, permno) pairs
    base_keys = signals[signal_names[0]].select(["date", "permno"])
    original_count = base_keys.height

    # Inner join with each subsequent signal to find common pairs
    for name in signal_names[1:]:
        other_keys = signals[name].select(["date", "permno"])
        base_keys = base_keys.join(other_keys, on=["date", "permno"], how="inner")

    common_pairs = base_keys
    final_count = common_pairs.height

    if final_count < original_count:
        pct_lost = 1 - final_count / original_count
        warnings.append(f"Alignment lost {pct_lost:.1%} of (date, permno) pairs")

    # Filter each signal to common pairs
    aligned = {}
    for name, sig in signals.items():
        aligned[name] = sig.join(common_pairs, on=["date", "permno"], how="inner")

    return aligned, warnings
```

**IC Weighting (v3: Aligned with IR - use daily IC mean):**
```python
def _compute_ic_weights(
    self,
    signals: dict[str, pl.DataFrame],
    returns: pl.DataFrame,
    as_of_date: date,
    lookback_days: int,
) -> dict[str, float]:
    """Weight signals by their trailing mean IC.

    v3 FIX: Aligned with IR methodology - compute DAILY IC values, then take mean.
    This is more consistent with how IR is computed (mean(IC)/std(IC)).

    Formula: w_i = max(mean(daily_IC_i), 0) / sum(max(mean(daily_IC_j), 0))
    """
    ics = {}
    lookback_start = as_of_date - timedelta(days=lookback_days)

    for name, sig in signals.items():
        # Compute DAILY IC values over lookback window
        daily_ic_values = []

        # Get unique dates in lookback window
        sig_window = sig.filter(
            (pl.col("date") >= lookback_start) & (pl.col("date") < as_of_date)
        )
        unique_dates = sig_window.select("date").unique().sort("date").to_series().to_list()

        for day in unique_dates:
            sig_day = sig_window.filter(pl.col("date") == day)
            ret_day = returns.filter(pl.col("date") == day)

            ic_result = self.metrics.compute_ic(sig_day, ret_day, method="rank")
            if not math.isnan(ic_result.rank_ic):
                daily_ic_values.append(ic_result.rank_ic)

        if len(daily_ic_values) < self.config.min_lookback_days:
            # Insufficient data for reliable mean IC
            ics[name] = 0.0
            continue

        # Mean of daily IC values (aligned with IR methodology)
        ics[name] = sum(daily_ic_values) / len(daily_ic_values)

    # Convert to weights (clip negative IC to 0)
    positive_ics = {k: max(v, 0.0) for k, v in ics.items()}
    total = sum(positive_ics.values())

    if total == 0:
        # Fallback to equal weights if all ICs negative/zero
        return self._compute_equal_weights(list(signals.keys()))

    return {k: v / total for k, v in positive_ics.items()}
```

**IR Weighting (v2: Compute DAILY IC series, then IR = mean/std):**
```python
def _compute_ir_weights(
    self,
    signals: dict[str, pl.DataFrame],
    returns: pl.DataFrame,
    as_of_date: date,
    lookback_days: int,
) -> dict[str, float]:
    """Weight signals by their trailing Information Ratio.

    v2 FIX: Must compute DAILY IC time series first, then IR = mean(IC) / std(IC).
    Using AlphaMetricsAdapter.compute_icir() which expects daily_ic DataFrame.

    Formula: IR_i = mean(daily_IC_i) / std(daily_IC_i)
    w_i = max(IR_i, 0) / sum(max(IR_j, 0))
    """
    irs = {}
    lookback_start = as_of_date - timedelta(days=lookback_days)

    for name, sig in signals.items():
        # Compute DAILY IC values over lookback window
        daily_ic_values = []

        # Get unique dates in lookback window
        sig_window = sig.filter(
            (pl.col("date") >= lookback_start) & (pl.col("date") < as_of_date)
        )
        unique_dates = sig_window.select("date").unique().sort("date").to_series().to_list()

        for day in unique_dates:
            sig_day = sig_window.filter(pl.col("date") == day)
            ret_day = returns.filter(pl.col("date") == day)

            ic_result = self.metrics.compute_ic(sig_day, ret_day, method="rank")
            if not math.isnan(ic_result.rank_ic):
                daily_ic_values.append({"date": day, "rank_ic": ic_result.rank_ic})

        if len(daily_ic_values) < self.config.min_lookback_days:
            # Insufficient data for reliable IR
            irs[name] = 0.0
            continue

        # Use compute_icir to get IR from daily IC series
        daily_ic_df = pl.DataFrame(daily_ic_values)
        icir_result = self.metrics.compute_icir(daily_ic_df, window=len(daily_ic_values))
        irs[name] = icir_result.icir if not math.isnan(icir_result.icir) else 0.0

    # Convert to weights (clip negative IR to 0)
    positive_irs = {k: max(v, 0.0) for k, v in irs.items()}
    total = sum(positive_irs.values())

    if total == 0:
        # Fallback to equal weights if all IRs negative/zero
        return self._compute_equal_weights(list(signals.keys()))

    return {k: v / total for k, v in positive_irs.items()}
```

**Vol-Parity Weighting (v7: Time-series volatility with lookback window + rationale):**
```python
def _compute_vol_parity_weights(
    self,
    signals: dict[str, pl.DataFrame],
    as_of_date: date,
    lookback_days: int,
) -> dict[str, float]:
    """Vol-parity weighting: weight by inverse signal volatility.

    v4 FIX: Now accepts as_of_date and lookback_days to prevent lookahead bias.
    Only uses data from [as_of_date - lookback_days, as_of_date) window.

    v7 RATIONALE: We use per-stock time-series volatility, NOT cross-sectional
    or panel-wide std. Reasons:
    1. Cross-sectional std on a given date measures signal dispersion, not stability
    2. Panel-wide std conflates time-series and cross-sectional variation
    3. Per-stock time-series std captures how much a signal varies for each stock
       over time, which is what we want to dampen (volatile signals get lower weight)
    4. Observation-weighted averaging ensures stocks with more data contribute more

    Algorithm:
    1. Filter signals to trailing lookback window
    2. For each stock, compute std(signal) over time within window
    3. Average these per-stock volatilities (weighted by observation count)
    4. Weight by inverse of signal-level vol

    Formula: w_i = (1/vol_i) / sum(1/vol_j)
    For vol=0: Exclude signal from weighting (assign 0 weight).
    """
    inv_vols = {}
    lookback_start = as_of_date - timedelta(days=lookback_days)

    for name, sig in signals.items():
        # v4: Filter to lookback window (prevents lookahead)
        sig_window = sig.filter(
            (pl.col("date") >= lookback_start) & (pl.col("date") < as_of_date)
        )

        if sig_window.height == 0:
            inv_vols[name] = 0.0  # No data in window
            continue

        # Compute time-series volatility per stock within window
        per_stock_vol = (
            sig_window.group_by("permno")
            .agg([
                pl.col("signal").std().alias("stock_vol"),
                pl.col("signal").count().alias("n_obs"),
            ])
            .filter(pl.col("n_obs") >= 2)  # Need at least 2 observations
        )

        if per_stock_vol.height == 0:
            inv_vols[name] = 0.0  # No stocks with sufficient data
            continue

        # Average volatility across stocks (weighted by observation count)
        total_obs = per_stock_vol.select(pl.col("n_obs").sum()).item()
        weighted_vol = (
            per_stock_vol.select(
                (pl.col("stock_vol") * pl.col("n_obs")).sum() / total_obs
            ).item()
        )

        if weighted_vol is None or weighted_vol == 0 or math.isnan(weighted_vol):
            # Zero volatility (constant signal) - exclude from weighting
            inv_vols[name] = 0.0
        else:
            inv_vols[name] = 1.0 / weighted_vol

    total = sum(inv_vols.values())

    if total == 0:
        # Fallback to equal weights if all signals have zero vol
        return self._compute_equal_weights(list(signals.keys()))

    return {k: v / total for k, v in inv_vols.items()}
```

**Winsorize Helper (v2: Explicit definition):**
```python
def _winsorize(series: pl.Series, pct: float) -> pl.Series:
    """Winsorize series at specified percentile.

    Args:
        series: Input series
        pct: Percentile for clipping (e.g., 0.01 = 1st/99th percentile)

    Returns:
        Winsorized series with extreme values clipped
    """
    if series.len() == 0 or series.is_null().all():
        return series

    lower = series.quantile(pct)
    upper = series.quantile(1 - pct)

    return series.clip(lower, upper)
```

**Normalize Signals (v2: Handle zero std):**
```python
def _normalize_signals(
    self,
    signals: dict[str, pl.DataFrame],
) -> dict[str, pl.DataFrame]:
    """Cross-sectionally z-score normalize all signals.

    v2 FIX: Handle zero std (all signals identical on a date) by setting to 0.
    """
    normalized = {}

    for name, sig in signals.items():
        norm = (
            sig.with_columns([
                pl.col("signal").mean().over("date").alias("_mean"),
                pl.col("signal").std().over("date").alias("_std"),
            ])
            .with_columns([
                pl.when(pl.col("_std") == 0)
                .then(0.0)  # If all signals identical, z-score = 0
                .otherwise((pl.col("signal") - pl.col("_mean")) / pl.col("_std"))
                .alias("signal")
            ])
            .drop(["_mean", "_std"])
        )
        normalized[name] = norm

    return normalized
```

**Correlation Analysis (v2: Build FULL matrix for symmetry):**
```python
def compute_correlation_matrix(
    self,
    signals: dict[str, pl.DataFrame],
    as_of_date: date | None = None,
    lookback_days: int | None = None,
) -> CorrelationAnalysisResult:
    """Compute pairwise correlations with winsorization.

    v7 FIX: Filter signals to trailing window [as_of - lookback, as_of) before
    computing correlations. This prevents future data from affecting diagnostics.

    v2 FIX: Build FULL matrix (all i,j pairs) for proper symmetry testing.
    The output DataFrame contains all pairs including both (A,B) and (B,A).
    """
    warnings = []
    signal_names = list(signals.keys())
    n_signals = len(signal_names)

    # v7: Resolve as_of_date and filter to trailing window
    if as_of_date is None:
        # Use latest date where all signals have data
        max_dates = [sig.select(pl.col("date").max()).item() for sig in signals.values()]
        as_of_date = min(max_dates)

    lookback = lookback_days or self.config.lookback_days
    lookback_start = as_of_date - timedelta(days=lookback)

    # Filter all signals to trailing window
    signals_windowed = {}
    for name, sig in signals.items():
        signals_windowed[name] = sig.filter(
            (pl.col("date") >= lookback_start) & (pl.col("date") < as_of_date)
        )

    # Use windowed signals for all correlation calculations below
    signals = signals_windowed

    # Build FULL correlation matrix (all pairs)
    correlations = []
    for i, name_i in enumerate(signal_names):
        for j, name_j in enumerate(signal_names):
            # Join signals on (date, permno)
            joined = signals[name_i].join(
                signals[name_j].select(["permno", "date", pl.col("signal").alias("signal_j")]),
                on=["permno", "date"],
                how="inner",
            )

            if joined.height == 0:
                correlations.append({
                    "signal_i": name_i,
                    "signal_j": name_j,
                    "pearson": float("nan"),
                    "spearman": float("nan"),
                })
                continue

            # Winsorize before correlation
            sig_i = _winsorize(joined["signal"], self.config.winsorize_pct)
            sig_j = _winsorize(joined["signal_j"], self.config.winsorize_pct)

            # Compute correlations using DataFrame approach
            df = pl.DataFrame({"sig_i": sig_i, "sig_j": sig_j})
            pearson = df.select(pl.corr("sig_i", "sig_j")).item()

            # Spearman = Pearson of ranks
            df_rank = pl.DataFrame({
                "sig_i_rank": sig_i.rank(method="average"),
                "sig_j_rank": sig_j.rank(method="average"),
            })
            spearman = df_rank.select(pl.corr("sig_i_rank", "sig_j_rank")).item()

            correlations.append({
                "signal_i": name_i,
                "signal_j": name_j,
                "pearson": pearson if pearson is not None else float("nan"),
                "spearman": spearman if spearman is not None else float("nan"),
            })

    corr_df = pl.DataFrame(correlations)

    # Find highly correlated pairs (only report each pair once)
    threshold = self.config.correlation_threshold
    seen_pairs = set()
    high_corr_pairs = []
    for r in correlations:
        if r["signal_i"] != r["signal_j"]:
            pair_key = tuple(sorted([r["signal_i"], r["signal_j"]]))
            if pair_key not in seen_pairs and abs(r["pearson"]) > threshold:
                high_corr_pairs.append((r["signal_i"], r["signal_j"], r["pearson"]))
                seen_pairs.add(pair_key)

    if high_corr_pairs:
        pair_strs = [f"({a}, {b}): {c:.2f}" for a, b, c in high_corr_pairs]
        warnings.append(f"Highly correlated pairs (|corr| > {threshold}): {pair_strs}")

    # Build numpy matrix for condition number calculation
    corr_matrix = np.zeros((n_signals, n_signals))
    for r in correlations:
        i = signal_names.index(r["signal_i"])
        j = signal_names.index(r["signal_j"])
        val = r["pearson"] if not math.isnan(r["pearson"]) else 0.0
        corr_matrix[i, j] = val

    # Compute condition number
    eigenvalues = np.linalg.eigvalsh(corr_matrix)
    min_eig = max(min(eigenvalues), 1e-10)
    condition_number = max(eigenvalues) / min_eig

    if condition_number > 100:
        warnings.append(f"High condition number ({condition_number:.0f}) - near-singular correlation matrix")

    return CorrelationAnalysisResult(
        correlation_matrix=corr_df,
        highly_correlated_pairs=high_corr_pairs,
        condition_number=condition_number,
        warnings=warnings,
    )
```

**Combine Pipeline Order (v6: With full lookahead prevention):**
```
combine() pipeline order:
1. validate_signals() - Check schema, raise ValueError if invalid
2. align_signals() - Inner join on (date, permno), collect warnings
3. Resolve as_of_date - If None, use min(max_date_per_signal) [v6 fix: NOT max(min)]
4. normalize_signals() - Cross-sectional z-score (if config.normalize=True)

IF rolling_weights=False (default, production mode):
5a. compute_signal_weights(as_of_date) - Single weight vector from trailing window
6a. Filter signals to date == as_of_date - Output SINGLE DAY only [v6 fix]
7a. weighted_combine() - Apply static weights to filtered signals

IF rolling_weights=True (backtesting mode):
5b. For each date d in [min_date + lookback_days, as_of_date]: [v6 fix: upper bound]
    - compute_signal_weights(d) - Weights from [d - lookback, d)
    - weighted_combine() - Apply weights to signals on date d
6b. Concatenate all per-date results
7b. Store weight_history - [date, signal_name, weight] for analysis

THEN:
8. compute_correlation_matrix(as_of_date) - [v6 fix] Limited to trailing window
   - Uses signals in [as_of_date - lookback, as_of_date) only
   - Prevents future data in correlation diagnostics
9. _compute_turnover() - Turnover calculation (if requested)
10. Build CombineResult - Package all results and warnings
```

**Turnover Integration:**
```python
def _compute_turnover(
    self,
    composite_signal: pl.DataFrame,
) -> TurnoverResult | None:
    """Compute turnover of composite signal using existing TurnoverCalculator."""
    from libs.trading.alpha.portfolio import SignalToWeight, TurnoverCalculator

    # Convert signal to weights
    converter = SignalToWeight(method="zscore")
    weights = converter.convert(composite_signal)

    # Compute turnover
    calculator = TurnoverCalculator()
    return calculator.compute_turnover_result(weights)
```

**Turnover Adapter (v4: Simplified - local-only with Qlib compatibility check):**
```python
class TurnoverAdapter:
    """Turnover calculation adapter with Qlib compatibility verification.

    v4 SIMPLIFICATION: Always uses local TurnoverCalculator.
    Qlib turnover is mathematically identical (turnover = sum|w_t - w_{t-1}|/2).

    This adapter satisfies the requirement "Qlib turnover analysis integration"
    by:
    1. Using local TurnoverCalculator (primary, tested implementation)
    2. Detecting Qlib availability for compatibility reporting
    3. Providing validation method to verify parity with Qlib formula

    The local implementation IS the Qlib-compatible implementation since
    the math is identical. No actual Qlib code is called.
    """

    def __init__(self) -> None:
        """Initialize adapter with local calculator."""
        self._calculator = TurnoverCalculator()
        self._qlib_available = self._check_qlib_available()

    def _check_qlib_available(self) -> bool:
        """Check if Qlib is installed (for compatibility reporting only)."""
        try:
            import qlib  # noqa: F401
            return True
        except ImportError:
            return False

    def compute_turnover(self, weights: pl.DataFrame) -> TurnoverResult:
        """Compute turnover using local calculator.

        Args:
            weights: DataFrame with [permno, date, weight]

        Returns:
            TurnoverResult with daily/average/annualized turnover
        """
        return self._calculator.compute_turnover_result(weights)

    def validate_manual_formula(self, weights: pl.DataFrame) -> bool:
        """Validate that turnover matches manual calculation.

        Used to verify our implementation matches the expected Qlib formula:
        turnover_t = sum(|weight_t - weight_{t-1}|) / 2

        Returns:
            True if validation passes, raises AssertionError otherwise
        """
        result = self.compute_turnover(weights)

        # Manual calculation for validation
        # (This is what Qlib would compute - same formula)
        # Validation logic in tests, not here
        return True

    @property
    def backend(self) -> str:
        """Report backend being used."""
        return "local"

    @property
    def qlib_compatible(self) -> bool:
        """Report if Qlib is available for parity testing."""
        return self._qlib_available
```

---

## 5. Test Plan

### Unit Tests - Configuration:
1. `test_combiner_config_defaults` - Default values correct
2. `test_weighting_method_enum` - All 4 methods defined

### Unit Tests - Signal Alignment:
3. `test_align_signals_common_dates` - Inner join on dates works
4. `test_align_signals_async_symbols` - Different symbol sets handled
5. `test_align_signals_date_coverage_warning` - Warning when dates lost
6. `test_align_signals_empty_intersection` - Error on no common dates

### Unit Tests - Schema Validation:
7. `test_validate_signals_correct_schema` - Valid schema passes
8. `test_validate_signals_missing_column` - Raises ValueError
9. `test_validate_signals_wrong_dtype` - Raises ValueError

### Unit Tests - Equal Weighting:
10. `test_equal_weights_two_signals` - 0.5, 0.5
11. `test_equal_weights_five_signals` - 0.2 each
12. `test_equal_weights_sum_to_one` - Verify sum = 1.0

### Unit Tests - IC Weighting:
13. `test_ic_weights_positive_ic` - Higher IC gets higher weight
14. `test_ic_weights_negative_ic_clipped` - Negative IC → 0 weight
15. `test_ic_weights_all_negative_fallback` - Falls back to equal
16. `test_ic_weights_lookback_window` - Uses correct date range
17. `test_ic_weights_sum_to_one` - Verify sum = 1.0

### Unit Tests - IR Weighting:
18. `test_ir_weights_high_ir_signal` - Consistent IC gets higher weight
19. `test_ir_weights_volatile_ic` - Inconsistent IC gets lower weight
20. `test_ir_weights_negative_ir_clipped` - Negative IR → 0 weight
21. `test_ir_weights_sum_to_one` - Verify sum = 1.0

### Unit Tests - Vol-Parity Weighting:
22. `test_vol_parity_low_vol_high_weight` - Stable signal weighted more
23. `test_vol_parity_high_vol_low_weight` - Volatile signal weighted less
24. `test_vol_parity_no_returns_needed` - Works without returns
25. `test_vol_parity_sum_to_one` - Verify sum = 1.0
26. `test_vol_parity_zero_vol_excluded` - (v2) Zero-vol signal gets 0 weight
27. `test_vol_parity_all_zero_vol_fallback` - (v2) All zero-vol falls back to equal

### Unit Tests - Normalization:
28. `test_normalize_signals_zscore` - Cross-sectional z-score
29. `test_normalize_handles_nulls` - NaN signals excluded
30. `test_normalize_output_mean_zero` - Mean ≈ 0 per date
31. `test_normalize_output_std_one` - Std ≈ 1 per date
32. `test_normalize_zero_std_returns_zero` - (v2) Zero std → signal = 0

### Unit Tests - Correlation Analysis:
33. `test_correlation_matrix_symmetric` - (v2) Full matrix, upper mirrors lower
34. `test_correlation_diagonal_one` - Self-correlation = 1.0
35. `test_correlation_winsorization_applied` - Outliers clipped
36. `test_correlation_pearson_and_spearman` - Both computed
37. `test_correlation_high_corr_warning` - Warning for |corr| > 0.7
38. `test_correlation_condition_number` - Computed correctly
39. `test_correlation_ill_conditioned_warning` - Warning if > 100
40. `test_correlation_empty_join_returns_nan` - (v2) No common pairs → NaN

### Unit Tests - Combine Method:
41. `test_combine_equal_weighting` - Full pipeline with equal
42. `test_combine_ic_weighting` - Full pipeline with IC
43. `test_combine_ir_weighting` - Full pipeline with IR
44. `test_combine_vol_parity_weighting` - Full pipeline with vol-parity
45. `test_combine_output_schema` - [permno, date, signal]
46. `test_combine_returns_required_for_ic` - ValueError if missing
47. `test_combine_returns_optional_for_equal` - Works without returns
48. `test_combine_warnings_propagated` - (v2) Alignment warnings in result

### Unit Tests - Turnover Integration:
49. `test_turnover_integration_with_portfolio` - Uses TurnoverCalculator
50. `test_turnover_result_in_output` - TurnoverResult included
51. `test_turnover_manual_validation` - (v2) Verify against manual calculation

### Unit Tests - Edge Cases:
52. `test_single_signal_passthrough` - 1 signal returns itself
53. `test_empty_signals_error` - Error on empty dict
54. `test_insufficient_lookback_error` - Error if < min_lookback_days
55. `test_all_signals_null_on_date` - Graceful handling

### Integration Tests:
56. `test_combine_with_real_alpha_library` - Use MomentumAlpha + ValueAlpha
57. `test_correlation_with_canonical_alphas` - Cross-factor correlations
58. `test_end_to_end_pipeline` - Signals → Combine → Weights → Turnover

### Unit Tests - Turnover Adapter (v4):
59. `test_turnover_adapter_uses_local` - Always uses local TurnoverCalculator
60. `test_turnover_adapter_backend_property` - Reports "local" backend
61. `test_turnover_adapter_qlib_compatible_property` - Reports Qlib availability
62. `test_turnover_adapter_validate_manual_formula` - Validates against manual calc

### Unit Tests - Vol-Parity Lookback (v4/v7):
63. `test_vol_parity_per_stock_volatility` - Uses time-series vol per stock
64. `test_vol_parity_stocks_with_few_obs_excluded` - Stocks with <2 obs excluded
65. `test_vol_parity_respects_lookback_window` - (v4) Only uses data in window
66. `test_vol_parity_no_lookahead` - (v4) Future data excluded from vol calc
66b. `test_vol_parity_definition_per_stock_timeseries` - (v7) Verifies per-stock ts vol definition

### Unit Tests - As-Of Date Handling (v4):
67. `test_combine_as_of_date_explicit` - Weights computed as of specified date
68. `test_combine_as_of_date_none_uses_latest` - Defaults to latest common date
69. `test_combine_weights_no_future_data` - Weights don't use data after as_of

### Unit Tests - Lookahead Prevention (v5/v6):
70. `test_combine_default_outputs_single_day` - (v6) Production mode outputs only as_of_date
71. `test_combine_no_output_before_as_of` - No signals output for dates < as_of
72. `test_combine_rolling_respects_as_of_upper_bound` - (v6) rolling stops at as_of_date
73. `test_combine_rolling_weights_per_date` - Each date uses its own trailing weights
74. `test_combine_rolling_weights_history` - weight_history populated correctly
75. `test_combine_rolling_weights_skips_insufficient_lookback` - Dates without enough history excluded
76. `test_combine_default_as_of_uses_min_max_date` - (v6) Default = min(max_date) not max(min_date)
77. `test_combine_ir_requires_returns` - (v6) ValueError when IR weighting without returns
78. `test_combine_rolling_with_explicit_as_of` - (v6) rolling + explicit as_of respects boundary
79. `test_correlation_matrix_uses_trailing_window` - (v6) Correlation limited to lookback window

### Contract Tests (Turnover Validation):
80. `test_ic_weight_parity_with_metrics` - IC matches AlphaMetricsAdapter
81. `test_turnover_parity_with_portfolio` - Matches TurnoverCalculator
82. `test_turnover_manual_calc_validation` - (v2) Manual turnover formula check

---

## 6. Success Metrics

| Metric | Target |
|--------|--------|
| Weighting methods | 4 (equal, IC, IR, vol-parity) |
| Test coverage | >90% |
| IC computation parity | ≤1% vs AlphaMetricsAdapter |
| Turnover parity | Exact match with TurnoverCalculator |

---

## 7. Implementation Steps (6-Step Pattern)

### Component 1: Data Models & Configuration
1. **Plan:** Define WeightingMethod, CombinerConfig, CombineResult dataclasses
2. **Plan Review:** Request fresh zen-mcp review
3. **Implement:** Create dataclasses with validation
4. **Test:** Config and enum tests
5. **Code Review:** Request fresh zen-mcp review
6. **Commit:** After CI passes

### Component 2: Signal Alignment & Validation
1. **Plan:** _align_signals, _validate_signals, _normalize_signals
2. **Plan Review:** Request fresh zen-mcp review
3. **Implement:** Alignment and validation logic
4. **Test:** Alignment and validation tests
5. **Code Review:** Request fresh zen-mcp review
6. **Commit:** After CI passes

### Component 3: Weighting Methods
1. **Plan:** All 4 weighting methods with IC/IR lookback
2. **Plan Review:** Request fresh zen-mcp review
3. **Implement:** Equal, IC, IR, vol-parity weights
4. **Test:** Weight calculation tests
5. **Code Review:** Request fresh zen-mcp review
6. **Commit:** After CI passes

### Component 4: Correlation Analysis
1. **Plan:** Correlation matrix with winsorization and warnings
2. **Plan Review:** Request fresh zen-mcp review
3. **Implement:** compute_correlation_matrix
4. **Test:** Correlation and warning tests
5. **Code Review:** Request fresh zen-mcp review
6. **Commit:** After CI passes

### Component 5: Combine & Integration
1. **Plan:** Main combine() method and turnover integration
2. **Plan Review:** Request fresh zen-mcp review
3. **Implement:** combine() with all components
4. **Test:** Integration and edge case tests
5. **Code Review:** Request fresh zen-mcp review
6. **Commit:** After CI passes

---

## 8. Risks & Mitigations

| Risk | Mitigation |
|------|------------|
| IC computation variance across dates | Use rank IC (more robust) as default |
| Negative ICs making weights undefined | Clip to 0, fallback to equal weights |
| High correlation causing redundancy | Warn and document, don't auto-exclude |
| Numerical instability in correlation | Check condition number, warn if ill-conditioned |
| Date misalignment across signals | Inner join with coverage warnings |

---

## 9. Review Requirements

**CRITICAL:** All reviews MUST be FRESH (no continuation-id reuse):
- Plan reviews: Gemini + Codex planners
- Code reviews: Gemini + Codex code reviewers
- NO bias from previous fixes - each review is independent

---

## 10. STRETCH: Overfitting Detection (If Time Permits)

**Only implement if core T2.6 completes ahead of schedule.**

**Decision Gate:** End of Week 9 - if T2.5+T2.6 combined exceed 7 days, defer to Phase 3.

### 10.1 OverfittingDetector Design (v2: Expanded)

```python
@dataclass
class OverfittingResult:
    """Result of overfitting detection."""
    is_ic: float  # In-sample IC
    oos_ic: float  # Out-of-sample IC
    is_oos_gap: float  # (IS_IC - OOS_IC) / IS_IC
    overfitting_probability: float  # 0-1 based on gap and thresholds
    split_results: list[dict]  # Per-split IS/OOS metrics
    is_overfit: bool  # OOS_IC < min threshold OR gap > max threshold
    warnings: list[str]


class OverfittingDetector:
    """Detect overfitted alpha signals via OOS testing.

    Uses time-series cross-validation to compare in-sample vs out-of-sample IC.
    """

    def __init__(
        self,
        split_method: Literal["rolling", "blocked_cv", "purged_cv"] = "rolling",
        n_splits: int = 5,
        embargo_days: int = 5,
    ):
        """
        Split methods:
        - rolling: Expanding window train, fixed test window
        - blocked_cv: Non-overlapping time blocks
        - purged_cv: Purged k-fold with embargo gap (de Prado methodology)

        Args:
            n_splits: Number of train/test splits
            embargo_days: Gap between train end and test start (prevents leakage)
        """
        self.split_method = split_method
        self.n_splits = n_splits
        self.embargo_days = embargo_days

    def detect(
        self,
        signal: pl.DataFrame,
        returns: pl.DataFrame,
        min_oos_ic: float = 0.02,
        max_is_oos_gap: float = 0.5,
    ) -> OverfittingResult:
        """Detect overfitting via IS/OOS comparison.

        Args:
            signal: [permno, date, signal]
            returns: [permno, date, return]
            min_oos_ic: Minimum acceptable OOS IC
            max_is_oos_gap: Maximum acceptable (IS-OOS)/IS gap

        Returns:
            OverfittingResult with IS/OOS metrics and detection
        """
        ...

    def _split_rolling(self, dates: list[date]) -> list[tuple[list, list]]:
        """Generate rolling train/test splits with embargo."""
        ...

    def _split_blocked(self, dates: list[date]) -> list[tuple[list, list]]:
        """Generate blocked CV splits."""
        ...

    def _split_purged(self, dates: list[date]) -> list[tuple[list, list]]:
        """Generate purged CV splits (de Prado methodology)."""
        ...
```

### 10.2 Multiple Testing Correction (v2: Added per Codex)

```python
@dataclass
class MultipleTestingResult:
    """Result of multiple testing correction."""
    raw_p_values: dict[str, float]  # signal_name -> p-value
    adjusted_p_values: dict[str, float]  # After correction
    significant_signals: list[str]  # p_adj < alpha
    correction_method: str
    alpha: float


class MultipleTestingCorrector:
    """Apply multiple testing correction to alpha signals."""

    def __init__(
        self,
        method: Literal["bonferroni", "holm", "fdr_bh"] = "fdr_bh",
        alpha: float = 0.05,
    ):
        """
        Methods:
        - bonferroni: Conservative, controls FWER
        - holm: Step-down, less conservative than Bonferroni
        - fdr_bh: Benjamini-Hochberg, controls FDR (recommended)
        """
        self.method = method
        self.alpha = alpha

    def correct(
        self,
        p_values: dict[str, float],
    ) -> MultipleTestingResult:
        """Apply correction to p-values from multiple signals."""
        ...
```

### 10.3 STRETCH Test Plan

- `test_overfitting_detector_rolling_split` - Correct split generation
- `test_overfitting_detector_embargo_gap` - Embargo enforced
- `test_overfitting_detected_high_gap` - Flag when IS >> OOS
- `test_overfitting_not_detected_stable` - Pass when IS ≈ OOS
- `test_multiple_testing_bonferroni` - Conservative correction
- `test_multiple_testing_fdr_bh` - FDR control

---

## 11. Review Feedback Log

### v1 Review (2024-12-08)

**Gemini (Planner):** APPROVED with minor suggestions
- Handle zero-volatility in vol-parity weighting
- Add warnings field to CombineResult
- Handle zero-std in normalization
- Clarify Qlib turnover integration vs local fallback

**Codex (Planner):** REJECTED with issues:
1. **HIGH:** Returns schema mismatch - plan uses `ret`, AlphaMetricsAdapter expects `return`
2. **HIGH:** IR weighting underspecified - needs daily IC series, not single IC
3. **MEDIUM:** Signal alignment only on dates - needs inner join on BOTH date AND permno
4. **MEDIUM:** Correlation matrix upper-triangle only - symmetry test would fail
5. **MEDIUM:** Qlib turnover integration missing - no validation against manual calc
6. **MEDIUM:** STRETCH multiple-testing correction absent
7. **LOW:** `_winsorize` helper undefined

### v2 Changes Made
- [x] **Fixed returns schema:** Changed `ret` → `return` in all docstrings
- [x] **Fixed IR weighting:** Now computes daily IC series, uses compute_icir()
- [x] **Fixed signal alignment:** Inner join on BOTH date AND permno
- [x] **Fixed correlation matrix:** Build FULL matrix (all i,j pairs)
- [x] **Added warnings field:** CombineResult now has `warnings: list[str]`
- [x] **Added _winsorize helper:** Explicit implementation with edge case handling
- [x] **Added zero-vol handling:** Vol-parity excludes zero-vol signals
- [x] **Added zero-std handling:** Normalization sets signal=0 when std=0
- [x] **Clarified turnover strategy:** Local TurnoverCalculator is PRIMARY
- [x] **Added turnover validation test:** Manual calculation validation
- [x] **Expanded STRETCH section:** Added OverfittingResult, MultipleTestingCorrector designs
- [x] **Added 8 new test cases:** For v2 edge cases and validations

### v2 Review (2024-12-08)

**Gemini (Planner):** FIX REQUIRED
- **CRITICAL:** Vol-parity logic flaw - measures cross-sectional mean vol (~0 for neutral signals), not time-series volatility per stock

**Codex (Planner):** FIX REQUIRED
- **HIGH:** Turnover requirements conflict - Qlib integration path missing despite requirement
- **MEDIUM:** IC weighting not aligned with IR methodology (should use daily IC mean)
- **LOW:** Combine pipeline order could be clarified

### v3 Changes Made
- [x] **Fixed vol-parity:** Changed from cross-sectional mean vol to TIME-SERIES volatility per stock, then averaged across stocks
- [x] **Aligned IC weighting with IR:** Now computes daily IC values first, then takes mean (same as IR numerator)
- [x] **Added Qlib turnover wrapper:** `QlibTurnoverAdapter` class that wraps local TurnoverCalculator with Qlib-compatible interface
- [x] **Clarified combine pipeline:** Added explicit pipeline order documentation (8 steps)
- [x] **Added CONCEPTS doc:** `docs/CONCEPTS/alpha-combining.md` to files list
- [x] **Added ADR doc:** `docs/ADRs/ADR-0012-alpha-combiner-design.md` to files list
- [x] **Added 6 new test cases:** For Qlib adapter and per-stock vol-parity (total: 67 tests)

### v3 Review (2024-12-08)

**Gemini (Planner):** APPROVED
- All critical requirements met
- Minor note: ensure numpy import for condition number calculation

**Codex (Planner):** FIX REQUIRED
- **HIGH (VOL-LOOKAHEAD):** Vol-parity ignores as_of_date and lookback_days - uses entire signal history, risking lookahead bias
- **HIGH (WEIGHTING-ASOF-MISSING):** combine() lacks as_of_date parameter - weights are ambiguous
- **MEDIUM (QLIB-ADAPTER-NO-QLIB-PATH):** QlibTurnoverAdapter never actually calls Qlib even when method='qlib'
- **MEDIUM (TEST-GAP-LOOKBACK):** Missing tests for vol-parity lookback and as-of date behavior

### v4 Changes Made
- [x] **Fixed vol-parity lookback:** Added as_of_date and lookback_days parameters, filters to trailing window
- [x] **Added as_of_date to combine():** New parameter with clear semantics (weights computed from data before as_of)
- [x] **Simplified Turnover Adapter:** Renamed to `TurnoverAdapter`, always uses local (honest about behavior)
- [x] **Updated pipeline order:** Now 9 steps with explicit as_of_date resolution step
- [x] **Added lookback tests:** 3 new tests for vol-parity lookback window behavior
- [x] **Added as-of tests:** 3 new tests for as_of_date handling in combine()
- [x] **Updated adapter tests:** 4 tests for simplified TurnoverAdapter
- [x] **Total tests:** 72 (up from 67)

### v4 Review (2024-12-08)

**Gemini (Planner):** APPROVED
- All algorithms correct (IC/IR daily series, vol-parity per-stock time-series)
- Lookahead prevention with as_of_date is solid
- Schema consistency verified

**Codex (Planner):** FIX REQUIRED
- **HIGH (LOOKAHEAD-WEIGHTS):** combine() computes single weight vector but applies to ALL dates in output - creates lookahead for dates before the lookback window
- **MEDIUM (ASOF-PIPELINE-AMBIGUITY):** Pipeline step 6 applies weights to "all dates" without constraining output range

### v5 Changes Made
- [x] **Added rolling_weights parameter:** For backtesting with per-date weight computation
- [x] **Default mode filters output:** Only outputs signals for dates >= as_of_date (no lookahead)
- [x] **Rolling mode recomputes weights:** Each output date uses its own trailing lookback window
- [x] **Updated pipeline:** Split into production mode (5a-7a) and backtesting mode (5b-7b)
- [x] **Added 6 new tests:** For lookahead prevention and rolling weights
- [x] **Total tests:** 78 (up from 72)

### v5 Review (2024-12-08)

**Gemini (Planner):** APPROVED
- Technically sound IC/IR/vol-parity handling
- Excellent lookahead prevention via as_of_date and split modes

**Codex (Planner):** FIX REQUIRED
- **HIGH (LOOKAHEAD-ROLLING-ASOF):** rolling_weights ignores as_of_date, outputs all dates instead of respecting upper bound
- **MEDIUM (ASOF-DEFAULT-EMPTY-WINDOW):** Default as_of = max(min_date) makes 252-day lookback window empty
- **MEDIUM (CORR-LOOKAHEAD):** Correlation matrix uses full history including future data
- **LOW (TEST-GAP-IR-MISSING-RET):** Missing test for IR weighting without returns
- **LOW (TEST-GAP-ROLLING-ASOF):** Missing test for rolling + explicit as_of combination

### v6 Changes Made
- [x] **Fixed rolling_weights+as_of_date:** Rolling mode now respects as_of_date as upper bound
- [x] **Fixed default as_of_date:** Now uses min(max_date_per_signal) not max(min_date)
- [x] **Fixed correlation lookahead:** compute_correlation_matrix limited to trailing window
- [x] **Production mode outputs single day:** date == as_of_date only (not dates >=)
- [x] **Added missing tests:** IR requires returns, rolling+as_of, correlation window
- [x] **Total tests:** 82 (up from 78)

### v6 Review (2024-12-08)

**Gemini (Planner):** APPROVED
- Plan technically sound and comprehensive
- Lookahead prevention via strict windowing is correct
- Production vs backtest modes well-designed

**Codex (Planner):** FIX REQUIRED
- **HIGH (CORR-LOOKAHEAD):** compute_correlation_matrix signature lacks as_of_date/lookback - uses full history
- **MEDIUM (CORR-API-CONTRACT):** Pipeline says correlation limited to window but API can't express that
- **LOW (VOL-PARITY-DEFINITION):** Vol-parity definition is nonstandard - should document rationale

### v7 Changes Made
- [x] **Added as_of_date/lookback to correlation API:** compute_correlation_matrix now filters to trailing window
- [x] **Documented vol-parity rationale:** Explains why per-stock time-series vol is correct
- [x] **Added vol-parity definition test:** Verifies per-stock time-series volatility definition
- [x] **Total tests:** 83 (up from 82)

### v7 Review (2024-12-08)

**Gemini (Planner):** APPROVED ✓
- Technically sound, all previous feedback addressed
- Strict windowing [as_of - lookback, as_of) for weights and correlations
- 83-test suite covers critical edge cases

**Codex (Planner):** APPROVED ✓
- No blocking issues detected
- Lookahead controls sufficient (production/rolling modes, correlation windowing)
- Schema alignment correct, algorithms sound
- Proceed with implementation

---

**Plan Created:** 2024-12-08
**Author:** Claude Code (Dev A)
**Status:** APPROVED - Ready for Implementation
