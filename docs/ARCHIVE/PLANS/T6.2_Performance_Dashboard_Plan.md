# T6.2 Performance Dashboard Implementation Plan

**Task:** T6.2 from P4_PLANNING.md
**Effort:** 3-4 days
**PR:** `feat(p4): performance dashboard`
**Status:** ✅ T6.1 RBAC Integration Complete (Iteration 10)

---

## Summary

Implement a Performance Dashboard for the trading platform web console with:
1. Real-time P&L display (using existing `/api/v1/positions/pnl/realtime`)
2. Historical performance charts (equity curve) - **Realized P&L only for MVP**
3. Drawdown visualization (from realized P&L series)
4. Position summary
5. **✅ T6.1 RBAC Integration** - Strategy-scoped access control (per P4T3_TASK.md dependency)

---

## T6.1 RBAC Integration (Iteration 10 - COMPLETED)

Per P4T3_TASK.md, T6.2 depends on T6.1a RBAC infrastructure. The following was implemented:

### API Layer (`apps/execution_gateway/main.py`)
- ✅ `@require_permission(Permission.VIEW_PNL)` decorator on `/api/v1/performance/daily`
- ✅ `_build_user_context()` extracts role/strategies from request headers
- ✅ `get_authorized_strategies(user)` filters data to user's assigned strategies
- ✅ Feature flag gate: `FEATURE_PERFORMANCE_DASHBOARD` env var
- ✅ Strategy-scoped cache keys: `performance:daily:{user_id}:{strategies_hash}:{dates}`
- ✅ Cache TTL updated to **300s (5 minutes)** per P4T3_TASK.md spec

### Database Layer (`apps/execution_gateway/database.py`)
- ✅ `get_daily_pnl_history(start_date, end_date, strategies)` - strategy parameter added
- ✅ SQL query filters by `strategy_id = ANY(strategies)`
- ✅ Returns empty list if no strategies authorized

### UI Layer (`apps/web_console/pages/performance.py`)
- ✅ `@require_auth` decorator on main()
- ✅ `has_permission(user, Permission.VIEW_PNL)` check
- ✅ `get_authorized_strategies(user)` for strategy scoping
- ✅ Feature flag gate: `FEATURE_PERFORMANCE_DASHBOARD`
- ✅ Date range presets: **7 Days, 30 Days, 90 Days, YTD, All, Custom**
- ✅ Strategy-aware empty state: "No trading activity for your assigned strategies"

### Tests Updated
- ✅ `test_performance_endpoint.py` - RBAC, feature flag, strategy filtering tests
- ✅ `test_performance_dashboard.py` - Permission, strategy scoping, preset tests

---

## Workflow Gates (per CLAUDE.md)

**Pre-Implementation:**
1. ✅ Complete 00-analysis checklist
2. `./scripts/workflow_gate.py start-task docs/TASKS/P4T3_TASK.md feature/P4T3-performance-dashboard`
3. `./scripts/workflow_gate.py set-component "T6.2-Performance-Dashboard"`
4. `./scripts/workflow_gate.py advance plan-review` → Gemini + Codex plan reviews

**Per Component:**
1. `./scripts/workflow_gate.py advance implement`
2. Implement + write tests (TDD)
3. `./scripts/workflow_gate.py advance test`
4. `./scripts/workflow_gate.py advance review` → Fresh zen-mcp reviews (Gemini + Codex)
5. `make ci-local` (MANDATORY)
6. `./scripts/workflow_gate.py record-review gemini approved`
7. `./scripts/workflow_gate.py record-review codex approved`
8. `./scripts/workflow_gate.py record-ci passed`
9. `git commit` with zen-mcp markers

---

## Design Decisions

| Decision | Choice | Rationale |
|----------|--------|-----------|
| **Historical Data Source** | Store per-fill `realized_pl` at webhook time using existing `calculate_position_update()` | Accurate P&L computed once, avoids recalculation issues |
| **Real-time P&L** | Use existing `/api/v1/positions/pnl/realtime` endpoint | Already implemented, includes unrealized P&L |
| **Page Structure** | New page in `pages/` folder | Follows Streamlit multi-page convention |
| **Chart Library** | Plotly (`plotly>=5.18.0,<6.0.0`) | Bundled with Streamlit, interactive, pinned version |
| **Drawdown Definition** | Peak-to-trough of cumulative realized P&L (equity curve) | Standard definition from peak, not from initial capital |
| **Date Range** | Default 30 days, max 90 days, bounded validation, UTC floor/ceil | Prevents unbounded DB queries |
| **Caching** | Redis TTL **300s (5 min)** with user-scoped + strategy-scoped keys + invalidation on fill | Per P4T3_TASK.md spec, fresh data after trades |
| **Auth** | `@require_auth` decorator (Streamlit) + API auth middleware | Security enforcement at both layers |

---

## Single Source of Truth: P&L Calculation (Iteration 8 CRITICAL Fixes)

### The Problem: Multiple Partial Fills + Concurrent Webhooks

Alpaca sends multiple webhook events for partial fills:
- Fill 1: 50 shares at $100 (status: `partially_filled`)
- Fill 2: 30 shares at $101 (status: `partially_filled`)
- Fill 3: 20 shares at $102 (status: `filled`)

**Key Issues Identified:**
1. Each webhook sends **cumulative** `filled_qty` and `filled_avg_price` - NOT incremental
2. Concurrent webhooks can cause race conditions (Read-Modify-Write)
3. `filled_at` is only set on final fill, so `partially_filled` orders have NULL `filled_at`
4. Partial fills may span multiple days - need per-fill timestamps

### Solution: Per-Fill P&L with Incremental Calculation + Row Locking

**Approach:**
1. Use **incremental fill qty** (current - previous) and **per-fill price** from webhook `price` field
2. Use `SELECT ... FOR UPDATE` to prevent race conditions
3. Store per-fill realized P&L with **fill timestamp** (not order.filled_at)
4. Wrap all operations in a single transaction for atomicity
5. Aggregate by **fill timestamp**, not order.filled_at

**Key Insight:** The `calculate_position_update()` function already handles all P&L edge cases correctly - we just need to:
1. Call it with **incremental** fill qty and **per-fill** price (not cumulative)
2. Protect it with row-level locking
3. Store results atomically with fill metadata

### Schema Analysis

**Current Database Schema (from `migrations/002_create_execution_tables.sql`):**

```sql
-- orders table (relevant columns)
client_order_id TEXT PRIMARY KEY,
symbol TEXT NOT NULL,
side TEXT CHECK (side IN ('buy', 'sell')) NOT NULL,
qty NUMERIC NOT NULL,
status TEXT NOT NULL,  -- 'filled', 'partially_filled', 'cancelled', etc.
filled_at TIMESTAMPTZ,
filled_qty NUMERIC DEFAULT 0,
filled_avg_price NUMERIC,
metadata JSONB DEFAULT '{}'

-- positions table (relevant columns)
symbol TEXT PRIMARY KEY,
qty NUMERIC NOT NULL,  -- Positive=long, Negative=short
avg_entry_price NUMERIC NOT NULL,
realized_pl NUMERIC DEFAULT 0  -- Cumulative, updated by calculate_position_update
```

### Existing P&L Logic: `calculate_position_update()` in database.py

The function at `apps/execution_gateway/database.py:39-133` returns `(new_qty, new_avg_price, new_realized_pl)`.

**The realized P&L delta for a fill is:**
```python
realized_pl_delta = new_realized_pl - old_realized_pl
```

This is computed ONCE at fill time and is ALWAYS correct (position flips, partial closes, etc.).

---

## Component 0: Per-Fill Realized P&L Storage (Iteration 8 CRITICAL Fixes)

### Approach: Per-Fill P&L with Row Locking + Transactions

**Key Changes from Previous Iterations:**
1. Use `SELECT ... FOR UPDATE` to prevent race conditions
2. Use **incremental** fill qty (current - previous) and **per-fill price** from webhook
3. Store fill **timestamp** for correct daily aggregation (not order.filled_at)
4. Wrap all operations in a single transaction

### Order Metadata Structure

```python
# Order metadata structure after fills:
{
    "fills": [
        {
            "fill_id": "fill_001",
            "fill_qty": 50,              # INCREMENTAL qty for this fill
            "fill_price": "100.00",      # Per-fill price (from webhook 'price' field)
            "realized_pl": "500.00",     # P&L for this fill only
            "timestamp": "2024-01-15T10:30:00Z"  # Fill timestamp (used for daily aggregation)
        },
        {
            "fill_id": "fill_002",
            "fill_qty": 30,
            "fill_price": "101.00",
            "realized_pl": "0.00",       # No P&L if opening position
            "timestamp": "2024-01-15T10:31:00Z"
        }
    ],
    "total_realized_pl": "500.00"  # Sum for quick access
}
```

**CRITICAL:** Daily P&L aggregation uses `fills[].timestamp`, NOT `order.filled_at`

### Files to Modify

| File | Changes |
|------|---------|
| `apps/execution_gateway/database.py` | Add `get_position()`, `append_fill_to_order_metadata()` |
| `apps/execution_gateway/main.py` | Update webhook to capture fill P&L before position update |

### DatabaseClient New Methods

```python
# In apps/execution_gateway/database.py

def get_position_for_update(self, symbol: str, conn: Connection) -> Position | None:
    """
    Get full Position object with row-level lock (FOR UPDATE).

    CRITICAL: Must be called within a transaction. Uses SELECT FOR UPDATE
    to prevent race conditions during concurrent webhook processing.

    Args:
        symbol: Stock symbol
        conn: Active database connection (must be in transaction)

    Returns:
        Position if exists, None otherwise (locked for update)
    """
    with conn.cursor(row_factory=dict_row) as cur:
        cur.execute(
            "SELECT * FROM positions WHERE symbol = %s FOR UPDATE",
            (symbol,),
        )
        row = cur.fetchone()
        if row is None:
            return None
        return Position(**row)


def append_fill_to_order_metadata(
    self,
    client_order_id: str,
    fill_data: dict,
    conn: Connection,
) -> OrderDetail | None:
    """
    Append a fill record to order metadata (for partial fills).

    Uses jsonb_set with array concatenation to preserve previous fills.
    Must be called within a transaction for atomicity.

    Args:
        client_order_id: Order ID
        fill_data: Dict with fill_id, fill_qty, fill_price, realized_pl, timestamp
        conn: Active database connection (must be in transaction)

    Returns:
        Updated OrderDetail if found, None otherwise
    """
    with conn.cursor(row_factory=dict_row) as cur:
        # Append to fills array and update total_realized_pl
        cur.execute(
            """
            UPDATE orders
            SET
                metadata = jsonb_set(
                    jsonb_set(
                        CASE
                            WHEN metadata ? 'fills'
                            THEN metadata
                            ELSE metadata || '{"fills": []}'::jsonb
                        END,
                        '{fills}',
                        COALESCE((metadata->'fills'), '[]'::jsonb) || %s::jsonb
                    ),
                    '{total_realized_pl}',
                    to_jsonb(
                        COALESCE((metadata->>'total_realized_pl')::NUMERIC, 0)
                        + (%s::jsonb->>'realized_pl')::NUMERIC
                    )
                ),
                updated_at = NOW()
            WHERE client_order_id = %s
            RETURNING *
            """,
            (json.dumps(fill_data), json.dumps(fill_data), client_order_id),
        )

        row = cur.fetchone()
        if not row:
            logger.warning(f"Order not found for fill append: {client_order_id}")
            return None

        logger.info(
            f"Fill appended to order: {client_order_id}",
            extra={
                "client_order_id": client_order_id,
                "fill_qty": fill_data.get("fill_qty"),
                "realized_pl": fill_data.get("realized_pl"),
            },
        )

        return OrderDetail(**row)


def get_order_filled_qty(self, client_order_id: str, conn: Connection) -> int:
    """
    Get current filled_qty for an order (to compute incremental fill).

    Args:
        client_order_id: Order ID
        conn: Active database connection

    Returns:
        Current filled_qty (0 if order not found or not filled)
    """
    with conn.cursor(row_factory=dict_row) as cur:
        cur.execute(
            "SELECT filled_qty FROM orders WHERE client_order_id = %s",
            (client_order_id,),
        )
        row = cur.fetchone()
        return int(row["filled_qty"]) if row and row["filled_qty"] else 0


def update_position_on_fill_with_conn(
    self,
    symbol: str,
    fill_qty: int,
    fill_price: Decimal,
    side: str,
    conn: Connection,
) -> Position:
    """
    Update position using existing calculate_position_update logic.

    Same as update_position_on_fill but accepts a connection for transaction support.
    The position must be locked with get_position_for_update first.

    Args:
        symbol: Stock symbol
        fill_qty: INCREMENTAL fill quantity (not cumulative!)
        fill_price: Per-fill price (not weighted average!)
        side: 'buy' or 'sell'
        conn: Active database connection (must be in transaction with position locked)

    Returns:
        Updated Position
    """
    # Implementation uses calculate_position_update internally
    # See existing update_position_on_fill for logic
    ...


def get_order_for_update(self, client_order_id: str, conn: Connection) -> OrderDetail | None:
    """
    Get order with row-level lock (FOR UPDATE) - Iteration 9 CRITICAL fix.

    CRITICAL: Must be called within a transaction. Locks order row to prevent
    concurrent webhook processing from causing duplicate fill appends.

    Args:
        client_order_id: Order ID
        conn: Active database connection (must be in transaction)

    Returns:
        OrderDetail if exists, None otherwise (locked for update)
    """
    with conn.cursor(row_factory=dict_row) as cur:
        cur.execute(
            "SELECT * FROM orders WHERE client_order_id = %s FOR UPDATE",
            (client_order_id,),
        )
        row = cur.fetchone()
        if row is None:
            return None
        return OrderDetail(**row)


def update_order_status_with_conn(
    self,
    client_order_id: str,
    status: str,
    filled_qty: int,
    filled_avg_price: Decimal,
    filled_at: datetime | None,
    conn: Connection,
) -> OrderDetail | None:
    """
    Update order status within a transaction - Iteration 9 HIGH fix.

    Unlike update_order_status(), this accepts a connection parameter to
    participate in the webhook transaction. Does NOT auto-commit.

    Args:
        client_order_id: Order ID
        status: New status
        filled_qty: Cumulative filled quantity
        filled_avg_price: Cumulative weighted average price
        filled_at: Fill timestamp (set on final fill only)
        conn: Active database connection (must be in transaction)

    Returns:
        Updated OrderDetail if found, None otherwise
    """
    with conn.cursor(row_factory=dict_row) as cur:
        cur.execute(
            """
            UPDATE orders
            SET status = %s,
                filled_qty = %s,
                filled_avg_price = %s,
                filled_at = COALESCE(%s, filled_at),
                updated_at = NOW()
            WHERE client_order_id = %s
            RETURNING *
            """,
            (status, filled_qty, filled_avg_price, filled_at, client_order_id),
        )
        row = cur.fetchone()
        if not row:
            return None
        return OrderDetail(**row)
```

### Webhook Handler Update (main.py) - TRANSACTIONAL APPROACH (Iteration 9 Fixes)

**CRITICAL:** All operations in a single transaction with row-level locking on BOTH order AND position.

```python
# In apps/execution_gateway/main.py - order_webhook handler

# Extract per-fill price from webhook (NOT cumulative filled_avg_price!)
# Alpaca webhooks include 'price' field for individual fill price
per_fill_price = Decimal(str(webhook_data.get("price", filled_avg_price)))

# CRITICAL: Use BROKER timestamp, not server time (Iteration 9 HIGH fix)
# Alpaca webhooks include 'timestamp' or 'event.ts' for fill time
broker_fill_timestamp = webhook_data.get("timestamp") or webhook_data.get("filled_at")
if broker_fill_timestamp:
    fill_timestamp = datetime.fromisoformat(broker_fill_timestamp.replace("Z", "+00:00"))
else:
    # Fallback to server time only if broker timestamp missing
    fill_timestamp = datetime.now(timezone.utc)
    logger.warning(f"Missing broker timestamp for {client_order_id}, using server time")

# All operations in a single transaction with row locking
with db_client.transaction() as conn:
    # 1. CRITICAL: Lock ORDER row first to prevent concurrent webhook processing
    #    This prevents duplicate fill appends from concurrent webhooks
    order = db_client.get_order_for_update(client_order_id, conn)
    if not order:
        logger.error(f"Order not found: {client_order_id}")
        return {"status": "error", "message": "Order not found"}

    prev_filled_qty = int(order.filled_qty or 0)
    incremental_fill_qty = int(filled_qty) - prev_filled_qty

    if incremental_fill_qty <= 0:
        # No new fill, skip (duplicate webhook or out-of-order delivery)
        # Iteration 9: Log but don't fail - out-of-order webhooks happen
        logger.info(f"No incremental fill: prev={prev_filled_qty}, current={filled_qty}")
        return {"status": "skipped", "reason": "no_incremental_fill"}

    # 2. Lock POSITION for update (prevents race conditions)
    old_position = db_client.get_position_for_update(symbol, conn)
    old_realized_pl = old_position.realized_pl if old_position else Decimal("0")

    # 3. Update position with INCREMENTAL fill qty and PER-FILL price
    position = db_client.update_position_on_fill_with_conn(
        symbol=symbol,
        fill_qty=incremental_fill_qty,      # INCREMENTAL, not cumulative!
        fill_price=per_fill_price,           # PER-FILL price, not avg!
        side=order.side,
        conn=conn,
    )

    # 4. Calculate P&L delta for this specific fill
    realized_pl_delta = position.realized_pl - old_realized_pl

    # 5. Append fill to order metadata with BROKER fill timestamp
    #    Order is already locked, so this is safe from concurrent appends
    db_client.append_fill_to_order_metadata(
        client_order_id=client_order_id,
        fill_data={
            "fill_id": f"{client_order_id}_{int(filled_qty)}",
            "fill_qty": incremental_fill_qty,
            "fill_price": str(per_fill_price),
            "realized_pl": str(realized_pl_delta),
            "timestamp": fill_timestamp.isoformat(),  # BROKER timestamp for daily aggregation
        },
        conn=conn,
    )

    # 6. Update order status (MUST use conn for atomicity - Iteration 9 HIGH fix)
    db_client.update_order_status_with_conn(
        client_order_id=client_order_id,
        status=order_data["status"],
        filled_qty=int(filled_qty),
        filled_avg_price=Decimal(str(filled_avg_price)),
        filled_at=fill_timestamp if order_data["status"] == "filled" else None,
        conn=conn,
    )

# Transaction commits automatically on success, rolls back on error
```

**Key Points (Iteration 9 Updates):**
1. **Order locking:** `get_order_for_update()` locks order row FIRST to prevent concurrent webhook processing
2. **Position locking:** `get_position_for_update()` locks position row to prevent concurrent P&L updates
3. **Broker timestamp:** Uses broker-provided timestamp (not server time) for accurate daily aggregation
4. **Incremental fill:** Uses `current - previous` filled_qty from locked order record
5. **Per-fill price:** Uses webhook `price` field, NOT weighted `filled_avg_price`
6. **Atomic status update:** `update_order_status_with_conn()` uses shared connection for transaction
7. **Atomicity:** All 4 operations (order, position, metadata, status) in one transaction

---

## SQL Query for Daily Realized P&L (Iteration 9 - With Status Filter)

**CRITICAL:** Aggregate by `fills[].timestamp`, NOT `order.filled_at`. This correctly handles partial fills spanning multiple days.

```sql
-- Daily realized P&L from per-fill data in order metadata
-- Unnests the fills array and aggregates by each fill's timestamp
-- Iteration 9 LOW fix: Filter by order status to exclude cancelled/test orders
SELECT
    DATE((fill->>'timestamp')::timestamptz AT TIME ZONE 'UTC') as trade_date,
    SUM((fill->>'realized_pl')::NUMERIC) as daily_realized_pl,
    COUNT(*) FILTER (WHERE (fill->>'realized_pl')::NUMERIC != 0) as closing_trade_count
FROM orders o,
     jsonb_array_elements(o.metadata->'fills') as fill
WHERE o.status IN ('filled', 'partially_filled')  -- Iteration 9 LOW fix: status filter
  AND o.metadata ? 'fills'
  AND jsonb_array_length(o.metadata->'fills') > 0
  AND (fill->>'timestamp')::timestamptz >= :start_date::timestamptz
  AND (fill->>'timestamp')::timestamptz < :end_date::timestamptz + interval '1 day'
GROUP BY DATE((fill->>'timestamp')::timestamptz AT TIME ZONE 'UTC')
ORDER BY trade_date;
```

**Why this approach:**
1. **Handles multi-day partial fills:** Fill on Monday at 10am and fill on Tuesday at 2pm are correctly attributed to their respective days
2. **No NULL issues:** Does NOT depend on `order.filled_at` (which is NULL for `partially_filled` orders)
3. **Per-fill granularity:** Each fill's P&L is counted on the day it occurred
4. **Pre-computed P&L:** No complex calculation at query time - just summing pre-computed values
5. **Status filter:** Excludes cancelled/test orders (Iteration 9 LOW fix)

**Index for Performance:**
```sql
-- GIN index for jsonb fills array query
CREATE INDEX IF NOT EXISTS idx_orders_metadata_fills
ON orders USING GIN ((metadata->'fills'));

-- Partial index for status filter
CREATE INDEX IF NOT EXISTS idx_orders_status_fills
ON orders(status)
WHERE status IN ('filled', 'partially_filled') AND metadata ? 'fills';
```

---

## Webhook Security (Iteration 6 MEDIUM Fix)

### Current State

The `order_webhook` handler at `main.py:2565` has signature verification as TODO.

### Required Implementation

```python
# In apps/execution_gateway/webhook_security.py (already exists)
# Ensure verify_alpaca_signature() is called in the webhook handler

@app.post("/webhooks/alpaca/orders")
async def order_webhook(request: Request) -> dict[str, str]:
    # Verify signature FIRST
    if not await verify_alpaca_signature(request):
        logger.warning("Invalid webhook signature rejected")
        raise HTTPException(status_code=401, detail="Invalid signature")

    # ... rest of handler
```

### Test for Invalid Signature

```python
def test_order_webhook_rejects_invalid_signature():
    """Test that webhook rejects requests with invalid signature."""
    response = client.post(
        "/webhooks/alpaca/orders",
        json={"event": "fill", "order": {...}},
        headers={"X-Alpaca-Signature": "invalid_signature"},
    )
    assert response.status_code == 401
```

---

## Cache Strategy (Iteration 10 - Use SCAN, not KEYS + Strategy Scoping)

**TTL + Event-Driven Invalidation with SCAN + Strategy Scoping:**

```python
# Cache TTL per P4T3_TASK.md spec (5 minutes)
PERFORMANCE_CACHE_TTL = int(os.getenv('PERFORMANCE_CACHE_TTL', '300'))  # 5 minutes

# Cache key scoped to user AND strategies (Iteration 10 RBAC fix)
strategies_hash = hashlib.md5(",".join(sorted(authorized_strategies)).encode()).hexdigest()[:8]
cache_key = f"performance:daily:{user_id}:{strategies_hash}:{start_date}:{end_date}"

# Event-driven invalidation on order fill (in webhook handler)
# CRITICAL: Use SCAN instead of KEYS to avoid O(N) blocking in production
async def invalidate_performance_cache(user_id: str, redis_client):
    """
    Invalidate all performance cache entries for a user.

    Uses SCAN instead of KEYS to avoid blocking Redis in production.
    KEYS is O(N) and blocks the Redis server; SCAN is iterative and non-blocking.
    """
    pattern = f"performance:daily:{user_id}:*"
    cursor = 0
    while True:
        cursor, keys = await redis_client.scan(cursor, match=pattern, count=100)
        if keys:
            await redis_client.delete(*keys)
        if cursor == 0:
            break
```

**Alternative: Store cache keys in a set per user** (more efficient for frequent invalidation):
```python
# On cache set, also add key to user's key set
await redis_client.sadd(f"performance:keys:{user_id}", cache_key)
await redis_client.setex(cache_key, PERFORMANCE_CACHE_TTL, data)

# On invalidation, get all keys from set and delete
async def invalidate_performance_cache(user_id: str, redis_client):
    key_set = f"performance:keys:{user_id}"
    keys = await redis_client.smembers(key_set)
    if keys:
        await redis_client.delete(*keys, key_set)
```

---

## data_available_from Derivation (Iteration 9 MEDIUM Fix)

**CRITICAL:** Use fill metadata timestamp, NOT `order.filled_at` (which is NULL for partially_filled).

```python
async def get_data_availability_date(db_client) -> date | None:
    """
    Get the first date with fill metadata (Iteration 9 MEDIUM fix).

    Uses the minimum fill timestamp from metadata, NOT order.filled_at.
    This handles partially_filled orders where filled_at is NULL.

    Returns None if no orders have fill metadata yet.
    """
    result = await db_client.execute_query("""
        SELECT MIN(DATE((fill->>'timestamp')::timestamptz AT TIME ZONE 'UTC')) as first_date
        FROM orders o,
             jsonb_array_elements(o.metadata->'fills') as fill
        WHERE o.status IN ('filled', 'partially_filled')
          AND o.metadata ? 'fills'
          AND jsonb_array_length(o.metadata->'fills') > 0
    """)
    return result['first_date'] if result else None
```

---

## Files to Create

| File | Purpose |
|------|---------|
| `apps/web_console/pages/performance.py` | Main performance dashboard page |
| `apps/web_console/components/pnl_chart.py` | Reusable Plotly chart components |
| `tests/apps/web_console/test_performance_dashboard.py` | UI/page tests |
| `tests/apps/execution_gateway/test_performance_endpoint.py` | API endpoint tests |
| `docs/CONCEPTS/performance-dashboard.md` | User documentation |

## Files to Modify

| File | Changes |
|------|---------|
| `apps/execution_gateway/database.py` | Add `get_position()`, `append_fill_to_order_metadata()`, `get_daily_pnl_history()` |
| `apps/execution_gateway/main.py` | Update webhook to store per-fill P&L + add `/api/v1/performance/daily` endpoint |
| `apps/execution_gateway/schemas.py` | Add `DailyPnL`, `PerformanceRequest`, `DailyPerformanceResponse` |
| `apps/web_console/config.py` | Add `performance_daily` endpoint URL + env vars |
| `apps/web_console/requirements.txt` | Add `plotly>=5.18.0,<6.0.0` |

---

## API Design

### New Endpoint: `GET /api/v1/performance/daily`

**Auth:**
- Protected by existing auth middleware (same as other execution gateway endpoints)
- Rate limited via existing rate limiter

**Request Validation:**
```python
from datetime import date, timedelta
from pydantic import BaseModel, Field, model_validator
import os

class PerformanceRequest(BaseModel):
    start_date: date = Field(
        default_factory=lambda: date.today() - timedelta(days=30),
        description="Start date (UTC, inclusive). Default: 30 days ago"
    )
    end_date: date = Field(
        default_factory=date.today,
        description="End date (UTC, inclusive). Default: today"
    )

    @model_validator(mode='after')
    def validate_range(self) -> 'PerformanceRequest':
        """Validate date range bounds."""
        max_days = int(os.getenv('MAX_PERFORMANCE_DAYS', '90'))

        if self.start_date > self.end_date:
            raise ValueError("start_date must be <= end_date")
        if self.end_date > date.today():
            raise ValueError("end_date cannot be in the future")
        if (self.end_date - self.start_date).days > max_days:
            raise ValueError(f"Date range cannot exceed {max_days} days")

        return self
```

**Response Schema:**
```python
from decimal import Decimal
from datetime import date, datetime
from pydantic import BaseModel

class DailyPnL(BaseModel):
    date: date
    realized_pl: Decimal           # Daily realized P&L (from closed positions)
    cumulative_realized_pl: Decimal # Running total
    peak_equity: Decimal           # Running max of cumulative P&L for drawdown calc
    drawdown_pct: Decimal          # (current - peak) / |peak| * 100, or 0 if peak <= 0
    closing_trade_count: int       # Number of position-closing fills that day

class DailyPerformanceResponse(BaseModel):
    daily_pnl: list[DailyPnL]
    total_realized_pl: Decimal
    max_drawdown_pct: Decimal      # Maximum drawdown percentage in period
    start_date: date
    end_date: date
    data_source: str = "realized_only"
    note: str = "Shows realized P&L from closed positions. Unrealized P&L not included."
    data_available_from: date | None  # First date with P&L metadata
    last_updated: datetime         # Cache timestamp
```

**Database Index:**
```sql
-- Composite index for daily P&L query performance
CREATE INDEX IF NOT EXISTS idx_orders_filled_closing
ON orders(filled_at, status)
WHERE status IN ('filled', 'partially_filled');
```

---

## Test Coverage

### API/DB Tests (`tests/apps/execution_gateway/test_performance_endpoint.py`):
```python
class TestDailyPerformanceEndpoint:
    def test_get_daily_pnl_default_range(self)
    def test_get_daily_pnl_custom_range(self)
    def test_get_daily_pnl_no_orders(self)
    def test_invalid_date_range_returns_400(self)
    def test_date_range_exceeds_90_days_returns_400(self)
    def test_future_end_date_returns_400(self)
    def test_auth_required_returns_401(self)
    def test_response_cached_in_redis(self)
    def test_cache_key_includes_user_id(self)
    def test_cache_invalidated_on_fill(self)


class TestDailyPnLDatabase:
    def test_get_daily_pnl_history_aggregation(self)
    def test_partial_fill_pnl_accumulated_correctly(self)
    def test_partial_fills_across_days_aggregated_by_fill_timestamp(self)  # Iteration 8 fix
    def test_position_flip_pnl_correct(self)
    def test_drawdown_calculation_accuracy(self)
    def test_drawdown_zero_when_peak_zero(self)
    def test_peak_tracking_correctness(self)
    def test_empty_result_handling(self)
    def test_missing_days_carry_forward(self)
    def test_utc_boundary_handling(self)
    def test_orders_without_metadata_excluded(self)
    def test_query_uses_fill_timestamp_not_order_filled_at(self)  # Iteration 8 fix


class TestOrderFillMetadata:
    def test_fill_handler_stores_realized_pl_delta(self)
    def test_multiple_partial_fills_accumulate_correctly(self)
    def test_fill_metadata_appended_not_overwritten(self)
    def test_total_realized_pl_updated_on_each_fill(self)
    def test_incremental_fill_qty_computed_correctly(self)  # Iteration 8 fix
    def test_per_fill_price_used_not_cumulative_avg(self)  # Iteration 8 fix
    def test_fill_timestamp_stored_for_daily_aggregation(self)  # Iteration 8 fix


class TestDatabaseClientNewMethods:
    def test_get_position_for_update_locks_row(self)
    def test_get_position_for_update_returns_none_for_missing(self)
    def test_get_order_for_update_locks_row(self)  # Iteration 9 fix
    def test_get_order_for_update_returns_none_for_missing(self)  # Iteration 9 fix
    def test_append_fill_to_order_metadata_creates_array(self)
    def test_append_fill_to_order_metadata_appends_to_existing(self)
    def test_append_fill_updates_total_realized_pl(self)
    def test_get_order_filled_qty_returns_current_value(self)
    def test_update_position_on_fill_with_conn_atomic(self)
    def test_update_order_status_with_conn_no_autocommit(self)  # Iteration 9 fix


class TestConcurrentWebhooks:  # Race condition tests
    def test_concurrent_fills_with_order_locking(self)  # Iteration 9 - order locking
    def test_concurrent_fills_with_position_locking(self)
    def test_no_lost_updates_under_concurrency(self)
    def test_no_duplicate_fills_under_concurrency(self)  # Iteration 9 fix
    def test_transaction_rollback_on_error(self)


class TestBrokerTimestamps:  # Iteration 9 - Broker timestamp tests
    def test_broker_timestamp_used_when_available(self)
    def test_fallback_to_server_time_when_missing(self)
    def test_timestamp_normalized_to_utc(self)


class TestOutOfOrderWebhooks:  # Iteration 9 - Out-of-order handling
    def test_lower_cumulative_qty_skipped(self)
    def test_duplicate_webhook_idempotent(self)
    def test_skip_logged_not_error(self)


class TestWebhookSecurity:
    def test_valid_signature_accepted(self)
    def test_invalid_signature_rejected(self)  # Iteration 6 fix
    def test_missing_signature_rejected(self)
```

### UI Tests (`tests/apps/web_console/test_performance_dashboard.py`):
```python
class TestPerformancePage:
    def test_render_realtime_pnl_with_positions(self)
    def test_render_realtime_pnl_no_positions(self)
    def test_render_historical_performance(self)
    def test_render_position_summary(self)
    def test_api_error_handling(self)
    def test_auth_required(self)
    def test_date_range_validation_ui(self)
    def test_loading_state_displayed(self)
    def test_empty_data_message(self)
    def test_realized_only_label_displayed(self)
    def test_data_availability_warning(self)


class TestPnLCharts:
    def test_render_equity_curve(self)
    def test_render_drawdown_chart(self)
    def test_empty_data_handling(self)
```

---

## Implementation Components (6-step pattern each)

### Component 0: Per-Fill P&L Storage (Pre-requisite)
**Files:** `apps/execution_gateway/database.py`, `apps/execution_gateway/main.py`

1. Add `get_position()` method to DatabaseClient (returns full Position with realized_pl)
2. Add `append_fill_to_order_metadata()` method to DatabaseClient
3. Update webhook handler to compute and store P&L delta per fill
4. Ensure webhook signature verification is active
5. Add tests for per-fill metadata storage
6. Test with partial fill scenario (multiple webhooks)

### Component 1: Backend API Extension
**Files:** `database.py`, `schemas.py`, `main.py`, `config.py`, `requirements.txt`

1. Add Pydantic schemas (`PerformanceRequest`, `DailyPnL`, `DailyPerformanceResponse`)
2. Add `get_daily_pnl_history()` using `total_realized_pl` from metadata
3. Add Redis caching layer with user-scoped keys + invalidation
4. Add `/api/v1/performance/daily` endpoint with auth + validation
5. Add endpoint URL and env vars to web_console config
6. Add `plotly>=5.18.0,<6.0.0` to requirements.txt

### Component 2: Chart Components
**Files:** `apps/web_console/components/pnl_chart.py`

1. `render_equity_curve()` - Plotly line chart
2. `render_drawdown_chart()` - Plotly area chart
3. Clear labeling ("Realized P&L Only")

### Component 3: Performance Page
**Files:** `apps/web_console/pages/performance.py`

1. `render_realtime_pnl()` - Uses existing `/api/v1/positions/pnl/realtime`
2. `render_historical_performance()` - With loading/error/empty states + data availability warning
3. `render_position_summary()` - Using existing positions endpoint
4. Date range selector (default 30 days, max 90 days)
5. Auth via `@require_auth` decorator
6. Navigation link from main app

### Component 4: Tests
**Files:** Both test files as specified above

### Component 5: Documentation
**Files:** `docs/CONCEPTS/performance-dashboard.md`

- Feature overview with clear data source explanation
- Realized vs Unrealized P&L distinction
- Partial fill handling explanation
- Date range limits (max 90 days) and caching behavior (1 min TTL)
- Env vars: `MAX_PERFORMANCE_DAYS`, `PERFORMANCE_CACHE_TTL`
- Data availability notes (accurate data from first fill with metadata)

---

## Addressing All Review Issues (Iteration 1-6)

| Iteration | Reviewer | Severity | Issue | Resolution |
|-----------|----------|----------|-------|------------|
| 1 | Codex | CRITICAL | Orders table lacks EOD prices | Changed to "Realized P&L only for MVP" |
| 1 | Codex | HIGH | Unbounded date range | Added max 90 days limit |
| 1 | Gemini | MEDIUM | O(N) replay performance | Added bounded range + index |
| 2 | Gemini | CRITICAL | P&L needs FIFO matching | Use existing FIFO-matched avg_entry_price |
| 2 | Codex | CRITICAL | Missing workflow gates | Added explicit workflow steps |
| 2 | Codex | HIGH | UTC timezone handling | Added UTC floor/ceil rules |
| 2 | Codex | HIGH | Auth/rate limiting | Add auth middleware + existing rate limiter |
| 2 | Codex | HIGH | Caching strategy | Redis TTL cache |
| 2 | Codex | MEDIUM | DB index | Composite index on (filled_at, status) |
| 2 | Codex | MEDIUM | Missing days handling | Carry forward cumulative P&L |
| 3 | Gemini | CRITICAL | Orders table lacks realized_pl column | Store P&L in order metadata |
| 3 | Codex | HIGH | Mixed sources without single source of truth | Single source: order metadata |
| 3 | Codex | HIGH | Cache key lacks user scoping | Added user_id to cache key |
| 3 | Codex | MEDIUM | Drawdown semantics unclear | Defined: peak-to-trough of cumulative realized P&L |
| 3 | Codex | MEDIUM | Default range not specified | Explicit: default 30 days, max 90 days |
| 3 | Codex | MEDIUM | Dependency not pinned | `plotly>=5.18.0,<6.0.0` |
| 4 | Gemini | CRITICAL | entry_price_at_fill not populated | Store P&L at webhook time (better!) |
| 4 | Gemini | HIGH | Fallback to current avg_entry_price wrong | Use P&L delta from position update |
| 4 | Gemini | MEDIUM | Direct DB access coupling | All queries via Execution Gateway API |
| 4 | Codex | CRITICAL | Only SELL orders - misses short covers | Use P&L from calculate_position_update (handles all cases) |
| 4 | Codex | HIGH | Side-aware P&L formula not specified | Reuse existing calculate_position_update logic |
| 4 | Codex | MEDIUM | Status filtering not defined | Include 'filled' and 'partially_filled' |
| 4 | Codex | LOW | 5 min cache stale | Reduced to 1 min + invalidation on fill |
| 5 | Gemini | CRITICAL | P&L uses filled_qty, fails on position flips | Reuse calculate_position_update (already correct) |
| 5 | Gemini | HIGH | get_position, update_order_metadata missing | Added explicit implementation |
| 5 | Gemini | HIGH | Invalid file path (services/order_service.py) | Corrected to database.py and main.py |
| 5 | Codex | LOW | data_available_from derivation not documented | Added explicit derivation query |
| 5 | Codex | LOW | NULL filled_avg_price not guarded | Added WHERE clause filter |
| 6 | Codex | HIGH | Partial fills overwrite metadata | Store per-fill P&L in array, use P&L delta |
| 6 | Codex | MEDIUM | Order-level filled_avg_price mixes with fill-level data | Use P&L delta from position update |
| 6 | Codex | MEDIUM | Webhook signature verification TODO | Implement signature check + negative test |
| 7 | Gemini | CRITICAL | Race condition in update_position_on_fill (Read-Modify-Write) | Use `SELECT ... FOR UPDATE` row-level locking |
| 7 | Gemini | HIGH | Atomicity - webhook operations not transactional | Wrap all operations in single transaction |
| 7 | Codex | CRITICAL | P&L uses cumulative filled_qty/filled_avg_price, not incremental | Use incremental fill qty + per-fill price from webhook |
| 7 | Codex | HIGH | Daily aggregation uses order.filled_at, NULL for partially_filled | Aggregate by fills[].timestamp instead |
| 7 | Codex | HIGH | partially_filled orders have NULL filled_at | Use fill timestamp from metadata, not order.filled_at |
| 7 | Codex | HIGH | Using cumulative filled_avg_price instead of per-fill price | Use webhook `price` field for per-fill price |
| 7 | Codex | MEDIUM | Order-level total_realized_pl can double-count same day | Aggregate by individual fills, not order totals |
| 7 | Codex | MEDIUM | Redis KEYS pattern is O(N) blocking | Use SCAN with batch delete instead |
| 8 | Codex | CRITICAL | Order row not locked - concurrent webhooks can duplicate fills | Lock order row with `SELECT ... FOR UPDATE` FIRST |
| 8 | Codex | HIGH | `update_order_status` doesn't accept connection | Add `update_order_status_with_conn()` method |
| 8 | Codex | HIGH | Order metadata append not locked | Order row locked first, so append is safe |
| 8 | Codex | HIGH | Using `datetime.utcnow()` instead of broker timestamp | Use broker-provided timestamp from webhook |
| 8 | Codex | MEDIUM | Out-of-order webhooks can drop legitimate fills | Log and skip non-incremental fills (idempotent) |
| 8 | Codex | MEDIUM | `data_available_from` uses `filled_at` instead of fill timestamp | Use MIN of fill metadata timestamps |
| 8 | Codex | LOW | Daily aggregation doesn't filter by order status | Add `status IN ('filled','partially_filled')` filter |
| 10 | Codex | CRITICAL | RBAC bypassed - API returns global P&L without auth | Added `@require_permission(Permission.VIEW_PNL)` + strategy filtering |
| 10 | Codex | HIGH | Missing feature flag gate | Added `FEATURE_PERFORMANCE_DASHBOARD` env var check |
| 10 | Codex | MEDIUM | Cache TTL mismatch (60s vs 300s spec) | Updated to 300s per P4T3_TASK.md |
| 10 | Codex | MEDIUM | Missing date range presets | Added 7d/30d/90d/YTD/All preset buttons |
| 10 | Codex | MEDIUM | Cache key not strategy-scoped | Added strategies hash to cache key |
| 11 | Codex | HIGH | Cache not user-scoped (only dates/strategies) | Added X-User-Id to cache key, fail-closed on missing |
| 11 | Codex | HIGH | Frontend omitted RBAC headers | Added role/user headers from session to API calls |
| 11 | Codex | MEDIUM | Permission errors returned 500 instead of 403 | Added PermissionError → 403 exception handler |
| 11 | Codex | MEDIUM | Missing user-id required test | Added test ensuring requests without X-User-Id rejected |
| 11 | Gemini | MEDIUM | Missing `test_cache_invalidated_on_fill` | Added test for webhook-triggered cache invalidation |

---

## Execution Strategy

**IMPORTANT:** Per user request:
- Use **Codex subagent** (via `mcp__pal__clink`) for code implementation
- Use **Codex subagent** for CI test execution
- This prevents main context pollution

### Review Protocol (per CLAUDE.md):
1. **Plan Review:** Fresh Gemini + Codex reviews (no continuation_id reuse)
2. **Code Review:** Fresh Gemini + Codex reviews after implementation
3. **Zero tolerance:** Fix ALL issues including LOW severity

---

## Success Criteria

### T6.1 RBAC Integration (Iteration 10 - COMPLETED ✅)
- [x] API endpoint protected with `@require_permission(Permission.VIEW_PNL)`
- [x] Strategy filtering via `get_authorized_strategies(user)` in API and UI
- [x] Feature flag gate: `FEATURE_PERFORMANCE_DASHBOARD` env var
- [x] Cache key includes strategy hash for proper scoping
- [x] Cache TTL set to **300s (5 minutes)** per P4T3_TASK.md spec
- [x] Date range presets: 7 Days, 30 Days, 90 Days, YTD, All, Custom
- [x] Strategy-aware empty state messaging in UI
- [x] Tests updated for RBAC, feature flag, strategy filtering

### Core Implementation
- [ ] Workflow gates completed (start-task, set-component, advance commands)
- [ ] Component 0: Per-fill P&L storage working with row-level locking on BOTH order AND position
- [ ] Component 0: Webhook uses **incremental** fill qty and **per-fill** price (not cumulative)
- [ ] Component 0: Webhook uses **broker timestamp** (not server time)
- [ ] Component 0: All webhook operations in single transaction (order, position, metadata, status)
- [ ] Component 0: Webhook signature verification active
- [ ] Component 0: Concurrent webhook test passes (no duplicate fills, no lost updates)
- [ ] Component 0: Out-of-order webhooks handled idempotently (skip, log, no error)
- [ ] Real-time P&L displays with auto-refresh (existing endpoint)
- [ ] Equity curve shows cumulative **realized** P&L over time
- [ ] Daily P&L aggregated by **fill timestamp** (not order.filled_at)
- [ ] Daily P&L query includes status filter (excludes cancelled/test orders)
- [ ] Drawdown chart shows peak-to-trough (server-calculated)
- [ ] Position summary shows current positions with P&L
- [ ] All tests pass including edge cases (partial fills, position flips, multi-day fills, concurrent webhooks)
- [ ] Auth enforced on endpoint and page
- [ ] Date range validation (default 30 days, max 90 days, proper error codes)
- [ ] Caching implemented (Redis, **300s TTL**, user+strategy-scoped keys, SCAN-based invalidation)
- [ ] Clear labeling: "Realized P&L Only" in historical view
- [ ] Data availability derived from fill metadata timestamps (not order.filled_at)
- [ ] CI passes locally via `make ci-local`
- [ ] Gemini + Codex reviews approved (zero issues)
