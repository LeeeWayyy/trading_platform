# T1.2: WRDS Connection & Bulk Sync Manager - Complete Implementation Plan

**Component:** WRDS Connection & Bulk Sync Manager
**Dependencies:** T1.1 (LockToken, exceptions, validation framework - COMPLETED)
**Effort:** 3-4 days
**Priority:** P0 (Foundation for all WRDS data)

---

## Overview

This component provides the infrastructure for syncing WRDS academic data (CRSP, Compustat, Fama-French) to local Parquet storage with:
- OS-atomic file locking for single-writer access
- WRDS connection with pooling and rate limiting
- Bulk sync with progress tracking, resume, and atomic writes
- CLI for operations and verification

---

## Files to Create

### Core Library (`libs/data_providers/`)
1. `__init__.py` - Package exports
2. `locking.py` - AtomicFileLock with stale recovery
3. `wrds_client.py` - WRDS connection wrapper
4. `sync_manager.py` - Bulk sync orchestration

### CLI (`scripts/`)
5. `wrds_sync.py` - CLI for sync operations

### Tests (`tests/libs/data_providers/`)
6. `__init__.py` - Test package
7. `test_locking.py` - Lock unit tests
8. `test_lock_contention.py` - Multi-process tests
9. `test_wrds_client.py` - Client tests (mocked)
10. `test_sync_manager.py` - Sync manager tests

### Documentation (`docs/`)
11. `docs/RUNBOOKS/wrds-lock-recovery.md` - Lock recovery procedures
12. `docs/RUNBOOKS/data-backup-restore.md` - Backup/restore with rclone, monthly drill
13. `docs/RUNBOOKS/duckdb-operations.md` - Cache invalidation, reader config during sync
14. `docs/RUNBOOKS/wrds-credentials.md` - Credential rotation, expiry monitoring
15. `docs/RUNBOOKS/data-storage.md` - Disk monitoring, cleanup procedures
16. `docs/CONCEPTS/wrds-data.md`
17. `docs/ADRs/ADR-012-local-data-warehouse.md` - Architecture decision record

---

## Component 1: OS-Atomic Locking System (`locking.py`)

### Classes

```python
class AtomicFileLock:
    """OS-atomic file lock using O_CREAT|O_EXCL for single-writer access."""

    LOCK_TIMEOUT_HOURS = 4
    REFRESH_INTERVAL_SECONDS = 60

    def __init__(self, lock_dir: Path, dataset: str, writer_id: str | None = None): ...
    def acquire(self, timeout_seconds: float = 30) -> LockToken: ...
    def release(self, token: LockToken) -> None: ...
    def refresh(self, token: LockToken) -> LockToken: ...
    def _is_lock_stale(self, lock_path: Path) -> tuple[bool, str]: ...
    def _recover_stale_lock(self, lock_path: Path) -> bool: ...
    def _is_pid_alive(self, pid: int) -> bool: ...

@contextmanager
def atomic_lock(lock_dir: Path, dataset: str, ...) -> Iterator[LockToken]: ...
```

### Exceptions
```python
class LockAcquisitionError(Exception): ...
class LockRecoveryError(Exception): ...
class MalformedLockFileError(Exception): ...
```

### Key Implementation Details

1. **Atomic Creation:** `os.open(path, os.O_CREAT | os.O_EXCL | os.O_WRONLY)`
2. **fsync:** After writing lock metadata for durability
3. **Stale Detection:** expires_at check + PID alive check + hostname match
4. **Recovery:** Atomic rename to `.lock.recovery.<pid>` (deterministic winner)
5. **PID Check:** `os.kill(pid, 0)` for local, timeout-only for remote hosts

---

## Component 2: WRDS Client (`wrds_client.py`)

### Classes

```python
class WRDSConfig(BaseSettings):
    """WRDS connection configuration."""
    username: str
    password: SecretStr  # From secrets manager
    host: str = "wrds-pgdata.wharton.upenn.edu"
    port: int = 9737
    database: str = "wrds"
    pool_size: int = 3
    max_overflow: int = 2
    query_timeout_seconds: int = 300
    rate_limit_queries_per_minute: int = 10

class WRDSClient:
    """WRDS database client with connection pooling and rate limiting."""

    def __init__(self, config: WRDSConfig): ...
    def connect(self) -> None: ...
    def close(self) -> None: ...
    def execute_query(self, sql: str, params: dict | None = None) -> pl.DataFrame: ...
    def get_table_info(self, schema: str, table: str) -> dict: ...
    def _rate_limit(self) -> None: ...

    # Context manager support
    def __enter__(self) -> WRDSClient: ...
    def __exit__(self, ...) -> None: ...
```

### Key Implementation Details

1. **Connection Pool:** SQLAlchemy engine with pool_size/max_overflow
2. **Rate Limiting:** Token bucket algorithm, configurable QPM
3. **Timeout:** Per-query timeout with cancellation
4. **Retry:** Exponential backoff for transient errors
5. **Credentials:** Load from secrets manager (existing pattern)

---

## Component 3: Bulk Sync Manager (`sync_manager.py`)

### Classes

```python
class SyncProgress(BaseModel):
    """Tracks sync progress for resume capability."""
    dataset: str
    started_at: datetime
    last_checkpoint: datetime
    years_completed: list[int]
    years_remaining: list[int]
    total_rows_synced: int
    status: Literal["running", "paused", "completed", "failed"]

class SyncManager:
    """Orchestrates bulk data sync from WRDS to local Parquet."""

    def __init__(
        self,
        wrds_client: WRDSClient,
        storage_path: Path,
        lock_dir: Path,
        manifest_manager: ManifestManager,
        validator: DataValidator,
        schema_registry: SchemaRegistry,  # For schema drift detection
    ): ...

    # Core operations
    def full_sync(self, dataset: str, start_year: int = 2000) -> SyncManifest: ...
    def incremental_sync(self, dataset: str) -> SyncManifest: ...
    def verify_integrity(self, dataset: str) -> list[str]: ...

    # Atomic write operations
    def _atomic_write_parquet(self, df: pl.DataFrame, path: Path) -> str: ...
    def _quarantine_failed(self, temp_path: Path, reason: str) -> None: ...

    # Progress tracking
    def _save_progress(self, progress: SyncProgress) -> None: ...
    def _load_progress(self, dataset: str) -> SyncProgress | None: ...
    def _resume_from_checkpoint(self, progress: SyncProgress) -> None: ...

    # Disk space checks
    def _check_disk_space(self, required_bytes: int) -> DiskSpaceStatus: ...

    # Schema drift detection (using T1.1 SchemaRegistry)
    def _validate_schema(self, df: pl.DataFrame, dataset: str) -> SchemaDrift | None: ...
    def _handle_schema_drift(self, drift: SchemaDrift) -> None: ...
```

### Key Implementation Details

1. **Atomic Writes:**
   ```python
   temp_path = target.with_suffix('.parquet.tmp')
   df.write_parquet(temp_path)
   checksum = compute_md5(temp_path)
   os.fsync(fd)
   os.rename(temp_path, target)
   os.fsync(parent_dir_fd)  # Directory sync for crash safety
   ```

2. **Progress Checkpointing:**
   - Save progress after each year partition
   - Resume from last checkpoint on restart
   - Progress file: `data/sync_progress/{dataset}.json`

3. **Disk Space Checks:**
   - Before sync: require 2x expected write size
   - Watermarks: 80% warning, 90% critical, 95% blocked
   - On ENOSPC: quarantine temp files, abort cleanly

4. **Quarantine:**
   - Failed files → `data/quarantine/<timestamp>_<operation>/`
   - Auto-cleanup after 7 days

5. **Manifest Coupling:**
   - Manifest update only after successful fsync
   - Lock held throughout manifest update
   - Validation gate: manifest only updated if DataValidator passes

6. **Schema Drift Policy (using T1.1 SchemaRegistry):**
   ```python
   def _validate_schema(self, df: pl.DataFrame, dataset: str) -> SchemaDrift | None:
       expected = self.schema_registry.get_schema(dataset)
       actual = DatasetSchema.from_dataframe(df)
       drift = expected.detect_drift(actual)

       if drift.has_breaking_changes:
           # Removed columns or type changes → reject sync
           raise SchemaError(drift, "Breaking schema change detected")
       elif drift.new_columns:
           # New columns → accept with warning, register new schema
           logger.warning(f"New columns detected: {drift.new_columns}")
           self.schema_registry.register_schema(dataset, actual)

       return drift
   ```

---

## Component 4: CLI (`scripts/wrds_sync.py`)

### Commands

```bash
# Full sync (initial)
python scripts/wrds_sync.py full-sync --dataset crsp --start-year 2000

# Incremental sync (daily)
python scripts/wrds_sync.py incremental --dataset crsp
python scripts/wrds_sync.py incremental --all

# Status and verification
python scripts/wrds_sync.py status
python scripts/wrds_sync.py verify --dataset crsp

# Lock management
python scripts/wrds_sync.py lock-status
python scripts/wrds_sync.py force-unlock --dataset crsp  # Requires confirmation
```

### Implementation
- Use `typer` for CLI (already in dependencies)
- Structured logging with JSON output
- Exit codes: 0=success, 1=error, 2=lock held

---

## Test Cases

### Locking Tests (`test_locking.py`)
1. Lock acquisition succeeds when unlocked
2. Lock acquisition fails when locked by active process
3. Lock release deletes lock file
4. Stale lock detection (timeout exceeded)
5. Stale lock recovery (dead PID)
6. PID reuse handling (hostname + writer_id check)
7. Malformed JSON rejection
8. Extra fields in JSON rejected
9. Token validation on release
10. Lock refresh extends expiry
11. fsync called after lock creation

### Contention Tests (`test_lock_contention.py`)
12. Lock contention between two processes
13. Concurrent stale-lock recovery (deterministic winner)
14. Split-brain recovery scenario

### WRDS Client Tests (`test_wrds_client.py`)
15. Connection pool creation
16. Rate limiting respects QPM
17. Query timeout cancellation
18. Retry on transient errors
19. Credential loading from secrets

### Sync Manager Tests (`test_sync_manager.py`)
20. Atomic write: temp file created, renamed on success
21. Atomic write: temp file cleaned up on failure
22. Atomic write: readers never see .tmp files (concurrent read during write)
23. Checksum computed and stored in manifest
24. Progress checkpointing after each partition
25. Resume from checkpoint after crash
26. Disk space check blocks at 95%
27. Quarantine on checksum mismatch
28. Manifest update only after fsync
29. Manifest validation gate: update blocked if validation fails
30. Incremental sync appends correctly
31. Verify-only mode validates checksums without downloading
32. Verify integrity detects missing files
33. Verify integrity detects checksum mismatch
34. Full sync creates expected Parquet partitions
35. Interrupted sync rollback: no partial data visible
36. Network timeout resume: sync continues from checkpoint
37. Partial-year rerun idempotency: no duplicate data

### Schema Drift Tests (`test_sync_manager.py` - schema section)
38. Schema drift: new columns accepted with warning
39. Schema drift: removed columns rejected with SchemaError
40. Schema drift: type changes rejected with SchemaError
41. SyncManifest serialization/deserialization roundtrip
42. SyncManifest validation against SchemaRegistry

---

## Storage Layout

```
data/
├── wrds/
│   ├── crsp/
│   │   └── daily/
│   │       ├── 2020.parquet
│   │       └── 2024.parquet
│   ├── compustat/
│   └── fama_french/
├── locks/
│   ├── crsp.lock
│   └── compustat.lock
├── sync_progress/
│   ├── crsp.json
│   └── compustat.json
├── quarantine/
│   └── 2025-01-15T10-30-00_crsp_sync/
├── backups/
│   └── daily/
└── tmp/
    └── duckdb/
```

---

## Error Handling

| Error | Action |
|-------|--------|
| Lock held by another process | Wait with backoff, timeout after 30s |
| Lock stale (expired/dead PID) | Recover atomically, retry |
| Lock file malformed | Delete and recreate |
| WRDS connection failed | Retry 3x with exponential backoff |
| WRDS query timeout | Cancel, log, continue to next partition |
| Disk full (ENOSPC) | Quarantine temp, abort, alert |
| Checksum mismatch | Quarantine, don't update manifest, alert |
| Network timeout | Save progress, resume on next run |

---

## Dependencies

### External (already in requirements.txt)
- `polars` - DataFrame operations
- `sqlalchemy` - Connection pooling
- `psycopg2-binary` - PostgreSQL driver
- `typer` - CLI framework
- `pydantic-settings` - Configuration

### Internal
- `libs/data_quality` - Validation, manifest, types (T1.1)
- `libs/common.exceptions` - Base exceptions
- `libs/secrets` - Credential management

---

## Monitoring & Alerting (from P4T1 Spec)

The SyncManager emits structured log events for these alerting scenarios:

| Alert | Trigger | Log Event |
|-------|---------|-----------|
| Disk usage warning | >80% capacity | `sync.disk.warning` |
| Disk usage critical | >90% capacity | `sync.disk.critical` |
| Sync blocked | >95% capacity | `sync.disk.blocked` |
| Stale lock detected | Lock age >4 hours | `sync.lock.stale` |
| Checksum mismatch | File hash differs | `sync.checksum.mismatch` |
| Backup failure | rclone exit != 0 | `sync.backup.failed` |
| WRDS credential expiry | <30 days to expiry | `sync.credential.expiring` |
| Manifest-version mismatch | Files vs manifest differ | `sync.manifest.mismatch` |
| Sync duration SLO breach | Full >4h, incremental >1h | `sync.duration.slo_breach` |
| Late-data arrival | Not updated by 8 AM ET | `sync.data.late` |

These events are emitted via structured logging (JSON) and can be consumed by external alerting systems (Prometheus/Grafana via log metrics).

---

## Success Criteria

- [ ] All 42 test cases pass
- [ ] No race conditions in contention tests
- [ ] Atomic writes verified (no partial reads possible)
- [ ] Resume from checkpoint works after simulated crash
- [ ] Disk space checks prevent out-of-space failures
- [ ] Schema drift policy enforced (new columns warn, breaking changes reject)
- [ ] Type hints complete, mypy --strict passes
- [ ] ADR-012 documents architecture decisions
- [ ] All 5 runbooks created (lock recovery, backup, duckdb, credentials, storage)
- [ ] All 10 alerting events emitted via structured logging
