# T2.1 Multi-Factor Model Construction - Implementation Plan

**Component:** T2.1-Multi-Factor-Model
**Effort Estimate:** 4-5 days
**Dependencies:** T1.3 (CRSP), T1.4 (Compustat), T1.6 (Versioning) - All COMPLETE from P4T1

---

## 1. Overview

Implement a factor construction library that computes standard equity factors (value, momentum, quality, size, low-vol) from the local data warehouse. All computations are point-in-time (PIT) correct using P4T1 data providers.

## 2. Module Structure

```
libs/factors/
├── __init__.py              # Public API exports
├── factor_builder.py        # FactorBuilder: Main computation engine
├── factor_definitions.py    # Canonical factor definitions (Protocol + implementations)
└── factor_analytics.py      # IC analysis, decay curves, neutralization utilities

tests/libs/factors/
├── __init__.py
├── test_factor_builder.py   # Core builder tests
├── test_factor_definitions.py  # Factor computation tests
├── test_factor_analytics.py # Analytics tests
└── conftest.py              # Shared fixtures

docs/
├── CONCEPTS/factor-investing.md  # Educational documentation
└── ADRs/ADR-0020-factor-model-architecture.md  # Architecture decision record
```

## 3. Interface Design

### 3.1 FactorDefinition Protocol

```python
# libs/factors/factor_definitions.py
from typing import Protocol
from datetime import date
import polars as pl

class FactorDefinition(Protocol):
    """Protocol for factor computation."""

    @property
    def name(self) -> str:
        """Unique factor name (e.g., 'momentum_12_1')."""
        ...

    @property
    def category(self) -> str:
        """Factor category: 'value', 'momentum', 'quality', 'size', 'low_vol'."""
        ...

    @property
    def description(self) -> str:
        """Human-readable description."""
        ...

    @property
    def requires_fundamentals(self) -> bool:
        """Whether factor needs Compustat data."""
        ...

    def compute(
        self,
        prices: pl.DataFrame,
        fundamentals: pl.DataFrame | None,
        as_of_date: date,
    ) -> pl.DataFrame:
        """
        Compute factor exposures as of a specific date.

        Args:
            prices: CRSP daily data with columns: date, permno, ret, prc, vol, shrout
            fundamentals: Compustat data (if requires_fundamentals=True)
            as_of_date: Point-in-time date for computation

        Returns:
            DataFrame with columns: permno, factor_value
            Must be point-in-time correct (no look-ahead bias).
        """
        ...
```

### 3.2 FactorBuilder

```python
# libs/factors/factor_builder.py
from dataclasses import dataclass
from datetime import date
from pathlib import Path
import polars as pl

from libs.data.data_providers.crsp_local_provider import CRSPLocalProvider
from libs.data.data_providers.compustat_local_provider import CompustatLocalProvider
from libs.data.data_quality.manifest import ManifestManager

@dataclass
class FactorConfig:
    """Configuration for factor computation."""
    winsorize_pct: float = 0.01      # Winsorize at 1%/99% percentiles
    neutralize_sector: bool = True   # Sector-neutralize factors
    min_stocks_per_sector: int = 5   # Minimum for neutralization
    lookback_days: int = 252         # For momentum, volatility

@dataclass
class FactorResult:
    """Result of factor computation with metadata."""
    exposures: pl.DataFrame          # permno, date, factor_name, raw_value, zscore, percentile, dataset_version_id
    as_of_date: date
    dataset_version_ids: dict[str, str]  # {'crsp': 'v1.2.3', 'compustat': 'v1.0.1'}
    computation_timestamp: datetime
    reproducibility_hash: str        # SHA-256 of inputs

    def to_storage_format(self) -> pl.DataFrame:
        """
        Convert to storage format with single dataset_version_id column.

        Storage contract: The `dataset_version_id` column in parquet encodes
        multiple source versions as a combined string:
        'crsp:v1.2.3|compustat:v1.0.1'

        This matches P4T2_TASK.md schema while preserving full provenance.
        """
        version_str = "|".join(f"{k}:{v}" for k, v in sorted(self.dataset_version_ids.items()))
        return self.exposures.with_columns([
            pl.lit(version_str).alias("dataset_version_id"),
            pl.lit(self.computation_timestamp).alias("computation_timestamp"),
        ])

    def validate(self) -> list[str]:
        """
        Check for nulls, infs, z-scores within +/- 5 sigma.

        Returns:
            List of validation error messages (empty if valid).

        Raises:
            No exceptions - returns error list for caller to handle.
        """
        errors: list[str] = []

        # Check for nulls
        null_counts = self.exposures.null_count()
        for col in ["raw_value", "zscore", "percentile"]:
            if col in null_counts.columns and null_counts[col][0] > 0:
                errors.append(f"Column '{col}' has {null_counts[col][0]} null values")

        # Check for infs
        for col in ["raw_value", "zscore", "percentile"]:
            if col in self.exposures.columns:
                inf_count = self.exposures.filter(
                    pl.col(col).is_infinite()
                ).height
                if inf_count > 0:
                    errors.append(f"Column '{col}' has {inf_count} infinite values")

        # Check z-scores within +/- 5 sigma
        if "zscore" in self.exposures.columns:
            extreme_count = self.exposures.filter(
                pl.col("zscore").abs() > 5.0
            ).height
            if extreme_count > 0:
                errors.append(f"Found {extreme_count} z-scores exceeding +/- 5 sigma")

        return errors

class FactorBuilder:
    """
    Build factor exposures from local data warehouse.

    Uses CRSP for prices/returns and Compustat for fundamentals.
    All computations are point-in-time correct.
    """

    def __init__(
        self,
        crsp_provider: CRSPLocalProvider,
        compustat_provider: CompustatLocalProvider,
        manifest_manager: ManifestManager,
        config: FactorConfig | None = None,
    ):
        self.crsp = crsp_provider
        self.compustat = compustat_provider
        self.manifest = manifest_manager
        self.config = config or FactorConfig()
        self._registry: dict[str, FactorDefinition] = {}

        # Register canonical factors on init
        self._register_canonical_factors()

    def register_factor(self, factor: FactorDefinition) -> None:
        """Register a factor definition."""
        self._registry[factor.name] = factor

    def compute_factor(
        self,
        factor_name: str,
        as_of_date: date,
        universe: list[int] | None = None,  # PERMNOs
        snapshot_date: date | None = None,  # Override for PIT time-travel
    ) -> FactorResult:
        """
        Compute single factor for given date.

        Args:
            factor_name: Registered factor name
            as_of_date: Point-in-time date for computation
            universe: Optional list of PERMNOs (None = all stocks)
            snapshot_date: Override data snapshot date for time-travel queries.
                          If None, uses current manifest versions.
                          Used for PIT regression testing and reproducibility.
        """
        ...

    def compute_all_factors(
        self,
        as_of_date: date,
        universe: list[int] | None = None,
        snapshot_date: date | None = None,
    ) -> FactorResult:
        """Compute all registered factors for given date."""
        ...

    def compute_composite(
        self,
        factor_names: list[str],
        weights: list[float] | Literal["equal", "ic_weighted"],
        as_of_date: date,
        universe: list[int] | None = None,
        snapshot_date: date | None = None,
    ) -> FactorResult:
        """Compute composite factor from multiple factors."""
        ...

    def _winsorize(self, df: pl.DataFrame, col: str) -> pl.DataFrame:
        """Winsorize column at configured percentiles."""
        ...

    def _get_pit_sector_mappings(self, permnos: list[int], as_of_date: date) -> pl.DataFrame:
        """
        Get point-in-time sector mappings for securities.

        PIT Sector Retrieval:
        - Uses Compustat GICS codes from the most recent AVAILABLE filing
        - Respects filing lag: sector = sector from filing where datadate + lag <= as_of_date
        - This prevents leaking future sector reclassifications into historical analysis

        Args:
            permnos: List of CRSP PERMNOs
            as_of_date: Point-in-time date

        Returns:
            DataFrame with columns: permno, gics_sector (2-digit code)
        """
        ...

    def _neutralize_sector(self, df: pl.DataFrame, col: str, as_of_date: date) -> pl.DataFrame:
        """
        Sector-neutralize factor using PIT-correct GICS codes.

        Uses _get_pit_sector_mappings() to ensure sector assignments
        are point-in-time correct (no future reclassifications leaked).
        """
        ...

    def _compute_zscore(self, df: pl.DataFrame, col: str) -> pl.DataFrame:
        """Compute cross-sectional z-score."""
        ...
```

### 3.3 Canonical Factors (5 Required)

| Factor | Category | Description | Data Source |
|--------|----------|-------------|-------------|
| `book_to_market` | value | Book value / Market cap | Compustat (ceq) + CRSP (prc, shrout) |
| `momentum_12_1` | momentum | 12-month return, skip last month | CRSP (ret) |
| `roe` | quality | Return on Equity (NI / CEQ) | Compustat (ni, ceq) |
| `log_market_cap` | size | Log of market capitalization | CRSP (prc, shrout) |
| `realized_vol` | low_vol | 60-day realized volatility | CRSP (ret) |

### 3.4 Factor Analytics

```python
# libs/factors/factor_analytics.py
@dataclass
class ICAnalysis:
    """Information Coefficient analysis results."""
    factor_name: str
    ic_mean: float           # Average IC
    ic_std: float            # IC standard deviation
    icir: float              # IC Information Ratio (ic_mean / ic_std)
    t_statistic: float       # Statistical significance
    hit_rate: float          # % of periods with positive IC

class FactorAnalytics:
    """Analytics for factor evaluation."""

    def compute_ic(
        self,
        factor_exposures: pl.DataFrame,
        forward_returns: pl.DataFrame,
        horizons: list[int] = [1, 5, 20],  # Days ahead
    ) -> dict[str, ICAnalysis]:
        """Compute Information Coefficient for each factor."""
        ...

    def analyze_decay(
        self,
        factor_exposures: pl.DataFrame,
        returns: pl.DataFrame,
        max_horizon: int = 60,
    ) -> pl.DataFrame:
        """Analyze factor predictiveness over time horizons."""
        ...

    def compute_turnover(
        self,
        factor_exposures: pl.DataFrame,
    ) -> pl.DataFrame:
        """Compute factor turnover (rank correlation between periods)."""
        ...

    def compute_correlation_matrix(
        self,
        factor_exposures: pl.DataFrame,
    ) -> pl.DataFrame:
        """Compute pairwise factor correlations."""
        ...
```

## 4. PIT/Versioning Integration

All factor computations MUST integrate with P4T1 Dataset Versioning.

**Dependency on ManifestManager:** The `snapshot_date` time-travel feature requires
`ManifestManager.load_manifest_at_date(dataset_name, snapshot_date)` method from P4T1.
This method returns the manifest version that was active on the specified date,
enabling reproducible historical factor computation. If not yet implemented, this
becomes a prerequisite or can fall back to latest manifest with a warning.

```python
# Required pattern for every factor computation
def compute_factor(
    self,
    factor_name: str,
    as_of_date: date,
    universe: list[int] | None = None,
    snapshot_date: date | None = None,
) -> FactorResult:
    # 1. Get manifest versions (current or time-travel to snapshot_date)
    if snapshot_date is not None:
        # Time-travel: load manifests as they existed on snapshot_date
        crsp_manifest = self.manifest.load_manifest_at_date("crsp_daily", snapshot_date)
        compustat_manifest = self.manifest.load_manifest_at_date("compustat_annual", snapshot_date)
    else:
        # Current: use latest manifests
        crsp_manifest = self.manifest.load_manifest("crsp_daily")
        compustat_manifest = self.manifest.load_manifest("compustat_annual")

    # 2. Compute reproducibility hash
    input_hash = hashlib.sha256(
        f"{factor_name}:{as_of_date}:{crsp_manifest.manifest_version}:"
        f"{compustat_manifest.manifest_version}".encode()
    ).hexdigest()

    # 3. Get data with PIT correctness
    prices = self.crsp.get_daily_prices(
        start_date=as_of_date - timedelta(days=self.config.lookback_days),
        end_date=as_of_date,
        as_of_date=as_of_date,
    )

    fundamentals = self.compustat.get_annual_fundamentals(
        start_date=as_of_date - timedelta(days=365*3),
        end_date=as_of_date,
        as_of_date=as_of_date,  # Enforces filing lag
    )

    # 4. Compute factor
    factor_def = self._registry[factor_name]
    raw_values = factor_def.compute(prices, fundamentals, as_of_date)

    # 5. Apply transformations
    winsorized = self._winsorize(raw_values, "factor_value")
    zscored = self._compute_zscore(winsorized, "factor_value")

    if self.config.neutralize_sector:
        zscored = self._neutralize_sector(zscored, "zscore")

    # 6. Build result with version tracking
    return FactorResult(
        exposures=zscored,
        as_of_date=as_of_date,
        dataset_version_ids={
            "crsp": f"v{crsp_manifest.manifest_version}",
            "compustat": f"v{compustat_manifest.manifest_version}",
        },
        computation_timestamp=datetime.now(UTC),
        reproducibility_hash=input_hash,
    )
```

## 5. Output Storage Schema

Factor outputs stored in Parquet per P4T2_TASK.md:

```sql
-- data/analytics/factor_exposures.parquet (partitioned by date)
CREATE TABLE factor_exposures (
    date DATE NOT NULL,
    permno INTEGER NOT NULL,
    factor_name VARCHAR NOT NULL,
    raw_value DOUBLE,
    zscore DOUBLE,
    percentile DOUBLE,
    dataset_version_id VARCHAR NOT NULL,
    computation_timestamp TIMESTAMP NOT NULL,
    PRIMARY KEY (date, permno, factor_name)
);
```

## 6. Testing Strategy

### 6.1 Unit Tests

```python
# tests/libs/factors/test_factor_definitions.py

class TestMomentumFactor:
    def test_momentum_uses_only_past_returns(self, mock_crsp_data):
        """Verify no look-ahead bias in momentum computation."""
        factor = Momentum12_1Factor()
        as_of_date = date(2023, 6, 15)

        result = factor.compute(mock_crsp_data, None, as_of_date)

        # Verify only data before as_of_date was used
        max_date_used = ... # extract from computation log
        assert max_date_used <= as_of_date

    def test_momentum_skips_last_month(self, mock_crsp_data):
        """Verify 12-1 skips the most recent month."""
        ...

    def test_momentum_handles_missing_returns(self, mock_crsp_data):
        """Verify graceful handling of stocks with gaps."""
        ...

class TestValueFactor:
    def test_book_to_market_uses_filing_lag(self, mock_compustat_data):
        """Verify B/M respects Compustat filing lag."""
        factor = BookToMarketFactor()
        as_of_date = date(2023, 3, 1)  # Q4 data not yet available

        result = factor.compute(mock_crsp_data, mock_compustat_data, as_of_date)

        # Should use Q3 data (not Q4)
        assert result["datadate"].max() <= date(2023, 9, 30)
```

### 6.2 Integration Tests

```python
# tests/libs/factors/test_factor_builder.py

@pytest.mark.integration
class TestFactorBuilderIntegration:
    def test_full_factor_computation(self, real_crsp_provider, real_compustat_provider):
        """End-to-end test with real data."""
        builder = FactorBuilder(real_crsp_provider, real_compustat_provider)

        result = builder.compute_all_factors(
            as_of_date=date(2023, 1, 31),
            universe=None,  # All stocks
        )

        # Validate shape
        assert len(result.exposures) > 0
        assert set(result.exposures["factor_name"].unique()) == {
            "book_to_market", "momentum_12_1", "roe", "log_market_cap", "realized_vol"
        }

        # Validate no look-ahead
        validation_errors = result.validate()
        assert len(validation_errors) == 0

    def test_reproducibility_same_version(self, ...):
        """Same data version produces identical results."""
        result1 = builder.compute_factor("momentum_12_1", as_of_date)
        result2 = builder.compute_factor("momentum_12_1", as_of_date)

        assert result1.reproducibility_hash == result2.reproducibility_hash
        assert_frame_equal(result1.exposures, result2.exposures)
```

### 6.3 PIT Validation Tests

```python
# tests/libs/factors/test_pit_correctness.py

@pytest.mark.pit
class TestPITCorrectness:
    def test_factor_differs_with_different_data_versions(self, ...):
        """Factors computed at T should differ from T-1 if data changed."""
        as_of_date = date(2023, 6, 15)

        # Compute with T-1 snapshot (data as it existed on 2023-12-31)
        result_t_minus_1 = builder.compute_factor(
            "book_to_market",
            as_of_date=as_of_date,
            snapshot_date=date(2023, 12, 31),
        )

        # Compute with T snapshot (data as it existed on 2024-01-01, after update)
        result_t = builder.compute_factor(
            "book_to_market",
            as_of_date=as_of_date,
            snapshot_date=date(2024, 1, 1),
        )

        # Should be different (proves PIT correctness with time-travel)
        assert result_t.reproducibility_hash != result_t_minus_1.reproducibility_hash

    def test_sector_neutralization_uses_pit_sectors(self, ...):
        """Verify sector neutralization uses PIT-correct sector assignments."""
        # Stock AAPL had sector change on 2023-06-01
        # Neutralization as of 2023-05-15 should use OLD sector
        ...
```

### 6.4 Validate() Contract Tests (Addresses LOW severity issue)

```python
# tests/libs/factors/test_factor_result_validation.py

class TestFactorResultValidation:
    """Tests for FactorResult.validate() error detection."""

    def test_validate_detects_null_values(self):
        """validate() catches null values in required columns."""
        df = pl.DataFrame({
            "permno": [1, 2, 3],
            "raw_value": [1.0, None, 3.0],  # Contains null
            "zscore": [0.5, 0.2, 0.1],
            "percentile": [0.9, 0.5, 0.1],
        })
        result = FactorResult(exposures=df, ...)

        errors = result.validate()

        assert len(errors) == 1
        assert "null values" in errors[0]

    def test_validate_detects_infinite_values(self):
        """validate() catches infinite values."""
        df = pl.DataFrame({
            "permno": [1, 2, 3],
            "raw_value": [1.0, float('inf'), 3.0],  # Contains inf
            "zscore": [0.5, 0.2, 0.1],
            "percentile": [0.9, 0.5, 0.1],
        })
        result = FactorResult(exposures=df, ...)

        errors = result.validate()

        assert len(errors) == 1
        assert "infinite values" in errors[0]

    def test_validate_detects_extreme_zscores(self):
        """validate() catches z-scores exceeding +/- 5 sigma."""
        df = pl.DataFrame({
            "permno": [1, 2, 3],
            "raw_value": [1.0, 2.0, 3.0],
            "zscore": [0.5, 6.5, 0.1],  # 6.5 > 5 sigma
            "percentile": [0.9, 0.5, 0.1],
        })
        result = FactorResult(exposures=df, ...)

        errors = result.validate()

        assert len(errors) == 1
        assert "5 sigma" in errors[0]

    def test_validate_returns_all_errors(self):
        """validate() collects and returns ALL errors, not just first."""
        df = pl.DataFrame({
            "permno": [1, 2, 3],
            "raw_value": [None, float('inf'), 3.0],  # null AND inf
            "zscore": [0.5, 6.5, float('-inf')],     # extreme AND inf
            "percentile": [0.9, 0.5, None],         # null
        })
        result = FactorResult(exposures=df, ...)

        errors = result.validate()

        # Should detect multiple errors
        assert len(errors) >= 3

    def test_validate_empty_on_valid_data(self):
        """validate() returns empty list for valid data."""
        df = pl.DataFrame({
            "permno": [1, 2, 3],
            "raw_value": [1.0, 2.0, 3.0],
            "zscore": [0.5, -0.5, 0.1],
            "percentile": [0.9, 0.5, 0.1],
        })
        result = FactorResult(exposures=df, ...)

        errors = result.validate()

        assert len(errors) == 0
```

### 6.5 Test Coverage Gate (Addresses MEDIUM severity issue)

**pytest.ini configuration:**
```ini
[pytest]
# ... existing config ...
addopts = --cov=libs/factors --cov-report=term-missing --cov-fail-under=90
```

**CI configuration (.github/workflows/test.yml):**
```yaml
- name: Run factor tests with coverage
  run: |
    pytest tests/libs/factors/ \
      --cov=libs/factors \
      --cov-report=xml \
      --cov-fail-under=90
  env:
    PYTHONPATH: .

- name: Upload coverage to Codecov
  uses: codecov/codecov-action@v3
  with:
    files: ./coverage.xml
    fail_ci_if_error: true
```

**Makefile target:**
```makefile
test-factors-coverage:
	pytest tests/libs/factors/ --cov=libs/factors --cov-report=html --cov-fail-under=90
	@echo "Coverage report: htmlcov/index.html"
```

### 6.6 Performance Validation Tests

```python
# tests/libs/factors/test_performance.py

@pytest.mark.benchmark
class TestFactorPerformance:
    """Performance regression tests for factor computation."""

    def test_single_factor_under_5_seconds(self, benchmark_data):
        """Single factor computation completes in <5s for 3000 stocks."""
        builder = FactorBuilder(...)

        start = time.perf_counter()
        result = builder.compute_factor("momentum_12_1", as_of_date)
        elapsed = time.perf_counter() - start

        assert elapsed < 5.0, f"Factor computation took {elapsed:.2f}s, expected <5s"
        assert len(result.exposures) >= 2500  # At least 2500 stocks

    def test_winsorization_under_1_second(self, benchmark_data):
        """Winsorization completes in <1s for 3000 stocks."""
        ...

    def test_neutralization_under_2_seconds(self, benchmark_data):
        """Sector neutralization completes in <2s for 3000 stocks."""
        ...
```

## 7. Performance Requirements

| Operation | Target | Data Volume |
|-----------|--------|-------------|
| Single factor computation | <5s | 3000 stocks, 252 days |
| All factors computation | <60s | 3000 stocks |
| Z-score + winsorization | <1s | 3000 stocks |
| Sector neutralization | <2s | 3000 stocks |

## 8. Implementation Order

1. **Day 1:** Core infrastructure
   - Create module structure
   - Implement `FactorDefinition` protocol
   - Implement `FactorConfig` and `FactorResult` dataclasses
   - Basic `FactorBuilder` skeleton

2. **Day 2:** Canonical factors (prices only)
   - `momentum_12_1` factor
   - `log_market_cap` factor
   - `realized_vol` factor
   - Unit tests for each

3. **Day 3:** Fundamental factors
   - `book_to_market` factor (requires Compustat integration)
   - `roe` factor
   - PIT correctness tests
   - Unit tests for each

4. **Day 4:** Analytics and transformations
   - Winsorization
   - Z-score computation
   - Sector neutralization
   - `FactorAnalytics` class (IC, decay, correlation)

5. **Day 5:** Integration and documentation
   - Integration tests with real data
   - ADR-0020 documentation
   - CONCEPTS/factor-investing.md
   - Performance testing
   - Code review fixes

## 9. Risk Mitigation

| Risk | Mitigation |
|------|------------|
| Compustat GICS codes missing | Fall back to no neutralization, log warning |
| Look-ahead bias in fundamentals | Enforce filing_lag_days in all Compustat queries |
| Numeric instability in z-scores | Use robust statistics (median/MAD) as fallback |
| Large universes slow computation | Batch by date, use DuckDB for aggregations |

## 10. Acceptance Criteria Checklist

- [ ] 5+ canonical factors implemented (value, momentum, quality, size, low-vol)
- [ ] Cross-sectional z-score computation with winsorization
- [ ] Sector neutralization using Compustat GICS codes
- [ ] Point-in-time correctness verified with PIT validation tests
- [ ] Factor correlation matrix computation
- [ ] `validate()` method checking for nulls, infs, z-scores within +/- 5 sigma
- [ ] Outputs stored with dataset_version_id metadata for reproducibility
- [ ] >90% test coverage
- [ ] ADR-0020 documented
- [ ] CONCEPTS/factor-investing.md created

---

**Plan Status:** Ready for Review
**Next Step:** Request plan review from Gemini + Codex (fresh, no continuation ID)
