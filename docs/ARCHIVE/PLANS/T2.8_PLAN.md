# T2.8 Model Registry Implementation Plan

**Task:** T2.8 Model Registry Integration
**Effort:** 5-6 days
**Developer:** DEV B (Week 10)
**Created:** 2025-12-08
**Revision:** 3

---

## Executive Summary

Implement a production-ready model registry for versioned model storage, deployment, and hot-reloading. This includes:
1. DuckDB-based model catalog with immutable versioning
2. Artifact serialization with SHA-256 checksums
3. ProductionModelLoader for signal_service integration
4. FastAPI endpoints with JWT authentication
5. DiskExpressionCache for computed factor caching
6. Version compatibility checking with P4T1 dataset versions
7. CLI commands for model lifecycle management
8. Backup/retention policies and GC jobs

---

## Dependencies (Verified Complete)

| Dependency | Status | Location |
|------------|--------|----------|
| T2.4 Portfolio Optimizer | COMPLETE | `libs/risk/portfolio_optimizer.py` |
| T2.6 Alpha Advanced | COMPLETE | `libs/alpha/alpha_combiner.py` |
| T1.6 Dataset Versioning | COMPLETE | `libs/data_quality/versioning.py` |
| DatasetVersionManager | COMPLETE | `libs/data_quality/versioning.py` |
| SnapshotManifest | COMPLETE | `libs/data_quality/versioning.py` |

---

## Architecture Overview

```
data/models/
├── registry.db           # DuckDB catalog (query index)
├── manifest.json         # Registry manifest (DR/discoverability)
├── artifacts/
│   ├── risk_model/
│   │   ├── v1.0.0/
│   │   │   ├── model.pkl
│   │   │   ├── metadata.json   # AUTHORITATIVE source for full metadata
│   │   │   └── checksum.sha256
│   │   └── v1.1.0/
│   ├── alpha_weights/
│   └── factor_definitions/
└── backups/              # Daily backups

libs/factors/cache/       # DiskExpressionCache (per spec ~2740-2755)
├── momentum_12m_2024-01-15_crsp-v1.2.3_snap_abc123_cfg_def456.parquet
└── ...
```

---

## Implementation Components

### Component 1: Core Data Models (`libs/models/types.py`)

**Files:**
- `libs/models/__init__.py`
- `libs/models/types.py`

**Data Models (Complete per spec ~2050-2085):**
```python
class ModelType(str, Enum):
    risk_model = "risk_model"
    alpha_weights = "alpha_weights"
    factor_definitions = "factor_definitions"
    feature_transforms = "feature_transforms"

class ModelStatus(str, Enum):
    staged = "staged"
    production = "production"
    archived = "archived"
    failed = "failed"

@dataclass
class EnvironmentMetadata:
    python_version: str           # e.g., "3.11.5"
    dependencies_hash: str        # SHA-256 of sorted requirements.txt
    platform: str                 # e.g., "linux-x86_64"
    created_by: str               # User/service that created
    # Key library versions
    numpy_version: str
    polars_version: str
    sklearn_version: str | None
    cvxpy_version: str | None

@dataclass
class ModelMetadata:
    model_id: str
    model_type: ModelType
    version: str                  # Semantic version (immutable)
    created_at: datetime
    # Provenance tracking (FULL linkage to P4T1)
    dataset_version_ids: dict[str, str]  # {'crsp': 'v1.2.3', 'compustat': 'v1.0.1'}
    snapshot_id: str              # DatasetVersionManager snapshot ID
    factor_list: list[str]
    parameters: dict
    # Validation
    checksum_sha256: str
    metrics: dict[str, float]     # IC, Sharpe, etc.
    env: EnvironmentMetadata
    # Training config (per spec ~2070-2075)
    config: dict                  # Hyperparams, settings
    config_hash: str              # SHA-256 of config
    feature_formulas: list[str] | None  # Phase 3 placeholder for FormulaicFactor
    # Qlib compatibility (per spec ~2074-2078)
    experiment_id: str | None     # Experiment grouping ID (nullable for non-Qlib)
    run_id: str | None            # Individual training run ID (nullable)
    dataset_uri: str | None       # Reference to dataset location
    qlib_version: str | None      # Qlib version if used
```

**Per-Artifact Required Metadata (per spec ~2021-2027):**
```python
# Validation schemas for each artifact type
ARTIFACT_REQUIRED_FIELDS: dict[ModelType, list[str]] = {
    ModelType.risk_model: ["factor_list", "halflife_days", "shrinkage_intensity"],
    ModelType.alpha_weights: ["alpha_names", "combination_method", "ic_threshold"],
    ModelType.factor_definitions: ["factor_names", "categories", "lookback_days"],
    ModelType.feature_transforms: ["feature_names", "normalization_params"],
}

def validate_artifact_metadata(model_type: ModelType, metadata: ModelMetadata) -> None:
    """Validate required fields are present in parameters dict."""
    required = ARTIFACT_REQUIRED_FIELDS[model_type]
    for field in required:
        if field not in metadata.parameters:
            raise MissingRequiredFieldError(
                f"Artifact type {model_type} requires '{field}' in parameters"
            )
```

**Key Features:**
- Pydantic models with strict validation
- Immutable version enforcement
- Complete dataset version linkage
- Nullable Qlib fields for non-Qlib models
- Per-artifact type validation

---

### Component 2: DuckDB Registry Catalog (`libs/models/registry.py`)

**Files:**
- `libs/models/registry.py`

**Schema (Complete per spec ~1993-2018):**
```sql
CREATE TABLE models (
    model_id VARCHAR PRIMARY KEY,
    model_type VARCHAR NOT NULL,
    version VARCHAR NOT NULL,
    status VARCHAR NOT NULL DEFAULT 'staged',
    artifact_path VARCHAR NOT NULL,
    checksum_sha256 VARCHAR NOT NULL,
    created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
    promoted_at TIMESTAMP,
    archived_at TIMESTAMP,
    -- Key provenance fields for querying (full data in metadata.json sidecar)
    config_hash VARCHAR NOT NULL,
    snapshot_id VARCHAR NOT NULL,
    dataset_version_ids_json VARCHAR NOT NULL,  -- JSON serialized for querying
    metrics_json VARCHAR,                        -- JSON serialized metrics
    factor_list_json VARCHAR,                    -- JSON serialized factor list
    -- Qlib fields (nullable)
    experiment_id VARCHAR,
    run_id VARCHAR,
    dataset_uri VARCHAR,
    qlib_version VARCHAR,
    UNIQUE(model_type, version),
    CHECK (status IN ('staged', 'production', 'archived', 'failed'))
);

CREATE TABLE promotion_history (
    id INTEGER PRIMARY KEY,
    model_id VARCHAR NOT NULL REFERENCES models(model_id),
    from_status VARCHAR NOT NULL,
    to_status VARCHAR NOT NULL,
    changed_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
    changed_by VARCHAR NOT NULL   -- 'cli:user' or 'api:service'
);
```

**Authoritative Data Source:**
- **metadata.json sidecar** is the AUTHORITATIVE source for full metadata
- **DuckDB** stores key fields for efficient querying/filtering
- API responses load from metadata.json to ensure complete provenance

**Methods:**
- `register_model(model, metadata)` → str (model_id)
  - **MUST validate dataset_version_ids against P4T1 registry**
  - **MUST validate per-artifact required fields**
  - Raises `InvalidDatasetVersionError` if version not found
  - Raises `MissingRequiredFieldError` if artifact fields missing
- `promote_model(model_type, version)` → PromotionResult
  - **Enforces promotion gates** (IC > 0.02, Sharpe > 0.5, 24h paper trade)
- `rollback_model(model_type)` → RollbackResult
- `get_current_production(model_type)` → ModelMetadata | None
- `get_model_metadata(model_type, version)` → ModelMetadata
- `list_models(model_type, status)` → list[ModelMetadata]
- `validate_model(model_type, version)` → ValidationResult

**DatasetVersionManager Validation (per spec ~2096-2106):**
```python
def register_model(self, model, metadata: ModelMetadata) -> str:
    # 1. Validate per-artifact required fields
    validate_artifact_metadata(metadata.model_type, metadata)

    # 2. Validate dataset versions at registration time
    for dataset, version_id in metadata.dataset_version_ids.items():
        if not self.version_manager.validate_exists(dataset, version_id):
            raise InvalidDatasetVersionError(
                f"{dataset}:{version_id} not found in P4T1 registry"
            )
    # 3. Validate snapshot_id exists
    if not self.version_manager.get_snapshot(metadata.snapshot_id):
        raise InvalidSnapshotError(f"Snapshot {metadata.snapshot_id} not found")
    # 4. Store full metadata to sidecar + key fields to DB
    ...
```

**Promotion Gates (per spec ~2134-2138):**
```python
class PromotionGates:
    min_ic: float = 0.02
    min_sharpe: float = 0.5
    min_paper_trade_hours: int = 24

def promote_model(self, model_type: str, version: str) -> PromotionResult:
    metadata = self.get_model_metadata(model_type, version)
    # Check metric thresholds
    if metadata.metrics.get("ic", 0) < self.gates.min_ic:
        raise PromotionGateError(f"IC {metadata.metrics['ic']} < {self.gates.min_ic}")
    if metadata.metrics.get("sharpe", 0) < self.gates.min_sharpe:
        raise PromotionGateError(f"Sharpe below threshold")
    # Check paper trading period
    if not self._verify_paper_trade_period(metadata, self.gates.min_paper_trade_hours):
        raise PromotionGateError("24h paper trading period not completed")
    ...
```

**Concurrency:**
- DuckDB transaction-level locking (aligned with P4T1 policy)
- Atomic writes using temp file + rename pattern
- Single-writer, multi-reader pattern

---

### Component 3: Registry Manifest (`libs/models/manifest.py`)

**Files:**
- `libs/models/manifest.py`

**manifest.json Structure:**
```python
@dataclass
class RegistryManifest:
    """Registry-level manifest for discoverability and DR."""
    registry_version: str         # Schema version (e.g., "1.0.0")
    created_at: datetime
    last_updated: datetime
    artifact_count: int
    production_models: dict[str, str]  # {model_type: version}
    total_size_bytes: int
    checksum: str                 # SHA-256 of registry.db

    # DR fields
    last_backup_at: datetime | None
    backup_location: str | None   # S3/GCS path if configured
```

**Manifest Lifecycle:**
- **Creation:** On first registry initialization
- **Update:** After every register/promote/rollback/GC operation
- **Read:** On registry load for validation, DR restore verification
- **Integrity:** Checksum of registry.db verified on load

**Methods:**
```python
class RegistryManifestManager:
    def load_manifest(self) -> RegistryManifest
    def update_manifest(self, registry: ModelRegistry) -> None
    def verify_integrity(self) -> bool  # Checksum match
    def get_production_summary(self) -> dict[str, str]
```

---

### Component 4: Serialization (`libs/models/serialization.py`)

**Files:**
- `libs/models/serialization.py`

**Features:**
- Pickle/joblib serialization with configurable protocol
- JSON serialization for alpha_weights (per spec)
- SHA-256 checksum generation and verification
- Atomic write using temp file + atomic rename
- Metadata JSON sidecar file with ALL fields
- Environment metadata capture

**Methods:**
- `serialize_model(model, path, metadata)` → ArtifactInfo
- `deserialize_model(path)` → tuple[Any, ModelMetadata]
- `compute_checksum(path)` → str
- `verify_checksum(path, expected)` → bool
- `capture_environment()` → EnvironmentMetadata
- `compute_config_hash(config: dict)` → str

**Error Handling:**
- ChecksumMismatchError for corruption detection
- PartialWriteError for incomplete artifacts
- DeserializationError for incompatible artifacts

---

### Component 5: ProductionModelLoader (`libs/models/loader.py`)

**Files:**
- `libs/models/loader.py`

**Integration Contract (per spec ~2250-2285):**
```python
class ProductionModelLoader:
    """Load models from registry for signal_service."""

    def get_risk_model(self, version: str | None = None) -> BarraRiskModel
    def get_alpha_weights(self, version: str | None = None) -> dict[str, float]
    def get_factor_definitions(self, version: str | None = None) -> dict
    def get_current_version(self, model_type: ModelType) -> str | None
    def check_compatibility(
        self,
        model_id: str,
        current_versions: dict[str, str]
    ) -> tuple[bool, list[str]]  # (compatible, drift_warnings)
    def on_load_failure(self, model_id: str, error: Exception) -> None
```

**Features:**
- Version polling with configurable interval (60s default)
- Atomic model swap (load → validate → swap)
- Fallback to last-good version on load failure
- Circuit breaker integration on consecutive failures (3 failures → trigger)
- In-memory cache with 24h TTL for resilience

---

### Component 6: FastAPI Endpoints (`apps/model_registry/routes.py`)

**Files:**
- `apps/model_registry/__init__.py`
- `apps/model_registry/routes.py`
- `apps/model_registry/schemas.py`
- `apps/model_registry/auth.py`

**Endpoints (per spec ~2177-2213):**
```python
# Mounted at /api/v1/models

@router.get("/{model_type}/current", response_model=CurrentModelResponse)
async def get_current_model(
    model_type: ModelType,
    auth: ServiceToken = Depends(verify_service_token)  # Requires 'model:read' scope
) -> CurrentModelResponse:
    """Returns current production model version, checksum, dataset_version_ids."""

@router.get("/{model_type}/{version}", response_model=ModelMetadataResponse)
async def get_model_metadata(
    model_type: ModelType,
    version: str,
    auth: ServiceToken = Depends(verify_service_token)  # Requires 'model:read' scope
) -> ModelMetadataResponse:
    """Returns full metadata including artifact_path, env, config."""

@router.post("/{model_type}/{version}/validate", response_model=ValidationResult)
async def validate_model(
    model_type: ModelType,
    version: str,
    auth: ServiceToken = Depends(verify_service_token)  # Requires 'model:write' scope
) -> ValidationResult:
    """Triggers checksum verification + test load."""
```

**Response Schemas (per spec ~2155-2175):**
```python
class CurrentModelResponse(BaseModel):
    model_type: str
    version: str
    checksum: str
    dataset_version_ids: dict[str, str]

class ModelMetadataResponse(BaseModel):
    model_id: str
    model_type: str
    version: str
    status: str
    artifact_path: str
    checksum_sha256: str
    dataset_version_ids: dict[str, str]
    snapshot_id: str
    factor_list: list[str]
    parameters: dict              # Includes per-artifact required fields
    metrics: dict[str, float]
    config: dict
    config_hash: str
    feature_formulas: list[str] | None  # Phase 3 placeholder
    env: EnvironmentMetadata
    # Qlib compatibility fields (all nullable)
    experiment_id: str | None
    run_id: str | None
    dataset_uri: str | None       # Added per Codex review
    qlib_version: str | None
    created_at: datetime
    promoted_at: datetime | None
```

**HTTP Error Semantics (per spec ~2407-2415):**
```python
# Error response codes and semantics
HTTP_ERRORS = {
    200: "Success",
    404: "Model/version not found",
    409: "Version already exists (immutable, conflict)",
    422: "Checksum mismatch, corrupt artifact, or missing required fields",
    503: "Registry temporarily locked or unavailable",
}

# Example error responses
class ErrorResponse(BaseModel):
    detail: str
    code: str  # e.g., "MODEL_NOT_FOUND", "VERSION_EXISTS", "CHECKSUM_MISMATCH"

# Error handling in routes
@router.get("/{model_type}/{version}")
async def get_model_metadata(...):
    try:
        metadata = registry.get_model_metadata(model_type, version)
    except ModelNotFoundError:
        raise HTTPException(status_code=404, detail={"detail": f"Model {model_type}/{version} not found", "code": "MODEL_NOT_FOUND"})
    except RegistryLockError:
        raise HTTPException(status_code=503, detail={"detail": "Registry locked", "code": "REGISTRY_LOCKED"})
```

**Auth Configuration (per spec ~2216-2240):**
```python
MODEL_REGISTRY_CONFIG = {
    "auth": {
        "type": "bearer",
        "token_env": "MODEL_REGISTRY_TOKEN",
        "scopes_required": ["model:read"]  # or model:write, model:admin
    },
    "timeout": {"connect": 5.0, "read": 30.0, "write": 60.0},
    "retry": {"max_attempts": 3, "backoff_base": 1.0, "backoff_factor": 2.0}
}
```

---

### Component 7: DiskExpressionCache (`libs/factors/cache.py`)

**Files (per spec ~2740-2755):**
- `libs/factors/cache.py`  # Location per spec ownership boundary

**Key Format (per spec ~2390):**
`{factor_name}:{as_of_date}:{dataset_version_id}:{snapshot_id}:{config_hash}`

Where `dataset_version_id` is derived from `version_ids` dict as:
- Deterministic serialization: `crsp-v1.2.3_compustat-v1.0.1` (sorted keys, hyphen-joined)
- This produces a single string for cache key compatibility

**All 5 components required for PIT safety.**

**Methods:**
```python
class DiskExpressionCache:
    def __init__(self, cache_dir: Path, ttl_days: int = 7):
        self.cache_dir = cache_dir
        self.ttl_days = ttl_days

    def _build_version_id_string(self, version_ids: dict[str, str]) -> str:
        """Build deterministic dataset_version_id from dict.

        Example: {'crsp': 'v1.2.3', 'compustat': 'v1.0.1'}
                 -> 'compustat-v1.0.1_crsp-v1.2.3' (sorted)
        """
        return "_".join(f"{k}-{v}" for k, v in sorted(version_ids.items()))

    def _build_key(
        self,
        factor_name: str,
        as_of_date: date,
        version_ids: dict[str, str],
        snapshot_id: str,
        config_hash: str,
    ) -> str:
        """Build cache key per spec format."""
        version_id_str = self._build_version_id_string(version_ids)
        return f"{factor_name}:{as_of_date}:{version_id_str}:{snapshot_id}:{config_hash}"

    def get_or_compute(
        self,
        factor_name: str,
        as_of_date: date,
        snapshot_id: str,           # REQUIRED: PIT safety
        version_ids: dict[str, str],
        config_hash: str,           # REQUIRED: Config safety
        compute_fn: Callable[[], pl.DataFrame],
    ) -> tuple[pl.DataFrame, bool]:  # (data, was_cached)

    def invalidate_by_snapshot(self, snapshot_id: str) -> int
    def invalidate_by_dataset_update(self, dataset: str, new_version: str) -> int
    def invalidate_by_config_change(self, factor_name: str) -> int
    def cleanup_expired(self, ttl_days: int | None = None) -> int
```

**Features:**
- TTL-based expiration (7 days default, configurable)
- Atomic writes using `_atomic_write_parquet` helper
- Snapshot-aware cache invalidation
- Config-hash aware invalidation
- Thread-safe concurrent access

**PIT Safety:**
- Cache key includes snapshot_id (prevents stale data)
- Cache key includes config_hash (prevents stale computation)
- Cache miss on ANY component mismatch

---

### Component 8: Version Compatibility (`libs/models/compatibility.py`)

**Files:**
- `libs/models/compatibility.py`

**Version Drift Policy (per spec ~2289-2294):**
- **STRICT_VERSION_MODE=true (production default):** ANY dataset version drift → BLOCK
- **STRICT_VERSION_MODE=false (development):** ANY dataset version drift → WARN, allow load
- **MISSING dataset → BLOCK always** (regardless of mode)

**Note:** This is stricter than semantic versioning - any version mismatch triggers the policy, not just major/minor changes.

**Methods:**
```python
class VersionCompatibilityChecker:
    def check_compatibility(
        self,
        model_versions: dict[str, str],
        current_versions: dict[str, str],
        strict_mode: bool = True  # Default to strict (production)
    ) -> CompatibilityResult:
        """
        Returns:
        - compatible: bool
        - level: "exact" | "drift" | "missing"
        - warnings: list[str]

        Policy:
        - ANY version difference is drift (not semantic-based)
        - strict_mode=True: drift blocks load
        - strict_mode=False: drift warns but allows load
        - Missing dataset always blocks
        """
        warnings = []
        has_drift = False

        for dataset, model_ver in model_versions.items():
            current_ver = current_versions.get(dataset)
            if current_ver is None:
                # MISSING dataset always blocks
                return CompatibilityResult(
                    compatible=False,
                    level="missing",
                    warnings=[f"Dataset {dataset} not available in current environment"]
                )
            if model_ver != current_ver:
                has_drift = True
                warnings.append(f"{dataset}: model trained on {model_ver}, current is {current_ver}")

        if has_drift:
            return CompatibilityResult(
                compatible=not strict_mode,  # Block in strict mode, allow otherwise
                level="drift",
                warnings=warnings
            )

        return CompatibilityResult(compatible=True, level="exact", warnings=[])
```

**Integration:**
- Controlled by `STRICT_VERSION_MODE` environment variable (default: true in production)
- ANY version mismatch triggers policy (not semantic-based)
- Missing dataset always blocks load with clear error message
- Drift warnings always logged regardless of mode

---

### Component 9: CLI Commands (`apps/cli/commands/model.py`)

**Files:**
- `apps/cli/commands/model.py`

**Commands (per spec ~2140-2148):**
```bash
model register <type> <path> --version <semver>
model promote <type> <version>
model rollback <type>
model list <type> --status [staged|production|archived]
model validate <type> <version>
model restore --from-backup <date>
```

**Features:**
- Rich CLI output with tables
- Dry-run mode for destructive operations
- Confirmation prompts for production changes
- JSON output option for scripting
- Handles ALL metadata fields including config, experiment_id, etc.
- Validates per-artifact required fields on register

---

### Component 10: Backup & Retention (`libs/models/backup.py`)

**Files:**
- `libs/models/backup.py`
- `scripts/registry_gc.py`

**Backup Policy (per spec ~2046-2050):**
- **Primary:** Local filesystem (`data/models/`)
- **Backup:** Optional S3/GCS sync via `rclone` (configured in env)
- **Backup frequency:** Daily at 02:00 UTC
- **Retention:** 90 days for backups

**Retention Policy (per spec ~2121-2127):**
- Production models: retained indefinitely
- Staged models: 30 days after promotion or rejection
- Archived models: 90 days after archival
- Artifacts: checksum re-validated on every load
- GC job: weekly cleanup of expired artifacts

**Implementation:**
```python
class RegistryBackupManager:
    def create_backup(self, backup_dir: Path) -> BackupManifest
    def restore_from_backup(self, backup_date: date) -> RestoreResult
    def sync_to_remote(self, remote_path: str) -> SyncResult  # rclone
    def update_manifest_backup_info(self, manifest_manager: RegistryManifestManager) -> None

class RegistryGC:
    def collect_expired_staged(self, max_age_days: int = 30) -> list[str]
    def collect_expired_archived(self, max_age_days: int = 90) -> list[str]
    def run_gc(self, dry_run: bool = True) -> GCReport
    def update_manifest_after_gc(self, manifest_manager: RegistryManifestManager) -> None
```

---

### Component 11: Migration Script (`scripts/migrate_registry.py`)

**Files:**
- `scripts/migrate_registry.py`

**Features (per spec ~2586-2655):**
- Legacy artifact discovery
- Metadata inference for legacy models
- Dry-run mode
- Automatic backup before migration
- Verification step

---

## Test Strategy

### Unit Tests
- `tests/libs/models/test_types.py` - Data model validation, per-artifact field validation
- `tests/libs/models/test_registry.py` - Registry CRUD operations
- `tests/libs/models/test_serialization.py` - Serialization/checksums
- `tests/libs/models/test_loader.py` - ProductionModelLoader
- `tests/libs/models/test_compatibility.py` - Version compatibility
- `tests/libs/models/test_manifest.py` - Manifest lifecycle
- `tests/libs/factors/test_cache.py` - DiskExpressionCache

### Integration Tests
- `tests/integration/test_registry_integration.py` - Full workflow
- `tests/integration/test_loader_integration.py` - signal_service integration
- `tests/integration/test_api_endpoints.py` - FastAPI endpoint tests
- `tests/integration/test_api_errors.py` - HTTP error semantics (404/409/422/503)

### E2E Tests (per spec ~2580-2585)
- `tests/e2e/test_registry_resilience.py` - Corruption detection & rollback
- `tests/e2e/test_version_drift.py` - Version drift handling
- `tests/e2e/test_alpha_to_signal.py` - Alpha → registry → signal_service load path
- `tests/e2e/test_promotion_gates.py` - Promotion threshold validation

### Property Tests
- Cache key canonicalization determinism
- Checksum determinism
- Compatibility checker edge cases
- Per-artifact field validation exhaustive

---

## Documentation

- `docs/ADRs/ADR-0022-model-deployment.md` - Architecture decisions
- `docs/CONCEPTS/model-registry.md` - User guide
- `docs/RUNBOOKS/model-registry-dr.md` - DR procedures

---

## Acceptance Criteria Checklist

### Core Registry
- [ ] DuckDB-based registry catalog with artifacts directory
- [ ] Pickle/joblib serialization with SHA-256 checksum verification
- [ ] Immutable versioning (no overwrites)
- [ ] Provenance: dataset_version_ids linked to P4T1
- [ ] **DatasetVersionManager validation on registration**
- [ ] **Per-artifact required metadata validation**
- [ ] ProductionModelLoader with latest version retrieval
- [ ] Compatibility check for data version drift
- [ ] **Missing dataset blocks load with clear error**

### Manifest
- [ ] manifest.json with registry state summary
- [ ] Manifest updated on register/promote/rollback/GC
- [ ] Integrity verification (checksum)
- [ ] DR fields (last_backup_at, backup_location)

### Metadata (per spec ~2050-2085)
- [ ] config and config_hash fields
- [ ] feature_formulas placeholder (nullable)
- [ ] experiment_id, run_id (nullable for non-Qlib)
- [ ] dataset_uri (nullable)
- [ ] qlib_version (nullable)
- [ ] **Metadata sidecar is authoritative; DB stores queryable subset**

### FastAPI Endpoints
- [ ] GET /{model_type}/current with model:read scope
- [ ] GET /{model_type}/{version} with model:read scope
- [ ] POST /{model_type}/{version}/validate with model:write scope
- [ ] JWT authentication with scopes
- [ ] **HTTP error semantics (404/409/422/503)**

### Promotion Gates
- [ ] IC > 0.02 threshold check
- [ ] Sharpe > 0.5 threshold check
- [ ] 24h paper trading period verification

### Backup & Retention
- [ ] Daily backup at 02:00 UTC
- [ ] 90-day backup retention
- [ ] Staged model cleanup (30 days)
- [ ] Archived model cleanup (90 days)
- [ ] Weekly GC job
- [ ] Manifest updated after backup/GC

### CLI
- [ ] Promotion/rollback CLI commands functional
- [ ] Register/list/validate commands working
- [ ] Restore from backup command
- [ ] Per-artifact field validation on register

### Testing
- [ ] Load failure returns graceful error
- [ ] Checksum mismatch detected and rejected
- [ ] Concurrent writers correctly serialized
- [ ] Partial artifact upload detected and rejected
- [ ] Version flip latency <100ms
- [ ] E2E corruption/rollback test passing
- [ ] **E2E alpha→registry→signal_service test passing**
- [ ] **E2E promotion gates test passing**
- [ ] **API error semantics tests (404/409/422/503)**
- [ ] >90% test coverage

### DiskExpressionCache (libs/factors/cache.py)
- [ ] Cache key format with all 5 components
- [ ] **dataset_version_id derived from dict with deterministic serialization**
- [ ] Cache invalidation on dataset update
- [ ] Cache invalidation on config change
- [ ] TTL-based expiration
- [ ] Atomic writes
- [ ] PIT safety (cache miss on snapshot mismatch)
- [ ] Config safety (cache miss on config hash mismatch)

### Compatibility
- [ ] **ANY version drift blocks in STRICT_VERSION_MODE=true**
- [ ] **ANY version drift warns but allows in STRICT_VERSION_MODE=false**
- [ ] **Missing dataset always blocks load with clear error**
- [ ] Drift warnings always logged regardless of mode

---

## Implementation Order

1. **Phase 1 (Core):** types.py (with per-artifact validation), serialization.py, manifest.py, registry.py
2. **Phase 2 (Loading):** loader.py, compatibility.py (with missing dataset handling)
3. **Phase 3 (API):** FastAPI endpoints with auth and error semantics
4. **Phase 4 (Cache):** libs/factors/cache.py (with deterministic key building)
5. **Phase 5 (CLI + Backup):** CLI commands, backup.py, registry_gc.py
6. **Phase 6 (Tests):** Unit, integration, E2E tests (including API errors, alpha→signal path)
7. **Phase 7 (Docs):** ADR, concepts, runbook

---

## Risk Mitigation

| Risk | Mitigation |
|------|------------|
| DuckDB concurrency issues | Use transaction isolation, single-writer pattern |
| Checksum collision | Use SHA-256 (collision-resistant) |
| Load failure cascade | Circuit breaker, last-good fallback |
| Migration data loss | Dry-run mode, automatic backup |
| Version drift undetected | STRICT_VERSION_MODE in production |
| Missing dataset at runtime | Explicit check with clear error message |
| Promotion of unvalidated model | Metric thresholds + paper trade gate |
| Manifest/DB inconsistency | Manifest updated atomically with DB changes |
| Per-artifact fields missing | Validation at registration time |

---

## Resolved Questions

1. **FastAPI endpoints:** YES, implement now. Required for signal_service polling.

2. **Qlib metadata:** YES, experiment_id/run_id/dataset_uri/qlib_version are nullable for non-Qlib models.

3. **Cache location:** `libs/factors/cache.py` per spec ownership boundary (~2740-2755).

4. **DatasetVersionManager validation:** REQUIRED on register with explicit error semantics.

5. **Promotion gates:** IC > 0.02, Sharpe > 0.5, 24h paper trade required before promotion.

6. **Per-artifact metadata:** Validated at registration per spec table (~2021-2027).

7. **Cache key format:** dataset_version_id is deterministically serialized from dict.

8. **Authoritative source:** metadata.json sidecar is authoritative; DB stores queryable subset.
