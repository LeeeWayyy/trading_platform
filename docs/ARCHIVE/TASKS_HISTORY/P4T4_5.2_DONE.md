# P4T4-T5.2: Backtest Result Storage

**Parent Task:** [P4T4_TASK.md](./P4T4_TASK.md)
**Task ID:** P4T4-T5.2

### T5.2: Backtest Result Storage

**Effort:** 2-3 days | **PR:** `feat(p4): backtest result storage`
**Status:** ✅ Complete
**Dependencies:** T5.1

**Deliverables:**
- Postgres result storage schema (see Architecture section)
- Result serialization (BacktestResult → JSONB)
- Result retrieval with filtering
- Cancellation and resume support
- Result retention policy (configurable, default 90 days)

**Sync vs Async Decision:**
RQ workers run synchronously. Use pooled psycopg connections for worker persistence (BacktestWorker uses `psycopg_pool.ConnectionPool`). Async storage (BacktestResultStorage) is for web layer queries only.

**Implementation:**
See [P4T4_TASK.md](./P4T4_TASK.md#result-storage-schema-hybrid-postgres--parquet) for the canonical `backtest_jobs` DDL; this task reuses that schema.

```python
# libs/backtest/result_storage.py
import shutil
from datetime import datetime, timedelta
from pathlib import Path
from typing import Any
from uuid import UUID

import polars as pl
from psycopg.rows import dict_row
from psycopg_pool import ConnectionPool

from libs.alpha.research_platform import BacktestResult

PARQUET_BASE_DIR = Path("data/backtest_results")

class JobNotFound(Exception):
    pass

class ResultPathMissing(Exception):
    pass

class BacktestResultStorage:
    """
    Persistent storage for backtest results (sync, used by web layer + workers).
    """

    DEFAULT_RETENTION_DAYS = 90

    def __init__(self, pool: ConnectionPool):
        self.pool = pool

    def get_result(self, job_id: str) -> BacktestResult | None:
        """
        Retrieve backtest result by job ID, loading Parquet artifacts.

        Returns:
            BacktestResult on success.
        Raises:
            JobNotFound: no backtest_jobs row exists.
            ResultPathMissing: row exists but result_path is null/empty.
        """
        with self.pool.connection() as conn, conn.cursor(row_factory=dict_row) as cur:
            cur.execute("SELECT result_path FROM backtest_jobs WHERE job_id = %s", (job_id,))
            row = cur.fetchone()
        if not row:
            raise JobNotFound(f"job_id {job_id} not found")
        if not row["result_path"]:
            raise ResultPathMissing(f"job_id {job_id} missing result_path; rerun or reconcile")

        # Load Parquet files and reconstruct BacktestResult
        return self._load_result_from_path(Path(row["result_path"]))

    def list_jobs(
        self,
        created_by: str | None = None,
        alpha_name: str | None = None,
        status: str | None = None,
        limit: int = 100,
        offset: int = 0,
    ) -> list[dict[str, Any]]:
        """List jobs with filtering."""
        clauses = ["1=1"]
        params: list[Any] = []
        if created_by:
            clauses.append("created_by = %s")
            params.append(created_by)
        if alpha_name:
            clauses.append("alpha_name = %s")
            params.append(alpha_name)
        if status:
            clauses.append("status = %s")
            params.append(status)
        where_sql = " AND ".join(clauses)
        sql = f"""
            SELECT * FROM backtest_jobs
            WHERE {where_sql}
            ORDER BY created_at DESC
            OFFSET %s LIMIT %s
        """
        params.extend([offset, limit])
        with self.pool.connection() as conn, conn.cursor(row_factory=dict_row) as cur:
            cur.execute(sql, params)
            rows = cur.fetchall()
        return [self._job_to_dict(job) for job in rows]

    def cleanup_old_results(
        self,
        retention_days: int = DEFAULT_RETENTION_DAYS,
    ) -> int:
        """
        Delete results older than retention period.

        CRITICAL: Only deletes TERMINAL jobs (completed, failed, cancelled).
        Never deletes pending/running jobs to prevent orphaning active work.
        Deletes Parquet artifacts first, then DB rows to keep DB/source of truth
        aligned with on-disk state.
        Returns count of jobs deleted.
        """
        cutoff = datetime.now() - timedelta(days=retention_days)
        terminal_statuses = ("completed", "failed", "cancelled")

        select_sql = """
            SELECT id, result_path
            FROM backtest_jobs
            WHERE created_at < %s
              AND status = ANY(%s)
        """
        delete_sql = """
            DELETE FROM backtest_jobs
            WHERE created_at < %s
              AND status = ANY(%s)
        """

        with self.pool.connection() as conn, conn.cursor(row_factory=dict_row) as cur:
            cur.execute(select_sql, (cutoff, list(terminal_statuses)))
            jobs_to_delete = cur.fetchall()
            artifact_paths = [Path(row["result_path"]) for row in jobs_to_delete if row["result_path"]]

            # Delete Parquet artifacts first (ensures disk cleanup even if DB delete fails later)
            for artifact_path in artifact_paths:
                if artifact_path.exists():
                    shutil.rmtree(artifact_path)

            cur.execute(delete_sql, (cutoff, list(terminal_statuses)))
            deleted_count = cur.rowcount
            conn.commit()

        return deleted_count

    def _load_result_from_path(self, path: Path) -> BacktestResult:
        """
        Load BacktestResult from Parquet artifacts.

        Schema Contract: See P4T4_TASK.md "Parquet Schema Contract (Authoritative)" section.
        This reader MUST stay synchronized with T5.1 writer.

        Raises ValueError if reproducibility metadata (snapshot_id, dataset_version_ids)
        is missing from summary.json. These fields are required for any valid result.
        """
        if not path.exists():
            raise ResultPathMissing(f"result_path {path} missing on disk")
        import json
        import polars as pl

        signals = pl.read_parquet(path / "daily_signals.parquet")
        weights = pl.read_parquet(path / "daily_weights.parquet")
        ic = pl.read_parquet(path / "daily_ic.parquet")

        summary_path = path / "summary.json"
        if not summary_path.exists():
            raise ValueError(f"Missing summary.json in {path}; cannot reconstruct BacktestResult")
        summary = json.loads(summary_path.read_text())

        # Reproducibility fields are REQUIRED - raise clear error if missing
        snapshot_id = summary.get("snapshot_id")
        dataset_version_ids = summary.get("dataset_version_ids")
        if snapshot_id is None or dataset_version_ids is None:
            raise ValueError(
                f"Missing reproducibility metadata in {path}/summary.json: "
                f"snapshot_id={snapshot_id}, dataset_version_ids={dataset_version_ids}. "
                "These fields are required for valid BacktestResult reconstruction."
            )

        return BacktestResult(
            daily_signals=signals,
            daily_weights=weights,
            daily_ic=ic,
            mean_ic=summary.get("mean_ic", float(ic["ic"].mean())),
            icir=summary.get("icir", float(ic["ic"].mean() / ic["ic"].std())) if float(ic["ic"].std()) != 0 else 0.0,
            hit_rate=summary.get("hit_rate"),
            snapshot_id=snapshot_id,  # Required, not None
            dataset_version_ids=dataset_version_ids,  # Required, not None
        )

    def _job_to_dict(self, job: BacktestJob) -> dict[str, Any]:
        """Convert ORM model to dict."""
        return {
            "job_id": job.job_id,
            "status": job.status,
            "alpha_name": job.alpha_name,
            "start_date": str(job.start_date),
            "end_date": str(job.end_date),
            "created_by": job.created_by,
            "created_at": job.created_at.isoformat() if job.created_at else None,
            "mean_ic": job.mean_ic,
            "icir": job.icir,
        }
```

**Files to Create:**
- `libs/backtest/result_storage.py`
- `libs/backtest/models.py` (dataclass models + psycopg row mappers; NOT SQLAlchemy)
- `db/migrations/0008_create_backtest_jobs.sql`  # Depends on 0007 from P4T3
- `tests/libs/backtest/test_result_storage.py`
- `docs/CONCEPTS/backtest-result-storage.md`

---
---
## Acceptance Criteria
### T5.2 Result Storage
- [ ] Results persist across Postgres restart (integration test)
- [ ] Can filter by user, alpha, date range, status (query test with 100+ records)
- [ ] Retention cleanup deletes jobs older than N days (N=90 default, verified by test)
- [ ] BacktestResult → Parquet → BacktestResult round-trip produces identical metrics
- [ ] **Reproducibility:** `get_result()` raises `ValueError` if `snapshot_id` or `dataset_version_ids` missing from summary.json
- [ ] **Error Handling:** `get_result()` distinguishes job-not-found vs result_path-missing (e.g., raises `JobNotFound` vs `ResultPathMissing`) instead of collapsing to `None`
