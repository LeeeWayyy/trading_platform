# P4T4-T5.4: Walk-Forward Optimization

**Parent Task:** [P4T4_TASK.md](./P4T4_TASK.md)
**Task ID:** P4T4-T5.4

### T5.4: Walk-Forward Optimization

**Effort:** 3-4 days | **PR:** `feat(p4): walk-forward optimization`
**Status:** ✅ Complete
**Dependencies:** T5.1, T5.2

**⚠️ Qlib Decision:** Use custom `WalkForwardOptimizer` over PITBacktester. **DO NOT use `qlib.workflow.rolling`** - it assumes Qlib DataHandler and would bypass our DatasetVersionManager PIT guarantees.

**Deliverables:**
- Rolling train/test window framework
- Parameter optimization per window (grid search, random search)
- Out-of-sample performance aggregation
- Overfitting prevention metrics (train/test performance gap)
- Overlap policy: enforce `step_months >= test_months` so evaluation windows never overlap; document that overlapping train windows are allowed but test windows must be disjoint
- Emit a warning when `step_months < train_months` to make overlapping train windows explicit to operators
- Documentation: update `docs/CONCEPTS/walk-forward-optimization.md` to state explicitly that train windows may overlap while test windows must remain disjoint to avoid information leakage
- Optional: Export per-window Pandas returns for `qlib.contrib.evaluate` post-processing

**Implementation:**
```python
# libs/backtest/walk_forward.py
from dataclasses import dataclass
from datetime import date, timedelta
from typing import Callable, Any
from dateutil.relativedelta import relativedelta

import polars as pl
import structlog

from libs.alpha.research_platform import PITBacktester, BacktestResult

@dataclass
class WalkForwardConfig:
    """Configuration for walk-forward optimization."""
    train_months: int = 12
    test_months: int = 3
    step_months: int = 3  # How much to advance each window
    min_train_samples: int = 252  # Minimum trading days in train

@dataclass
class WindowResult:
    """Result for a single walk-forward window."""
    window_id: int
    train_start: date
    train_end: date
    test_start: date
    test_end: date
    best_params: dict[str, Any]
    train_ic: float
    test_ic: float
    test_result: BacktestResult

@dataclass
class WalkForwardResult:
    """Complete walk-forward optimization result."""
    windows: list[WindowResult]
    aggregated_test_ic: float
    aggregated_test_icir: float
    overfitting_ratio: float  # train_ic / test_ic (>2 suggests overfit)

    @property
    def is_overfit(self) -> bool:
        return self.overfitting_ratio > 2.0

class WalkForwardOptimizer:
    """Walk-forward optimization framework."""

    def __init__(
        self,
        backtester: PITBacktester,
        config: WalkForwardConfig,
    ):
        self.backtester = backtester
        self.config = config
        self.logger = structlog.get_logger(__name__)

    def generate_windows(
        self,
        start_date: date,
        end_date: date,
    ) -> list[tuple[date, date, date, date]]:
        """Generate (train_start, train_end, test_start, test_end) tuples."""
        if self.config.step_months < self.config.test_months:
            raise ValueError(
                "step_months must be >= test_months to prevent information leakage from "
                "overlapping evaluation periods. Train overlap is allowed for rolling windows "
                "(see docs/CONCEPTS/walk-forward-optimization.md)."
            )
        if self.config.step_months < self.config.train_months:
            self.logger.warning(
                "walk_forward_train_overlap",
                step_months=self.config.step_months,
                train_months=self.config.train_months,
                overlap_months=self.config.train_months - self.config.step_months,
                message="Train windows will overlap (intended for rolling optimization); test windows remain disjoint. Overlap is safe for train data but operators should be aware.",
            )

        windows: list[tuple[date, date, date, date]] = []
        cursor = start_date

        while True:
            train_start = cursor
            train_end = (train_start + relativedelta(months=self.config.train_months)) - timedelta(days=1)
            test_start = train_end + timedelta(days=1)
            test_end = (test_start + relativedelta(months=self.config.test_months)) - timedelta(days=1)

            if test_end > end_date:
                break  # would exceed requested range

            if (train_end - train_start).days + 1 < self.config.min_train_samples:
                raise ValueError("train window shorter than min_train_samples")

            windows.append((train_start, train_end, test_start, test_end))
            cursor = cursor + relativedelta(months=self.config.step_months)

        return windows

    def optimize_window(
        self,
        alpha_factory: Callable[..., AlphaDefinition],
        param_grid: dict[str, list[Any]],
        train_start: date,
        train_end: date,
    ) -> tuple[dict[str, Any], float]:
        """Find best params on training window. Returns (best_params, train_ic)."""
        ...

    def run(
        self,
        alpha_factory: Callable[..., AlphaDefinition],
        param_grid: dict[str, list[Any]],
        start_date: date,
        end_date: date,
        snapshot_id: str | None = None,
    ) -> WalkForwardResult:
        """Run complete walk-forward optimization."""
        windows = self.generate_windows(start_date, end_date)
        results = []

        for i, (train_start, train_end, test_start, test_end) in enumerate(windows):
            # 1. Optimize on training window
            best_params, train_ic = self.optimize_window(
                alpha_factory, param_grid, train_start, train_end
            )

            # 2. Evaluate on test window (out-of-sample)
            alpha = alpha_factory(**best_params)
            test_result = self.backtester.run_backtest(
                alpha, test_start, test_end, snapshot_id=snapshot_id
            )

            results.append(WindowResult(
                window_id=i,
                train_start=train_start,
                train_end=train_end,
                test_start=test_start,
                test_end=test_end,
                best_params=best_params,
                train_ic=train_ic,
                test_ic=test_result.mean_ic,
                test_result=test_result,
            ))

        return self._aggregate_results(results)
```

All window backtests (train + test) must forward `snapshot_id` to `run_backtest` to keep PIT determinism across windows.

**Files to Create:**
- `libs/backtest/walk_forward.py`
- `libs/backtest/param_search.py` (grid search, random search utilities)
- `tests/libs/backtest/test_walk_forward.py`
- `docs/CONCEPTS/walk-forward-optimization.md`

---
---
## Acceptance Criteria
### T5.4 Walk-Forward
- [ ] Window generator produces non-overlapping test periods
- [ ] Train period ≥ min_train_samples (252) for each window
- [ ] Overfitting ratio = mean(train_ic) / mean(test_ic) computed correctly
- [ ] Edge case: handles backtests spanning <2 windows gracefully
- [ ] **Overlap Policy:** Raises `ValueError` when `step_months < test_months` (prevents test window overlap/information leakage)
- [ ] **Train Overlap Warning:** Emits warning when `step_months < train_months` (makes overlapping train windows explicit)
- [ ] **Reproducibility:** All window backtests forward `snapshot_id` to `run_backtest` for PIT determinism (unit test verifies)

