# P4T4-T5.5: Monte Carlo Simulation

**Parent Task:** [P4T4_TASK.md](./P4T4_TASK.md)
**Task ID:** P4T4-T5.5

### T5.5: Monte Carlo Simulation

**Effort:** 3-4 days | **PR:** `feat(p4): monte carlo simulation`
**Status:** ⏳ Pending
**Dependencies:** T5.1, T5.2

**⚠️ Qlib Decision:** Qlib has **no bootstrap/resampling utilities**. Implement NumPy-based simulation. Optionally use `qlib.contrib.evaluate.risk_analysis` for per-simulation metrics when Qlib is installed.

**Deliverables:**
- Trade resampling (bootstrap with replacement) - NumPy implementation
- Return shuffling (path simulation) - NumPy implementation
- Confidence intervals for key metrics (Sharpe, max drawdown, etc.)
- Distribution visualization
- Optional: Per-path metrics via `qlib.contrib.evaluate.risk_analysis`

**Implementation:**
```python
# libs/backtest/monte_carlo.py
from dataclasses import dataclass, field
from typing import Literal
import numpy as np
import structlog
import polars as pl

from libs.alpha.research_platform import BacktestResult

@dataclass
class MonteCarloConfig:
    """Configuration for Monte Carlo simulation."""
    n_simulations: int = 1000
    method: Literal["bootstrap", "shuffle"] = "bootstrap"
    confidence_levels: list[float] = field(default_factory=lambda: [0.05, 0.50, 0.95])
    random_seed: int | None = None

@dataclass
class ConfidenceInterval:
    """Confidence interval for a metric."""
    metric_name: str
    observed: float
    lower_5: float
    median: float
    upper_95: float

    @property
    def is_significant(self) -> bool:
        """True if observed value is above median of simulations."""
        return self.observed > self.median

@dataclass
class MonteCarloResult:
    """Complete Monte Carlo simulation result."""
    config: MonteCarloConfig
    n_simulations: int

    # Confidence intervals for key metrics
    sharpe_ci: ConfidenceInterval
    max_drawdown_ci: ConfidenceInterval
    mean_ic_ci: ConfidenceInterval
    hit_rate_ci: ConfidenceInterval

    # Full distributions (for visualization)
    sharpe_distribution: np.ndarray
    max_drawdown_distribution: np.ndarray

    # Statistical significance
    p_value_sharpe: float  # Probability of observing Sharpe by chance

class MonteCarloSimulator:
    """Monte Carlo simulation for backtest robustness analysis."""

    def __init__(self, config: MonteCarloConfig):
        self.config = config
        self.rng = np.random.default_rng(config.random_seed)
        self.logger = structlog.get_logger(__name__)
        if config.random_seed is None:
            self.logger.warning(
                "monte_carlo_unseeded",
                message="Monte Carlo running without fixed random_seed; results are non-reproducible",
            )

    def run_bootstrap(
        self,
        result: BacktestResult,
    ) -> MonteCarloResult:
        """
        Bootstrap resampling of daily returns.

        Preserves return distribution but breaks temporal structure.
        """
        daily_returns = self._extract_daily_returns(result)
        n_days = len(daily_returns)

        sharpes = []
        drawdowns = []

        for _ in range(self.config.n_simulations):
            # Resample with replacement
            indices = self.rng.integers(0, n_days, size=n_days)
            simulated_returns = daily_returns[indices]

            # Compute metrics on simulated path
            sharpe = self._compute_sharpe(simulated_returns)
            max_dd = self._compute_max_drawdown(simulated_returns)

            sharpes.append(sharpe)
            drawdowns.append(max_dd)

        self.logger.info(
            "monte_carlo_bootstrap_complete",
            simulations=self.config.n_simulations,
            n_days=n_days,
        )

        return self._build_result(result, np.array(sharpes), np.array(drawdowns))

    def run_shuffle(
        self,
        result: BacktestResult,
    ) -> MonteCarloResult:
        """
        Shuffle returns across time (permutation test).

        Tests if observed performance could arise by chance.
        """
        ...

    def _compute_confidence_interval(
        self,
        observed: float,
        simulated: np.ndarray,
        metric_name: str,
    ) -> ConfidenceInterval:
        """Compute confidence interval from simulated distribution."""
        return ConfidenceInterval(
            metric_name=metric_name,
            observed=observed,
            lower_5=np.percentile(simulated, 5),
            median=np.percentile(simulated, 50),
            upper_95=np.percentile(simulated, 95),
        )
```

**Files to Create:**
- `libs/backtest/monte_carlo.py`
- `tests/libs/backtest/test_monte_carlo.py`
- `docs/CONCEPTS/monte-carlo-backtesting.md`

---
---
## Acceptance Criteria
### T5.5 Monte Carlo
- [ ] 1000 simulations complete in <10 seconds for 500-day backtest
- [ ] Confidence intervals: lower_5 < median < upper_95 (invariant)
- [ ] Fixed seed=42 produces identical CI vectors across runs (unit test)
- [ ] P-value in [0, 1] range with documented interpretation

