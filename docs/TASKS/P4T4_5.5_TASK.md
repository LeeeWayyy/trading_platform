# P4T4-T5.5: Monte Carlo Simulation

**Parent Task:** [P4T4_TASK.md](./P4T4_TASK.md)
**Task ID:** P4T4-T5.5

### T5.5: Monte Carlo Simulation

**Effort:** 3-4 days | **PR:** `feat(p4): monte carlo simulation`
**Status:** ðŸ”„ In Progress
**Dependencies:** T5.1, T5.2
**Plan Review:** âœ… Approved with Comments (Gemini + Codex, 2025-12-14)

**âš ï¸ Qlib Decision:** Qlib has **no bootstrap/resampling utilities**. Implement NumPy-based simulation. Optionally use `qlib.contrib.evaluate.risk_analysis` for per-simulation metrics when Qlib is installed.

**Deliverables:**
- Trade resampling (bootstrap with replacement) - NumPy implementation
- Return shuffling (path simulation) - NumPy implementation
- Confidence intervals for key metrics (Sharpe, max drawdown, mean IC, hit rate)
- Distribution arrays for visualization
- Optional: Per-path metrics via `qlib.contrib.evaluate.risk_analysis`

---

## Architectural Decision

### Key Decision: Add `daily_portfolio_returns` to BacktestResult

**Decision:** Add a new field `daily_portfolio_returns: pl.DataFrame` to the `BacktestResult` dataclass in `libs/alpha/research_platform.py`.

**Rationale:**
1. Portfolio returns are computed as `sum(weight * forward_return)` per day
2. Computed during `PITBacktester.run_backtest()` using existing `daily_weights` and `daily_returns`
3. MonteCarloSimulator consumes this field (clean separation of concerns)
4. Portfolio returns are fundamental data needed by other future features (risk metrics, attribution)

**Trade-off:** Modifies core `BacktestResult`, but:
- Field is additive (backward compatible with default empty DataFrame)
- Clean separation: PITBacktester computes, Monte Carlo consumes
- Storage layer handles missing parquet gracefully

**Alternatives Considered:**
| Option | Approach | Why Not Chosen |
|--------|----------|----------------|
| B | Compute returns in MonteCarloSimulator | Requires access to return data not in BacktestResult |
| C | Use IC time series as proxy | IC â‰  returns (different statistical interpretation) |

---

## Schema Contracts

### daily_portfolio_returns.parquet
```
Columns: date (pl.Date), return (pl.Float64)
Compression: snappy
Convention: Date T represents return realized from T to T+1 (decision date labeling)
```

### Backward Compatibility
`libs/backtest/result_storage.py` MUST handle missing `daily_portfolio_returns.parquet`:
```python
daily_portfolio_returns_path = path / "daily_portfolio_returns.parquet"
if daily_portfolio_returns_path.exists():
    daily_portfolio_returns = pl.read_parquet(daily_portfolio_returns_path)
else:
    daily_portfolio_returns = pl.DataFrame(schema={"date": pl.Date, "return": pl.Float64})
```

---

## Metric Definitions

| Metric | Formula | Notes |
|--------|---------|-------|
| **Sharpe Ratio** | `sqrt(252) * mean(returns) / std(returns, ddof=1)` | Annualized, risk-free = 0 |
| **Max Drawdown** | `min((cum_ret - peak) / peak)` | Geometric compounding: `cum_ret = cumprod(1 + returns)` |
| **Hit Rate** | `sum(returns >= 0) / len(returns)` | Portfolio hit rate (% of days with non-negative return) |
| **Mean IC** | `mean(daily_ic["rank_ic"])` | Bootstrapped from existing daily_ic series |
| **P-Value** | `sum(simulated >= observed) / n_simulations` | One-sided; interpretation depends on method (see concept doc). For shuffle, tests path dependency. |

---

## Files to Create
- `libs/backtest/monte_carlo.py` - Core implementation
- `tests/libs/backtest/test_monte_carlo.py` - Tests (>90% coverage)
- `docs/CONCEPTS/monte-carlo-backtesting.md` - User documentation

## Files to Modify
- `libs/alpha/research_platform.py` - Add `daily_portfolio_returns` field and computation
- `libs/backtest/__init__.py` - Export new classes
- `libs/backtest/worker.py` - Save `daily_portfolio_returns.parquet`
- `libs/backtest/result_storage.py` - Load with backward compatibility
- `tests/libs/alpha/test_research_platform.py` - Update fixtures

---

## Implementation Design
```python
# libs/backtest/monte_carlo.py
from __future__ import annotations

from dataclasses import dataclass, field
from typing import TYPE_CHECKING, Literal

import math
import numpy as np
import structlog

if TYPE_CHECKING:
    from libs.alpha.research_platform import BacktestResult

logger = structlog.get_logger(__name__)


@dataclass
class MonteCarloConfig:
    """Configuration for Monte Carlo simulation."""
    n_simulations: int = 1000
    method: Literal["bootstrap", "shuffle"] = "bootstrap"
    confidence_levels: list[float] = field(default_factory=lambda: [0.05, 0.50, 0.95])
    random_seed: int | None = None  # None = warn about non-reproducibility


@dataclass
class ConfidenceInterval:
    """Confidence interval for a metric."""
    metric_name: str
    observed: float
    quantiles: dict[float, float]

    @property
    def lower_5(self) -> float:
        return self.quantiles.get(0.05, float("nan"))

    @property
    def median(self) -> float:
        return self.quantiles.get(0.5, float("nan"))

    @property
    def upper_95(self) -> float:
        return self.quantiles.get(0.95, float("nan"))

    @property
    def is_significant(self) -> bool:
        """True if observed value is above median of simulations (basic check)."""
        median = self.median
        return not math.isnan(median) and self.observed > median


@dataclass
class MonteCarloResult:
    """Complete Monte Carlo simulation result."""
    config: MonteCarloConfig
    n_simulations: int

    # Confidence intervals for key metrics
    sharpe_ci: ConfidenceInterval
    max_drawdown_ci: ConfidenceInterval
    mean_ic_ci: ConfidenceInterval       # Bootstrapped from daily_ic series
    hit_rate_ci: ConfidenceInterval      # Portfolio hit rate (% days >= 0)

    # Full distributions (for visualization)
    sharpe_distribution: np.ndarray
    max_drawdown_distribution: np.ndarray

    # Statistical significance
    p_value_sharpe: float  # One-sided: P(simulated >= observed)


class MonteCarloSimulator:
    """Monte Carlo simulation for backtest robustness analysis.

    Supports two methods:
    - bootstrap: Resample returns WITH replacement (preserves distribution, breaks time)
    - shuffle: Permute returns WITHOUT replacement (tests if order matters)

    Example:
        >>> config = MonteCarloConfig(n_simulations=1000, random_seed=42)
        >>> simulator = MonteCarloSimulator(config)
        >>> result = simulator.run_bootstrap(backtest_result)
        >>> print(f"Sharpe 95% CI: [{result.sharpe_ci.lower_5:.2f}, {result.sharpe_ci.upper_95:.2f}]")
    """

    def __init__(self, config: MonteCarloConfig):
        self.config = config
        self.rng = np.random.default_rng(config.random_seed)
        self.logger = logger
        if config.random_seed is None:
            self.logger.warning(
                "monte_carlo_unseeded",
                message="Monte Carlo running without fixed random_seed; results are non-reproducible",
            )

    def run_bootstrap(self, result: BacktestResult) -> MonteCarloResult:
        """Bootstrap resampling of daily returns (with replacement)."""
        ...

    def run_shuffle(self, result: BacktestResult) -> MonteCarloResult:
        """Permutation test - shuffle returns (without replacement)."""
        ...

    def _extract_daily_returns(self, result: BacktestResult) -> np.ndarray:
        """Extract portfolio returns from BacktestResult.daily_portfolio_returns."""
        ...

    def _extract_daily_ic(self, result: BacktestResult) -> np.ndarray:
        """Extract rank IC values from BacktestResult.daily_ic."""
        ...

    def _compute_sharpe(self, returns: np.ndarray) -> float:
        """Annualized Sharpe: sqrt(252) * mean / std, risk-free=0."""
        ...

    def _compute_max_drawdown(self, returns: np.ndarray) -> float:
        """Max drawdown using geometric compounding."""
        ...

    def _compute_hit_rate(self, returns: np.ndarray) -> float:
        """Portfolio hit rate: fraction of days with return >= 0."""
        ...

    def _compute_confidence_interval(
        self, observed: float, simulated: np.ndarray, metric_name: str
    ) -> ConfidenceInterval:
        """Compute CI from simulated distribution (5th, 50th, 95th percentiles)."""
        ...

    def _build_result(self, ...) -> MonteCarloResult:
        """Assemble final MonteCarloResult with all CIs and p-value."""
        ...


__all__ = [
    "MonteCarloConfig",
    "ConfidenceInterval",
    "MonteCarloResult",
    "MonteCarloSimulator",
]
```

---

## Acceptance Criteria

### T5.5 Monte Carlo
- [ ] 1000 simulations complete in <10 seconds for 500-day backtest
- [ ] Confidence intervals: lower_5 <= median <= upper_95 (invariant for all metrics)
- [ ] Fixed seed=42 produces identical CI vectors across runs (unit test)
- [ ] P-value in [0, 1] range with documented interpretation
- [ ] Test coverage >90% for `libs/backtest/monte_carlo.py`
- [ ] Backward compatibility: Loading old BacktestResults without `daily_portfolio_returns` does not crash
- [ ] Both bootstrap and shuffle methods implemented and tested
- [ ] All four CIs computed: sharpe_ci, max_drawdown_ci, mean_ic_ci, hit_rate_ci

---

## Plan Review Feedback (2025-12-14)

### Gemini (planner role) - APPROVED WITH COMMENTS
1. âœ… Option A (add daily_portfolio_returns to BacktestResult) is architecturally sound
2. âš ï¸ Backward compatibility in result_storage.py is CRITICAL
3. âž• Add `mean_ic_ci` - cheap to compute from existing daily_ic series
4. âž• Add `hit_rate_ci` as Portfolio Hit Rate (% of days with return >= 0)
5. âœ… Date alignment: label returns with date T (decision date)

### Codex (planner role) - APPROVED WITH COMMENTS
1. âœ… Pin schema for daily_portfolio_returns: `{date: pl.Date, return: pl.Float64}`
2. âš ï¸ Define metric formulas clearly (Sharpe annualization, drawdown calc)
3. âš ï¸ Clarify bootstrap vs shuffle (with vs without replacement)
4. âž• Document p-value interpretation (one-sided)
5. âž• Consider extensibility for future metrics (Sortino, skew)

### Consolidated Action Items
| Item | Resolution |
|------|------------|
| Schema | `{"date": pl.Date, "return": pl.Float64}`, snappy compression |
| Backward compat | Try/except with empty DataFrame fallback in result_storage |
| Sharpe | `sqrt(252) * mean / std(ddof=1)`, risk-free = 0 |
| Max Drawdown | `min((cum_ret - peak) / peak)`, geometric compounding |
| Hit Rate | `sum(returns >= 0) / len(returns)` |
| P-value | One-sided: `sum(sim >= obs) / n_sims` |
| Seed | Warn once if None; respect in both bootstrap and shuffle |

---

## Risk Assessment

| Risk | Impact | Mitigation |
|------|--------|------------|
| Modifying BacktestResult breaks existing code | High | Backward-compatible: new field with default empty DataFrame |
| Performance regression | Medium | Benchmark test with assertion (<10s for 500 days) |
| Non-reproducibility | Medium | Warn if random_seed is None; document in user docs |
| Schema drift breaks stored backtests | High | Graceful optional load with defaults in result_storage |
| Statistical misinterpretation | Medium | Document definitions clearly; unit tests for bounds |
