# P4T4-T5.6: Backtest Regression Harness

**Parent Task:** [P4T4_TASK.md](./P4T4_TASK.md)
**Task ID:** P4T4-T5.6

### T5.6: Backtest Regression Harness

**Effort:** 2-3 days | **PR:** `feat(p4): backtest regression harness`
**Status:** ⏳ Pending
**Priority:** P1 (Prevents strategy drift)
**Dependencies:** T1.6 (Dataset Versioning), T5.1, T5.2

**Deliverables:**
- Golden backtest results with fixed seeds and dataset versions
- Automated regression tests in CI
- Alert on metric drift > threshold
- Dataset version pinning via DatasetVersionManager

**Golden Data Governance:**
```
tests/regression/golden_results/
├── manifest.json                      # Version info, regeneration history
├── momentum_2020_2022.json            # Golden metrics
├── momentum_2020_2022_config.json     # Alpha config + seed
├── value_2020_2022.json
├── value_2020_2022_config.json
└── README.md                          # Governance documentation
```

**Manifest Schema:**
```json
{
  "version": "v1.0.0",
  "created_at": "2025-12-09T00:00:00Z",
  "dataset_snapshot_id": "golden_v1.0.0",
  "regeneration_triggers": [
    "Major alpha logic change",
    "Dataset schema change",
    "Quarterly refresh (optional)"
  ],
  "last_regenerated": "2025-12-09T00:00:00Z",
  "regenerated_by": "scripts/generate_golden_results.py",
  "storage_size_mb": 0.5,
  "golden_files": [
    {"name": "momentum_2020_2022.json", "checksum": "sha256:abc123..."},
    {"name": "value_2020_2022.json", "checksum": "sha256:def456..."}
  ]
}
```

`storage_size_mb` = sum of all `golden_files` sizes in **decimal megabytes** (1 MB = 1,000,000 bytes, NOT 1,048,576 binary), rounded to 2 decimal places using Python `round()` (banker's rounding / half-even). Formula: `round(sum(os.path.getsize(f) for f in golden_files) / 1_000_000, 2)`

**Note:** Decimal MB (SI units) differs from binary MB displayed by `ls -lh` (uses 1,048,576). This is intentional for cross-platform consistency. Add `"storage_size_note": "Decimal MB (1MB = 1,000,000 bytes)"` to manifest if clarity is needed.

**Governance Rules:**
1. **Naming:** `{alpha_name}_{start_year}_{end_year}.json`
2. **Regeneration triggers:** Major alpha logic change, dataset schema change
3. **Storage limit:** <10MB total (summary metrics only, no time-series)
4. **Review process:** Golden regeneration requires PR review
5. **Staleness alert:** CI warns if manifest >90 days old

**Implementation:**
```python
# tests/regression/test_backtest_golden.py
import pytest
from datetime import date
from pathlib import Path

from libs.alpha.research_platform import PITBacktester, BacktestResult
from libs.data_quality.versioning import DatasetVersionManager

GOLDEN_RESULTS_DIR = Path(__file__).parent / "golden_results"
METRIC_TOLERANCE = 0.001  # 0.1% tolerance for floating point

@pytest.fixture
def pinned_backtester(version_manager: DatasetVersionManager):
    """Backtester pinned to golden dataset version."""
    # Pin to specific snapshot for reproducibility
    version_manager.set_active_snapshot("golden_v1.0.0")
    return PITBacktester(version_manager, ...)

class TestBacktestGoldenResults:
    """Regression tests against golden backtest results."""

    def test_momentum_alpha_metrics(self, pinned_backtester):
        """Verify momentum alpha produces expected metrics."""
        golden = load_golden_result("momentum_2020_2022.json")

        result = pinned_backtester.run_backtest(
            alpha=MomentumAlpha(lookback=20),
            start_date=date(2020, 1, 1),
            end_date=date(2022, 12, 31),
            weight_method="zscore",
        )

        assert_metrics_match(result, golden, tolerance=METRIC_TOLERANCE)

    def test_value_alpha_metrics(self, pinned_backtester):
        """Verify value alpha produces expected metrics."""
        ...

def assert_metrics_match(
    actual: BacktestResult,
    expected: dict,
    tolerance: float,
) -> None:
    """Assert all key metrics match within tolerance."""
    metrics = ["mean_ic", "icir", "hit_rate", "coverage", "long_short_spread"]

    for metric in metrics:
        actual_val = getattr(actual, metric)
        expected_val = expected[metric]
        diff = abs(actual_val - expected_val)

        assert diff <= tolerance, (
            f"Metric {metric} drifted: expected {expected_val}, got {actual_val} "
            f"(diff={diff:.6f}, tolerance={tolerance})"
        )

def load_golden_result(filename: str) -> dict:
    """Load golden result from fixture file."""
    path = GOLDEN_RESULTS_DIR / filename
    with open(path) as f:
        return json.load(f)


# scripts/generate_golden_results.py (one-time regeneration)
def _hash_file(path: Path) -> str:
    import hashlib

    h = hashlib.sha256()
    with open(path, "rb") as f:
        for chunk in iter(lambda: f.read(8192), b""):
            h.update(chunk)
    return h.hexdigest()


def write_manifest(results_dir: Path) -> None:
    """Write manifest with per-file checksums for reproducibility validation."""
    manifest = {
        "generated_at": datetime.now(UTC).isoformat(),
        "golden_files": [],
    }
    for file_path in sorted(results_dir.glob("*.json")):
        manifest["golden_files"].append({
            "file": file_path.name,
            "sha256": _hash_file(file_path),
        })
    (results_dir / "manifest.json").write_text(json.dumps(manifest, indent=2))


# tests/regression/test_backtest_golden.py (validation hook)
def test_golden_manifest_checksums():
    manifest = json.loads((GOLDEN_RESULTS_DIR / "manifest.json").read_text())
    for entry in manifest["golden_files"]:
        path = GOLDEN_RESULTS_DIR / entry["file"]
        assert _hash_file(path) == entry["sha256"], "Golden result checksum mismatch"
```

**Files to Create:**
- `tests/regression/__init__.py`
- `tests/regression/test_backtest_golden.py`
- `tests/regression/conftest.py` (fixtures for pinned data)
- `tests/regression/golden_results/momentum_2020_2022.json`
- `tests/regression/golden_results/value_2020_2022.json`
- `scripts/generate_golden_results.py` (one-time generation script)
- `docs/CONCEPTS/backtest-regression.md`

**CI Integration:**
```yaml
# .github/workflows/backtest-regression.yml
name: Backtest Regression

on:
  push:
    paths:
      - 'libs/alpha/**'
      - 'libs/backtest/**'
      - 'libs/factors/**'

jobs:
  regression:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Setup Python
        uses: actions/setup-python@v5
      - name: Fail if golden manifest is stale (>90 days)
        run: |
          python - <<'PY'
          import json, datetime, pathlib, sys
          manifest = pathlib.Path('tests/regression/golden_results/manifest.json')
          data = json.loads(manifest.read_text())
          last = datetime.datetime.fromisoformat(data['last_regenerated'].replace('Z','+00:00'))
          if (datetime.datetime.now(datetime.timezone.utc) - last).days > 90:
              print('::error::Golden manifest older than 90 days; regenerate fixtures')
              sys.exit(1)
          PY
      - name: Run regression tests
        run: |
          pytest tests/regression/ -v --tb=short
      - name: Alert on drift
        if: failure()
        run: |
          echo "::error::Backtest regression detected! Review metric changes."
```

---
## Acceptance Criteria
### T5.6 Regression
- [ ] Golden manifest includes dataset_snapshot_id and checksum per file
- [ ] Tests fail when any metric drifts > 0.001 (0.1%)
- [ ] CI runs only on changes to libs/alpha/, libs/backtest/, libs/factors/
- [ ] GitHub Actions annotation emitted on failure with drift details
- [ ] Staleness warning if manifest.last_regenerated > 90 days old

