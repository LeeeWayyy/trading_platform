# T3.3 Event Study Framework - Implementation Plan

**Task:** T3.3 from P4T2_TASK.md
**Status:** Planning (v8 - Revised after fresh Codex rejection)
**Dependencies:** T1.3 (CRSP Local Provider) - COMPLETE, T1.5 (Fama-French Local Provider) - COMPLETE
**Estimated Effort:** 3-4 days

**Review History:**
- v1 (2025-12-08): Initial plan - Gemini APPROVED, Codex REJECTED
- v2 (2025-12-08): Revised with Codex feedback - Added gap_days, expected-return models, significance tests, overlapping-event handling, trading calendar, delisting adjustments
- v3 (2025-12-08): Fresh review - Gemini APPROVED, Codex REJECTED. Fixes: corrected Newey-West formula (centered, proper scaling), added explicit Patell/BMP algorithms, removed AGGREGATE overlap policy (undefined academically), specified CRSP DLRET handling, added config validation
- v4 (2025-12-08): Fresh review - Gemini APPROVED, Codex REJECTED. Fixes: Patell test with forecast-error adjustment per Patell(1976), BMP test with event-induced variance per BMP(1991), DLRET combined multiplicatively on same date, Newey-West lag cap at T-1, cross-sectional correlation mitigation, expanded validation tests
- v5 (2025-12-08): Fresh review - Gemini APPROVED, Codex REJECTED. Fixes: Restricted Patell/BMP to market model only (multi-factor forecast error undefined), window-aware overlap handling, unified delisting helper to permno, corrected clustered SE formula, added missing config validations
- v6 (2025-12-08): Fresh review - Gemini APPROVED, Codex REJECTED. Fixes: Corrected clustered SE scaling (1/N² not 1/N), added clustered SE validation tests, added calendar-time portfolio validation tests
- v7 (2025-12-08): Fresh review - Gemini APPROVED, Codex REJECTED. Fixes: Defined AUTO clustering thresholds, fixed same-day event handling (keep events, use clustering mitigation), renamed bmp_z to bmp_t
- v8 (2025-12-08): Fresh review - Gemini APPROVED, Codex REJECTED. Fixes: Corrected Newey-West to standard LRV formula (removed (T-j) double-scaling), added total_dates to clustering detection, added DLRET date validation, added statsmodels to dev dependencies

---

## 1. Requirement Summary

Build an `EventStudyFramework` to analyze stock price reactions to corporate events (earnings announcements, index rebalances) using standard academic methodology.

**Primary Objective:** Provide event study analytics for analyzing abnormal returns around corporate events, with configurable expected-return models and multiple significance tests.

**Acceptance Criteria (from P4T2_TASK.md):**
- [ ] Market model estimation for expected returns
- [ ] CAR calculation with configurable windows (pre/post event)
- [ ] PEAD analysis by earnings surprise quintile
- [ ] Index rebalance effect analysis (additions/deletions)
- [ ] T-statistics and significance testing (Newey-West SE)
- [ ] >90% test coverage

**Additional Requirements (from Codex Review v2):**
- [ ] Configurable gap between estimation and event windows (default: 5 days)
- [ ] Multiple expected-return models (market, mean-adjusted, FF3/FF5)
- [ ] Multiple significance tests (t-test, Patell, BMP)
- [ ] Overlapping event handling with min_days_between_events
- [ ] Non-trading day event date handling (roll to next trading day)
- [ ] Delisting return adjustments
- [ ] Index announcement/effective date duality

---

## 2. Impacted Components

### Files to Create:
1. `libs/analytics/event_study.py` - Main framework module
2. `tests/libs/analytics/test_event_study.py` - Comprehensive tests
3. `docs/CONCEPTS/event-studies.md` - Concept documentation

### Files to Modify:
1. `libs/analytics/__init__.py` - Add exports for new classes

### Dependencies (Read-Only):
- `libs/data_providers/crsp_local_provider.py` - Stock returns data (including delisting returns)
- `libs/data_providers/fama_french_local_provider.py` - Market returns, factors, and risk-free rate (RF)
- `libs/analytics/microstructure.py` - Base result patterns (MicrostructureResult, CompositeVersionInfo)

### Trading Calendar Source:
- **Primary:** Infer trading calendar from CRSP daily data (dates with valid returns)
- **Fallback:** NYSE trading calendar from pandas_market_calendars if available
- Reuse pattern from T3.1/T3.2 microstructure utilities

### Existing Patterns to Follow:
- `MicrostructureResult` base class pattern from T3.1
- `CompositeVersionInfo` for multi-dataset operations (CRSP + Fama-French)
- PIT support via `as_of` parameter
- Structured logging with extra context
- Polars DataFrames for efficient computation

---

## 3. Academic Background

### 3.1 Expected Return Models

Support multiple models for computing expected returns (configurable):

| Model | Formula | Use Case |
|-------|---------|----------|
| **Market Model** | E[R] = α + β × R_m | Standard, accounts for systematic risk |
| **Mean-Adjusted** | E[R] = μ_estimation | Simple, no market factor |
| **FF3** | E[R] = α + β_mkt×MKT + β_smb×SMB + β_hml×HML | Controls for size/value |
| **FF5** | E[R] = α + β_mkt×MKT + β_smb×SMB + β_hml×HML + β_rmw×RMW + β_cma×CMA | Full factor model |

**Default:** Market Model (most widely used in literature)

**Market Return:** Use `MKT-RF + RF` from Fama-French to form **excess returns**:
- Stock excess return: R_i - RF
- Market excess return: MKT-RF (already excess)

### 3.2 Estimation Window and Gap

```
|-----Estimation Window (120 days)-----|--Gap (5 days)--|---Event Window---|
                                       ^                ^
                               estimation_end    event_window_start

Gap enforces separation to avoid:
- Information leakage from pre-event rumors
- Contamination of "normal" return estimates
```

**Configurable Parameters:**
- `estimation_window`: Default 120 trading days
- `gap_days`: Default 5 trading days (min 1, academic convention: 1-10)
- `pre_window`: Days before event (default 5)
- `post_window`: Days after event (default 20)

### 3.3 Abnormal Return (AR) Calculation

For each day in the **event window** [-w_pre, +w_post]:

```
AR_i,t = (R_i,t - RF_t) - E[R_excess_i,t]

Where E[R_excess] depends on model:
- Market Model: α̂_i + β̂_i × MKT-RF_t
- Mean-Adjusted: μ_estimation
- FF3/FF5: Factor model prediction
```

### 3.4 Cumulative Abnormal Return (CAR)

```
CAR_i[-w_pre, +w_post] = Σ AR_i,t  for t ∈ [-w_pre, +w_post]
```

### 3.5 Significance Tests

Support multiple tests (configurable, default: t-test with Newey-West):

| Test | Description | When to Use | Model Restriction |
|------|-------------|-------------|-------------------|
| **T-test (Newey-West)** | HAC-corrected standard errors | Default, robust to autocorrelation | All models |
| **Patell Test** | Standardized abnormal returns | When cross-sectional correlation low | **Market model only** |
| **BMP Test** | Boehmer, Musumeci, Poulsen | When event-induced variance is present | **Market model only** |

**Important:** Patell and BMP tests are ONLY available when using `ExpectedReturnModel.MARKET`. The forecast-error adjustment formula (C_i,t term) is derived for single-factor OLS regression and does not generalize to multi-factor models (FF3/FF5) without substantial derivation of the (X'X)⁻¹ matrix inversion for higher-dimensional factor spaces. For multi-factor models, use T-test with Newey-West only.

#### 3.5.1 Newey-West HAC Standard Error (Primary Method)

**Simplified approach:** Compute HAC variance directly on the AR series, avoiding complex parameter covariance propagation.

**Formula for Var(CAR) using HAC on AR (Long-Run Variance approach):**
```
Let AR_t be mean-centered abnormal returns: ar_t = AR_t - mean(AR)

Step 1: Compute Long-Run Variance (LRV) with Bartlett kernel:
  LRV = γ₀ + 2 × Σ_{j=1}^{L} w_j × γ_j

Step 2: Var(CAR) = T × LRV

Where:
- T = number of days in event window
- γ₀ = (1/T) × Σ ar_t² (variance of centered AR)
- γ_j = (1/T) × Σ_{t=j+1}^{T} ar_t × ar_{t-j} (autocovariance at lag j)
- w_j = 1 - j/(L+1) (Bartlett kernel weight)
- L = auto-selected or user-specified, with bounds: 1 ≤ L ≤ T-1

Auto-selection rule:
  L = max(1, min(T-1, floor(4 × (T/100)^(2/9))))

SE(CAR) = sqrt(Var(CAR)) = sqrt(T × LRV)
t-stat = CAR / SE(CAR)
```

**Note:** The standard formula is `Var(CAR) = T × LRV` where LRV uses simple Bartlett weights. Do NOT use the alternative `(T-j)` scaling in the summation, which double-counts and overstates variance.

**Key implementation details:**
- Mean-center the AR series before computing autocovariances
- Minimum lag L = 1 (never 0, even for T=2)
- Maximum lag L = T-1 (cannot exceed available data)
- For T < 2: return NaN (insufficient data)
- Configurable via `newey_west_lags` (None = auto-select)

#### 3.5.2 Patell Test Algorithm (Patell 1976)

**For aggregating multiple events (PEAD, index rebalance):**

The Patell test standardizes abnormal returns to account for estimation-period variance AND forecast-error from market-model parameter uncertainty.

```
1. For each event i and event-window day t, compute standardized abnormal return:

   SAR_i,t = AR_i,t / S_i,t

   Where S_i,t = σ̂_i × sqrt(C_i,t)

   And C_i,t = 1 + 1/T₁ + (R_m,t - R̄_m)² / Sxx

   Terms:
   - σ̂_i = residual std from estimation window (sqrt of MSE)
   - T₁ = number of observations in estimation window
   - R_m,t = market return on event day t
   - R̄_m = mean market return over estimation window
   - Sxx = Σ_{s∈estimation}(R_m,s - R̄_m)² (sum of squared deviations)

2. For each event i, compute standardized cumulative abnormal return:

   SCAR_i = (1/sqrt(T₂)) × Σ_{t∈event_window} SAR_i,t

   Where T₂ = number of days in event window

3. Cross-sectional test statistic:

   Z_patell = (1/sqrt(N)) × Σ_{i=1}^{N} SCAR_i

   Where N = number of events

4. Degrees-of-freedom adjustment (optional for small samples):

   Under H₀, Z_patell ~ N(0, 1) for large N (N > 30)
   For smaller N, use t(N-1) distribution
```

**Why forecast-error adjustment matters:**
- The term C_i,t accounts for uncertainty in α̂ and β̂ estimates
- Days where R_m,t is far from R̄_m have higher prediction variance
- Without this adjustment, SAR variance is understated → inflated Z

**Data requirements:** Multiple events on different dates (cross-sectional aggregation). Events on same day violate independence; see Section 3.5.4 for mitigation.

#### 3.5.3 BMP Test Algorithm (Boehmer-Musumeci-Poulsen 1991)

**Accounts for event-induced variance increase:**

The BMP test uses Patell-standardized returns but replaces the assumed unit variance with the actual cross-sectional variance, making it robust to event-induced variance shifts.

```
1. For each event i, compute Patell-standardized CAR (SCAR_i):

   First compute SAR_i,t for each event-window day t (per Section 3.5.2):
   SAR_i,t = AR_i,t / (σ̂_i × sqrt(C_i,t))

   Then compute SCAR with degrees-of-freedom correction:
   SCAR_i = sqrt((T₁ - 2) / (T₁ - 4)) × (1/sqrt(T₂)) × Σ SAR_i,t

   Where:
   - T₁ = estimation window length (need T₁ > 4)
   - T₂ = event window length
   - The sqrt((T₁-2)/(T₁-4)) corrects for estimation-error variance inflation

2. Compute cross-sectional statistics:

   SCAR_mean = (1/N) × Σ_{i=1}^{N} SCAR_i
   SCAR_var = (1/(N-1)) × Σ_{i=1}^{N} (SCAR_i - SCAR_mean)²

3. BMP test statistic:

   t_bmp = sqrt(N) × SCAR_mean / sqrt(SCAR_var)

4. Distribution under H₀:

   t_bmp ~ t(N-1) for moderate N
   Use two-tailed test for p-value
```

**Key differences from Patell:**
- Patell assumes SCAR variance = 1 under H₀
- BMP estimates actual cross-sectional variance → robust to variance shifts
- BMP uses t-distribution (heavier tails) → more conservative

**When to use BMP over Patell:**
- Events known to increase volatility (earnings, M&A announcements)
- Heterogeneous event types in same analysis
- When Patell rejects but you suspect variance inflation

#### 3.5.4 Cross-Sectional Correlation Mitigation

**Problem:** Patell and BMP tests assume cross-sectional independence. Events clustered on the same calendar date (common for index rebalances, earnings seasons) violate this assumption, causing understated standard errors and inflated rejection rates.

**Detection:**
```python
def _detect_event_clustering(events: pl.DataFrame, date_col: str = "event_date") -> dict:
    """Detect calendar-date clustering in events."""
    date_counts = events.group_by(date_col).agg(pl.count().alias("n_events"))
    total_dates = date_counts.height  # Number of unique event dates
    max_same_day = date_counts.select(pl.col("n_events").max()).item()
    n_clustered_dates = date_counts.filter(pl.col("n_events") > 1).height

    return {
        "max_events_same_day": max_same_day,
        "n_clustered_dates": n_clustered_dates,
        "total_dates": total_dates,  # Required for AUTO mitigation selection
        "clustering_severe": max_same_day > 5 or n_clustered_dates > total_dates * 0.1,
    }
```

**Mitigation Options (configurable via `ClusteringMitigation` enum):**

| Option | Method | When to Use |
|--------|--------|-------------|
| `NONE` | Ignore clustering | When events are well-dispersed (< 5% overlap) |
| `CALENDAR_TIME` | Calendar-time portfolio regression | Standard academic approach; robust but loses power |
| `CLUSTERED_SE` | Cluster standard errors by date | Moderate clustering; preserves event-level detail |
| `WARN_ONLY` | Compute tests, add severe warning | Quick analysis with caveats |

**Calendar-Time Portfolio Approach:**
```
1. For each calendar day t with at least one event in its event window:
   - Compute average AR across all active events: AR_portfolio,t = (1/N_t) × Σ AR_i,t

2. Regress portfolio AR on constant:
   AR_portfolio,t = α + ε_t

3. Test H₀: α = 0 using Newey-West standard errors on portfolio AR series
```

**Clustered Standard Errors (Cameron-Miller 2015, Petersen 2009):**
```
1. Compute SCAR_i for each event (per BMP/Patell)

2. Compute overall mean:
   SCAR_mean = (1/N) × Σ_i SCAR_i

3. Group events by calendar date d and compute cluster residual sums:
   u_d = Σ_{i ∈ cluster d} (SCAR_i - SCAR_mean)

4. Cluster-robust variance of the mean:
   Var(SCAR_mean) = (G / (G-1)) × (1/N²) × Σ_{d=1}^{G} u_d²

   Where:
   - G = number of unique dates (clusters)
   - N = total number of events
   - The (G/(G-1)) factor is the finite-cluster correction

5. Standard error and t-statistic:
   SE_clustered = sqrt(Var(SCAR_mean))
   t_clustered = SCAR_mean / SE_clustered

6. Degrees of freedom: df = G - 1 (use t-distribution)
```

**Implementation:**
```python
def _compute_clustered_se(
    scars: np.ndarray,
    cluster_ids: np.ndarray,
) -> tuple[float, float, int]:
    """Compute cluster-robust standard error for mean.

    Uses Cameron-Miller (2015) cluster-robust variance estimator:
    Var(mean) = (G/(G-1)) × (1/N²) × Σ u_g²

    Args:
        scars: Array of SCAR values for each event.
        cluster_ids: Array of cluster IDs (e.g., date ordinals).

    Returns:
        Tuple of (clustered_se, t_stat, df).
    """
    N = len(scars)
    scar_mean = np.mean(scars)
    unique_clusters = np.unique(cluster_ids)
    G = len(unique_clusters)

    if G < 2:
        return float("nan"), float("nan"), 0

    # Compute cluster residual sums
    u_sq_sum = 0.0
    for cluster in unique_clusters:
        mask = cluster_ids == cluster
        cluster_resids = scars[mask] - scar_mean
        u_d = np.sum(cluster_resids)
        u_sq_sum += u_d ** 2

    # Cluster-robust variance of mean: (G/(G-1)) × (1/N²) × Σ u_d²
    var_mean = (G / (G - 1)) * u_sq_sum / (N ** 2)

    se_clustered = np.sqrt(var_mean)
    t_stat = scar_mean / se_clustered if se_clustered > 0 else float("nan")

    return se_clustered, t_stat, G - 1
```

**Default behavior (AUTO mode):**

AUTO clustering mitigation uses deterministic thresholds based on `_detect_event_clustering`:

```python
def _select_clustering_mitigation(
    clustering_info: dict,
    config: EventStudyConfig,
) -> ClusteringMitigation:
    """Select clustering mitigation strategy based on detection results.

    Thresholds:
    - NONE: max_events_same_day <= 1 (no clustering)
    - CLUSTERED_SE: 2 <= max_events_same_day <= 10 AND n_clustered_dates < 20% of total dates
    - CALENDAR_TIME: max_events_same_day > 10 OR n_clustered_dates >= 20% of total dates
    """
    max_same_day = clustering_info["max_events_same_day"]
    n_clustered = clustering_info["n_clustered_dates"]
    total_dates = clustering_info.get("total_dates", 1)
    cluster_pct = n_clustered / total_dates if total_dates > 0 else 0

    if max_same_day <= 1:
        return ClusteringMitigation.NONE
    elif max_same_day <= 10 and cluster_pct < 0.20:
        return ClusteringMitigation.CLUSTERED_SE
    else:
        return ClusteringMitigation.CALENDAR_TIME
```

| Scenario | max_events_same_day | clustered_date_pct | Mitigation |
|----------|---------------------|-------------------|------------|
| No clustering | ≤ 1 | any | NONE |
| Mild clustering | 2-10 | < 20% | CLUSTERED_SE |
| Severe clustering | > 10 | any | CALENDAR_TIME |
| Many clustered dates | any | ≥ 20% | CALENDAR_TIME |

- Add clustering diagnostics to result (which method was used, why)

### 3.6 PEAD (Post-Earnings Announcement Drift)

- **Formation day:** First trading day after earnings announcement (not announcement day)
- **Quintile assignment:** Cross-sectional per event date (equal-count, 20% each)
- **Fallback:** Terciles if any quintile has < 5 events
- Sort stocks by earnings surprise, compute average CAR for each quintile over 60 trading days post-announcement.

---

## 4. Implementation Design

### 4.1 Configuration Enums

```python
from enum import Enum, auto


class ExpectedReturnModel(str, Enum):
    """Expected return model for abnormal return calculation."""
    MARKET = "market"  # CAPM / market model
    MEAN_ADJUSTED = "mean_adjusted"  # Simple mean
    FF3 = "ff3"  # Fama-French 3-factor
    FF5 = "ff5"  # Fama-French 5-factor


class SignificanceTest(str, Enum):
    """Statistical test for CAR significance."""
    T_TEST = "t_test"  # Standard t-test with Newey-West SE
    PATELL = "patell"  # Patell standardized residual test
    BMP = "bmp"  # Boehmer-Musumeci-Poulsen test


class OverlapPolicy(str, Enum):
    """Policy for handling overlapping events.

    Note: AGGREGATE removed in v3 - academically undefined how to combine events.
    """
    DROP_LATER = "drop_later"  # Keep first event, drop later overlapping events
    DROP_EARLIER = "drop_earlier"  # Keep last event, drop earlier overlapping events
    WARN_ONLY = "warn_only"  # Include all events, add warning, record overlap count


class ClusteringMitigation(str, Enum):
    """Strategy for handling cross-sectional correlation from same-day events.

    See Section 3.5.4 for detailed algorithms.
    """
    NONE = "none"  # Ignore clustering (when events well-dispersed)
    CALENDAR_TIME = "calendar_time"  # Calendar-time portfolio regression
    CLUSTERED_SE = "clustered_se"  # Cluster standard errors by date
    WARN_ONLY = "warn_only"  # Compute tests, add warning
    AUTO = "auto"  # Auto-detect and choose appropriate method
```

### 4.2 Configuration Dataclass

```python
@dataclass
class EventStudyConfig:
    """Configuration for event study analysis.

    Includes validation to prevent invalid combinations.
    """

    # Window configuration
    estimation_window: int = 120  # Trading days
    gap_days: int = 5  # Days between estimation and event window (min 1)
    pre_window: int = 5  # Days before event
    post_window: int = 20  # Days after event
    min_estimation_obs: int = 60  # Minimum observations for estimation

    # Model selection
    expected_return_model: ExpectedReturnModel = ExpectedReturnModel.MARKET
    significance_test: SignificanceTest = SignificanceTest.T_TEST

    # Newey-West settings
    newey_west_lags: int | None = None  # None = auto-select, bounds: 1 ≤ L ≤ T-1

    # Overlap handling
    overlap_policy: OverlapPolicy = OverlapPolicy.DROP_LATER
    min_days_between_events: int | None = None  # None = auto (pre_window + 1 + post_window)

    # Cross-sectional correlation handling
    clustering_mitigation: ClusteringMitigation = ClusteringMitigation.AUTO

    # Data quality
    winsorize_ar_percentile: float = 0.99  # Winsorize extreme AR at this percentile
    cap_beta: float = 5.0  # Cap |beta| at this value

    # Trading day handling
    roll_nontrading_direction: Literal["forward", "backward"] = "forward"

    def __post_init__(self) -> None:
        """Validate configuration after initialization."""
        errors = []

        # Compute event window length (used in multiple validations)
        event_window_len = self.pre_window + 1 + self.post_window

        # Auto-compute min_days_between_events if not specified (window-aware default)
        if self.min_days_between_events is None:
            # Use object.__setattr__ since dataclass may be frozen
            object.__setattr__(self, "min_days_between_events", event_window_len)

        # gap_days must be >= 1
        if self.gap_days < 1:
            errors.append("gap_days must be >= 1")

        # min_days_between_events must be > 0
        if self.min_days_between_events is not None and self.min_days_between_events < 1:
            errors.append("min_days_between_events must be >= 1")

        # newey_west_lags must be >= 1 if specified
        if self.newey_west_lags is not None and self.newey_west_lags < 1:
            errors.append("newey_west_lags must be >= 1 (or None for auto)")

        # newey_west_lags must be <= event_window - 1
        if self.newey_west_lags is not None and self.newey_west_lags >= event_window_len:
            errors.append(
                f"newey_west_lags ({self.newey_west_lags}) must be < "
                f"event_window_length ({event_window_len})"
            )

        # estimation_window must be >= min_estimation_obs
        if self.estimation_window < self.min_estimation_obs:
            errors.append(
                f"estimation_window ({self.estimation_window}) must be >= "
                f"min_estimation_obs ({self.min_estimation_obs})"
            )

        # estimation_window must be > 4 for BMP test DOF correction
        if self.significance_test == SignificanceTest.BMP and self.estimation_window <= 4:
            errors.append(
                f"estimation_window ({self.estimation_window}) must be > 4 "
                f"for BMP test (DOF correction requires T₁ > 4)"
            )

        # Patell/BMP tests ONLY work with market model (forecast-error formula is model-specific)
        if self.significance_test in (SignificanceTest.PATELL, SignificanceTest.BMP):
            if self.expected_return_model != ExpectedReturnModel.MARKET:
                errors.append(
                    f"significance_test={self.significance_test.value} requires "
                    f"expected_return_model=MARKET (forecast-error adjustment is "
                    f"only defined for single-factor OLS). Use T_TEST for {self.expected_return_model.value}."
                )

        # pre_window and post_window must be >= 0
        if self.pre_window < 0:
            errors.append("pre_window must be >= 0")
        if self.post_window < 0:
            errors.append("post_window must be >= 0")

        # cap_beta must be positive
        if self.cap_beta <= 0:
            errors.append("cap_beta must be > 0")

        # winsorize_ar_percentile must be in (0.5, 1.0)
        if not 0.5 < self.winsorize_ar_percentile <= 1.0:
            errors.append("winsorize_ar_percentile must be in (0.5, 1.0]")

        if errors:
            raise ValueError(f"Invalid EventStudyConfig: {'; '.join(errors)}")
```

### 4.3 Result Dataclasses

```python
from dataclasses import dataclass, field
from datetime import UTC, date, datetime
from typing import Literal

import polars as pl


@dataclass
class EventStudyResult:
    """Base result with versioning metadata (per MicrostructureResult pattern)."""

    dataset_version_id: str
    dataset_versions: dict[str, str] | None
    computation_timestamp: datetime
    as_of_date: date | None


@dataclass
class MarketModelResult(EventStudyResult):
    """Result of market model estimation for a single security."""

    symbol: str
    permno: int
    estimation_start: date
    estimation_end: date
    n_observations: int

    # Model configuration
    model_type: ExpectedReturnModel

    # Model parameters
    alpha: float  # Intercept
    beta: float  # Market sensitivity (or factor betas for FF3/FF5)
    factor_betas: dict[str, float] | None  # For FF3/FF5: {SMB: x, HML: y, ...}
    alpha_tstat: float
    beta_tstat: float

    # Model fit
    r_squared: float
    residual_std: float  # For SE calculation

    # Warnings
    warnings: list[str] = field(default_factory=list)


@dataclass
class EventStudyAnalysis(EventStudyResult):
    """Result of single event study analysis."""

    # Event identification
    event_id: str
    symbol: str
    permno: int
    event_date: date  # Original event date
    adjusted_event_date: date  # After rolling to trading day
    event_type: str

    # Configuration used
    config: EventStudyConfig

    # Market model parameters
    alpha: float
    beta: float
    model_type: ExpectedReturnModel

    # Abnormal returns
    car_pre: float  # CAR days [-w_pre, -1]
    car_event: float  # CAR on event day
    car_post: float  # CAR days [+1, +w_post]
    car_window: float  # CAR full window

    # Daily AR series
    daily_ar: pl.DataFrame  # [relative_day, date, return, rf, excess_return, expected_return, ar]

    # Volume analysis
    abnormal_volume: float | None
    volume_estimation_avg: float | None

    # Statistical tests
    t_statistic: float
    p_value: float
    is_significant: bool  # p < 0.05

    # Additional tests (if computed)
    patell_z: float | None = None  # Z-statistic, ~N(0,1) for large N
    bmp_t: float | None = None  # t-statistic, ~t(N-1) distribution

    # Standard errors
    se_car: float
    newey_west_lags: int

    # Delisting handling
    is_delisted: bool = False
    delisting_return: float | None = None

    # Warnings
    warnings: list[str] = field(default_factory=list)


@dataclass
class PEADAnalysisResult(EventStudyResult):
    """Result of Post-Earnings Announcement Drift analysis."""

    holding_period_days: int
    n_events: int
    n_events_excluded: int  # Due to overlap, data issues
    analysis_start: date
    analysis_end: date

    # Configuration
    config: EventStudyConfig

    # Quintile results
    quintile_results: pl.DataFrame  # [quintile, n_events, avg_surprise, car, se, t_stat, p_value]

    # Summary statistics
    drift_magnitude: float  # Q5 CAR - Q1 CAR
    drift_t_stat: float
    drift_significant: bool

    # Overlap statistics
    n_overlapping_dropped: int

    # Warnings
    warnings: list[str] = field(default_factory=list)


@dataclass
class IndexRebalanceResult(EventStudyResult):
    """Result of index rebalance event study."""

    index_name: str

    # Configuration
    config: EventStudyConfig

    # Event counts
    n_additions: int
    n_deletions: int

    # Addition effects (using effective_date by default)
    addition_car_pre: float
    addition_car_post: float
    addition_t_stat: float
    addition_significant: bool

    # Deletion effects
    deletion_car_pre: float
    deletion_car_post: float
    deletion_t_stat: float
    deletion_significant: bool

    # Volume effects
    addition_volume_change: float
    deletion_volume_change: float

    # Announcement vs effective date analysis
    uses_announcement_date: bool
    announcement_effective_gap_days: float | None  # Average gap

    # Detailed results
    addition_results: pl.DataFrame
    deletion_results: pl.DataFrame

    # Warnings
    warnings: list[str] = field(default_factory=list)
```

### 4.4 Core EventStudyFramework Class

```python
class EventStudyFramework:
    """Framework for event study analysis.

    Uses CRSP for stock returns (including delisting) and Fama-French
    for market returns and risk-free rate.
    """

    DATASET_CRSP = "crsp_daily"
    DATASET_FF = "fama_french"

    def __init__(
        self,
        crsp_provider: CRSPLocalProvider,
        fama_french_provider: FamaFrenchLocalProvider,
        config: EventStudyConfig | None = None,
    ) -> None:
        """Initialize framework with data providers and config."""
        self.crsp = crsp_provider
        self.ff = fama_french_provider
        self.config = config or EventStudyConfig()
        self._trading_calendar: pl.DataFrame | None = None

    def _get_trading_calendar(self, start: date, end: date) -> pl.DataFrame:
        """Get trading calendar from CRSP data."""
        ...

    def _roll_to_trading_day(self, event_date: date, direction: str = "forward") -> date:
        """Roll non-trading date to next (forward) or previous (backward) trading day."""
        ...

    def _handle_overlapping_events(
        self,
        events: pl.DataFrame,
        symbol_col: str = "symbol",
        date_col: str = "event_date",
    ) -> tuple[pl.DataFrame, int]:
        """Handle overlapping events per symbol based on config.overlap_policy.

        Returns:
            Tuple of (filtered_events, n_dropped)
        """
        ...

    def _get_version_info(self, as_of: date | None = None) -> CompositeVersionInfo:
        """Get version info from both providers."""
        ...

    def _compute_excess_returns(
        self,
        returns: pl.DataFrame,
        ff_data: pl.DataFrame,
    ) -> pl.DataFrame:
        """Compute excess returns (R_i - RF) using Fama-French RF."""
        ...

    def estimate_market_model(
        self,
        symbol: str,
        estimation_end: date,
        model: ExpectedReturnModel | None = None,
        as_of: date | None = None,
    ) -> MarketModelResult:
        """Estimate expected return model for a security.

        Enforces gap_days before estimation_end for event window separation.
        Uses excess returns (R - RF) for all models.
        """
        ...

    def compute_car(
        self,
        symbol: str,
        event_date: date,
        event_type: str = "custom",
        event_id: str | None = None,
        config: EventStudyConfig | None = None,
        as_of: date | None = None,
    ) -> EventStudyAnalysis:
        """Compute cumulative abnormal return around event.

        Features:
        - Enforces gap_days between estimation and event windows
        - Rolls non-trading event dates to next trading day
        - Handles delisting returns for stocks that delist during event window
        - Computes multiple significance tests if configured
        """
        ...

    def analyze_pead(
        self,
        earnings_events: pl.DataFrame,
        holding_period_days: int = 60,
        config: EventStudyConfig | None = None,
        as_of: date | None = None,
    ) -> PEADAnalysisResult:
        """Analyze post-earnings announcement drift.

        Args:
            earnings_events: DataFrame with columns [symbol, event_date, surprise_pct].
                            event_date should be announcement date; analysis starts
                            from first trading day after announcement.
            holding_period_days: Days to hold after announcement.
            config: Override default config.
            as_of: Point-in-time date for PIT queries.

        Features:
        - Cross-sectional quintile formation per event date
        - Overlap handling via min_days_between_events
        - Falls back to terciles if < 5 events per quintile
        """
        ...

    def analyze_index_rebalance(
        self,
        index_changes: pl.DataFrame,
        index_name: str = "SP500",
        use_announcement_date: bool = False,
        config: EventStudyConfig | None = None,
        as_of: date | None = None,
    ) -> IndexRebalanceResult:
        """Analyze price impact of index additions/deletions.

        Args:
            index_changes: DataFrame with columns:
                          [symbol, effective_date, action, announcement_date (optional)]
                          action: 'add' or 'drop'
            index_name: Name of index for labeling.
            use_announcement_date: If True, use announcement_date as event date
                                  (for studying anticipation effects).
            config: Override default config.
            as_of: Point-in-time date for PIT queries.

        Features:
        - Dual-date support (announcement vs effective)
        - Separate analysis for additions and deletions
        - Volume change analysis
        """
        ...
```

### 4.5 Helper Functions

```python
def _compute_newey_west_se(
    ar_series: np.ndarray,
    n_lags: int | None = None,
) -> float:
    """Compute Newey-West HAC standard error for CAR.

    Uses Long-Run Variance (LRV) approach with Bartlett kernel:
    Var(CAR) = T × LRV, where LRV = γ₀ + 2 × Σ w_j × γ_j

    Args:
        ar_series: Array of abnormal returns in event window.
        n_lags: Number of lags (None = auto-select). Bounds: 1 ≤ L ≤ T-1.

    Returns:
        HAC-corrected standard error for sum of AR (CAR).
    """
    T = len(ar_series)
    if T < 2:
        return float("nan")

    # Auto-select lags with bounds: 1 ≤ L ≤ T-1
    if n_lags is None:
        n_lags = max(1, min(T - 1, int(np.floor(4 * (T / 100) ** (2/9)))))
    else:
        # Enforce bounds on user-specified lags
        n_lags = max(1, min(T - 1, n_lags))

    # Mean-center the AR series (CRITICAL for correct autocovariance)
    ar_centered = ar_series - np.mean(ar_series)

    # Compute autocovariances with mean-centered data
    # γ_j = (1/T) × Σ_{t=j+1}^{T} ar_t × ar_{t-j}
    gamma = np.zeros(n_lags + 1)
    for j in range(n_lags + 1):
        if j == 0:
            gamma[0] = np.sum(ar_centered ** 2) / T
        else:
            gamma[j] = np.sum(ar_centered[j:] * ar_centered[:-j]) / T

    # Long-Run Variance: LRV = γ₀ + 2 × Σ_{j=1}^{L} w_j × γ_j
    # Note: NO (T-j) factor - that would double-scale
    lrv = gamma[0]
    for j in range(1, n_lags + 1):
        weight = 1 - j / (n_lags + 1)  # Bartlett kernel
        lrv += 2 * weight * gamma[j]

    # Var(CAR) = T × LRV
    var_car = T * lrv

    return np.sqrt(max(var_car, 0))  # Ensure non-negative


def _run_ols_regression(
    y: np.ndarray,
    X: np.ndarray,
) -> tuple[np.ndarray, np.ndarray, float, float]:
    """Run OLS regression: y = X @ beta + epsilon.

    Args:
        y: Dependent variable (excess returns).
        X: Independent variables (with intercept column).

    Returns:
        Tuple of (coefficients, t_stats, r_squared, residual_std).
    """
    ...


def _compute_trading_days_offset(
    base_date: date,
    offset_days: int,
    trading_calendar: pl.DataFrame,
) -> date:
    """Get date that is offset_days trading days from base_date.

    Args:
        base_date: Starting date.
        offset_days: Number of trading days (positive = forward, negative = backward).
        trading_calendar: DataFrame with 'date' column of trading days.

    Returns:
        Target trading date.

    Raises:
        ValueError: If offset goes beyond calendar bounds.
    """
    ...


def _adjust_for_delisting(
    returns: pl.DataFrame,
    crsp_provider: CRSPLocalProvider,
    permno: int,
    event_window_start: date,
    event_window_end: date,
) -> tuple[pl.DataFrame, float | None, list[str]]:
    """Adjust returns for delisting if stock delists during event window.

    See Section 5.1.1 for full implementation with DLRET combination.
    IMPORTANT: Only applies DLRET if delisting date is within event window.

    Returns:
        Tuple of (adjusted_returns, combined_delisting_return, warnings).
    """
    ...
```

---

## 5. Edge Cases and Error Handling

### 5.1 Data Quality Issues

| Issue | Detection | Handling |
|-------|-----------|----------|
| Insufficient estimation data | n_obs < min_estimation_obs | Raise `DataNotFoundError` with details |
| Missing returns in event window | Gap in daily returns | Fill with NaN, warn in result |
| Delisted stock during event | Return ends before post_window | Include DLRET, truncate, warn |
| IPO during estimation window | Estimation start < IPO | Adjust estimation window forward, warn |
| Stock split during windows | Large return spike | Use adjusted returns from CRSP (RET field) |
| Missing RF data | NULL in Fama-French RF | Raise `DataNotFoundError` |

#### 5.1.1 Delisting Return Handling (CRSP DLRET)

When a stock delists during the event window, use CRSP's DLRET (delisting return) field. **CRITICAL:** DLRET must be combined multiplicatively with the last regular return on the SAME delisting date.

```python
def _adjust_for_delisting(
    returns: pl.DataFrame,
    crsp_provider: CRSPLocalProvider,
    permno: int,
    event_window_start: date,
    event_window_end: date,
) -> tuple[pl.DataFrame, float | None, list[str]]:
    """Adjust returns for delisting if stock delists during event window.

    CRSP DLRET captures:
    - Final trading day return adjustment
    - Post-delisting value changes (mergers, liquidation proceeds)

    CRITICAL: DLRET is combined with RET on the SAME date:
        combined_return = (1 + RET) * (1 + DLRET) - 1

    IMPORTANT: Only apply DLRET if delisting date is within the event window.
    Historical DLRET for delistings outside the window should NOT be applied.

    Args:
        returns: DataFrame with 'date' and 'ret' columns
        crsp_provider: CRSP data provider
        permno: CRSP permanent identifier
        event_window_start: Start of event window (for date validation)
        event_window_end: End of event window

    Returns:
        Tuple of (adjusted_returns, combined_delisting_return, warnings).
    """
    warnings = []

    # Get last return date
    last_date = returns.select(pl.col("date").max()).item()

    if last_date >= event_window_end:
        return returns, None, warnings  # No delisting within window

    # Query CRSP for delisting info using permno
    delist_info = crsp_provider.get_delisting_info(permno)

    if delist_info is None:
        return returns, None, warnings  # No delisting info available

    # CRITICAL: Validate delisting date is within event window
    delist_date = delist_info.get("dlstdt")
    if delist_date is not None:
        if delist_date < event_window_start or delist_date > event_window_end:
            warnings.append(
                f"DLRET exists for permno {permno} but delisting date {delist_date} "
                f"is outside event window [{event_window_start}, {event_window_end}]. "
                f"Not applying DLRET adjustment."
            )
            return returns, None, warnings

    if delist_info is None:
        warnings.append(f"No delisting info for permno {permno}")
        return returns, None, warnings

    dlret = delist_info.get("dlret")
    dlstcd = delist_info.get("dlstcd")  # Delisting code

    # Handle missing DLRET based on delisting code (DLSTCD)
    if dlret is None or np.isnan(dlret):
        # DLSTCD-based fallbacks per Shumway (1997):
        # 500-599: Dropped for cause → assume -30%
        # 400-499: Liquidation → assume -100%
        # 200-399: Mergers → assume 0% (value captured in last RET)
        if dlstcd is not None:
            if 500 <= dlstcd < 600:
                dlret = -0.30
                warnings.append(f"DLRET missing, DLSTCD={dlstcd} (dropped), using -30%")
            elif 400 <= dlstcd < 500:
                dlret = -1.0
                warnings.append(f"DLRET missing, DLSTCD={dlstcd} (liquidation), using -100%")
            else:
                dlret = 0.0
                warnings.append(f"DLRET missing, DLSTCD={dlstcd}, using 0%")
        else:
            dlret = -0.30  # Conservative default
            warnings.append(f"DLRET and DLSTCD missing, using -30%")

    # Get last regular return
    last_ret = returns.filter(pl.col("date") == last_date).select("ret").item()

    # Combine multiplicatively on SAME date (not next day!)
    combined_return = (1 + last_ret) * (1 + dlret) - 1

    # Update the last row with combined return
    adjusted_returns = returns.with_columns(
        pl.when(pl.col("date") == last_date)
        .then(pl.lit(combined_return))
        .otherwise(pl.col("ret"))
        .alias("ret")
    ).with_columns(
        pl.when(pl.col("date") == last_date)
        .then(pl.lit(True))
        .otherwise(pl.lit(False))
        .alias("is_delisting")
    )

    return adjusted_returns, combined_return, warnings
```

**Why same-date combination matters:**
- DLRET represents the ADDITIONAL return on delisting date beyond regular trading
- Appending DLRET on next day mis-dates the loss and double-counts if RET already reflects some delisting impact
- CRSP documentation: "DLRET should be used with RET to compute total return on delisting date"

### 5.2 Statistical Edge Cases

| Issue | Detection | Handling |
|-------|-----------|----------|
| Perfect multicollinearity | R² = 1.0 or singular X'X | Warn, fall back to mean-adjusted model |
| Extreme beta values | \|β\| > cap_beta | Warn, cap at ±cap_beta |
| Near-zero variance | Residual std ≈ 0 | Return NaN for significance tests |
| Outlier abnormal returns | \|AR\| > winsorize threshold | Winsorize at configured percentile |

### 5.3 Window Configuration Issues

| Issue | Detection | Handling |
|-------|-----------|----------|
| Overlapping windows | estimation_end + gap >= event_start | Raise `ValueError` with overlap details |
| Zero-length window | pre_window == post_window == 0 | Allow, compute event-day only |
| Negative windows | pre_window or post_window < 0 | Raise `ValueError` |
| Future event date | event_date > today and as_of is None | Warn, allow for scheduled events |
| Non-trading event date | event_date not in trading calendar | Roll to next trading day, warn |

### 5.4 Overlapping Events

| Issue | Detection | Handling |
|-------|-----------|----------|
| Events < min_days apart | Consecutive events for same symbol | Apply overlap_policy from config |
| Same-day events (different symbols) | Multiple events on same calendar day | **Keep all**, apply clustering mitigation (see Section 3.5.4) |
| True duplicates | Same symbol AND event_id on same day | Drop duplicates, keep first |

**Important:** Same-day events for DIFFERENT symbols should NOT be dropped. Cross-sectional correlation is handled via clustered standard errors or calendar-time portfolio regression (Section 3.5.4), not by discarding data.

### 5.5 PEAD-Specific Issues

| Issue | Detection | Handling |
|-------|-----------|----------|
| Insufficient events per quintile | n < 5 | Warn, merge to terciles |
| All events same direction | All surprises positive/negative | Split by median instead |
| Missing surprise data | NULL in surprise_pct | Exclude event, warn |

### 5.6 Index Rebalance Issues

| Issue | Detection | Handling |
|-------|-----------|----------|
| Missing announcement_date | Column not present | Use effective_date, warn |
| Announcement after effective | announcement_date > effective_date | Raise `ValueError` |
| Very short notice | effective - announcement < 2 days | Warn, may affect anticipation analysis |

---

## 6. PIT/Versioning Requirements

### 6.1 Version Tracking

All results must include:
- `dataset_version_id`: Composite hash from CRSP + Fama-French versions
- `dataset_versions`: Dict mapping dataset names to individual versions
- `computation_timestamp`: UTC timestamp of computation
- `as_of_date`: PIT date if used (None if current data)

### 6.2 CompositeVersionInfo

Use `CompositeVersionInfo` from T3.1 microstructure module:

```python
def _get_version_info(self, as_of: date | None = None) -> CompositeVersionInfo:
    """Get version info from both providers."""
    versions = {
        self.DATASET_CRSP: self._get_crsp_version(as_of),
        self.DATASET_FF: self._get_ff_version(as_of),
    }
    return CompositeVersionInfo(
        versions=versions,
        snapshot_id=None,
        is_pit=as_of is not None,
    )
```

### 6.3 Reproducibility Requirement

Given the same:
- Event date and symbol
- Config (model, windows, tests)
- dataset_version_id

The framework MUST produce identical results.

---

## 7. Test Strategy

### 7.1 Unit Tests

| Test Category | Coverage Target | Key Tests |
|---------------|-----------------|-----------|
| Market Model | 95% | OLS regression, R², t-stats, factor models |
| CAR Computation | 95% | Positive/negative CAR, all significance tests |
| Newey-West | 100% | Deterministic validation against statsmodels |
| Trading Calendar | 95% | Offset computation, weekend/holiday handling |
| Gap Enforcement | 100% | Verify gap between estimation and event |
| Overlap Handling | 100% | All overlap policies |
| PEAD | 90% | Quintile sorting, drift calculation |
| Index Rebalance | 90% | Addition/deletion effects, dual dates |
| Edge Cases | 100% | All edge cases from Section 5 |
| Version Propagation | 100% | CompositeVersionInfo in all results |

### 7.2 Test Data

Create mock data helpers:
- `_create_mock_returns()`: Generate stock returns with known properties
- `_create_mock_ff_data()`: Generate Fama-French factors with RF
- `_create_trading_calendar()`: Generate trading calendar with holidays
- `_create_earnings_events()`: Generate earnings events with surprises
- `_create_index_changes()`: Generate index addition/deletion events

### 7.3 Validation Tests

| Test | Expected Result | Validation Method |
|------|-----------------|-------------------|
| Newey-West SE | Match statsmodels | Cross-check with statsmodels.stats.sandwich_covariance |
| Newey-West mean-centering | Centered AR produces correct γ_j | Compare centered vs uncentered |
| OLS regression | Match numpy.linalg.lstsq | Deterministic synthetic data |
| Patell Z-statistic | ~N(0,1) under null | Simulate N events with AR=0, check Z~N(0,1) |
| BMP t-statistic | ~t(N-1) under null | Simulate N events with AR=0, check coverage |
| PEAD drift direction | Q5 > Q1 | Use synthetic data with known drift |
| Index add effect | Positive CAR | Use synthetic data with known effect |
| Gap enforcement | Raise ValueError if gap < config | Boundary tests |
| Version propagation | All results have dataset_version_id | Assert on every result type |

#### 7.3.1 Patell Test Validation (with Estimation Error)

```python
def test_patell_z_with_estimation_error():
    """Validate Patell Z ~ N(0,1) when AR = 0, accounting for estimation error.

    This test properly simulates the full data generating process including
    market model estimation, not just the final SCAR distribution.
    """
    np.random.seed(42)
    n_simulations = 500
    n_events = 30
    T1 = 120  # estimation window
    T2 = 21  # event window
    z_stats = []

    for _ in range(n_simulations):
        scars = []
        for _ in range(n_events):
            # Generate estimation period
            mkt_est = np.random.normal(0.0005, 0.01, T1)
            true_alpha, true_beta = 0.0, 1.0
            stock_est = true_alpha + true_beta * mkt_est + np.random.normal(0, 0.015, T1)

            # Estimate market model
            X = np.column_stack([np.ones(T1), mkt_est])
            beta_hat = np.linalg.lstsq(X, stock_est, rcond=None)[0]
            resid = stock_est - X @ beta_hat
            sigma_hat = np.std(resid, ddof=2)
            mkt_mean = np.mean(mkt_est)
            Sxx = np.sum((mkt_est - mkt_mean) ** 2)

            # Generate event period (null: no abnormal return)
            mkt_evt = np.random.normal(0.0005, 0.01, T2)
            stock_evt = true_alpha + true_beta * mkt_evt + np.random.normal(0, 0.015, T2)

            # Compute AR and SAR with forecast-error adjustment
            predicted = beta_hat[0] + beta_hat[1] * mkt_evt
            ar = stock_evt - predicted

            sar = np.zeros(T2)
            for t in range(T2):
                C_t = 1 + 1/T1 + (mkt_evt[t] - mkt_mean)**2 / Sxx
                sar[t] = ar[t] / (sigma_hat * np.sqrt(C_t))

            scar = np.sum(sar) / np.sqrt(T2)
            scars.append(scar)

        z_patell = np.sum(scars) / np.sqrt(n_events)
        z_stats.append(z_patell)

    # Z should be approximately N(0,1)
    assert abs(np.mean(z_stats)) < 0.15  # Mean near 0
    assert 0.85 < np.std(z_stats) < 1.15  # Std near 1
```

#### 7.3.2 BMP Test Validation (with Estimation Error)

```python
def test_bmp_coverage_with_estimation_error():
    """Validate BMP t-statistic has correct 95% coverage with estimation error."""
    np.random.seed(42)
    n_simulations = 500
    n_events = 30
    T1 = 120
    T2 = 21
    rejections = 0

    for _ in range(n_simulations):
        scars = []
        for _ in range(n_events):
            # [Same DGP as Patell test above]
            mkt_est = np.random.normal(0.0005, 0.01, T1)
            stock_est = mkt_est + np.random.normal(0, 0.015, T1)
            X = np.column_stack([np.ones(T1), mkt_est])
            beta_hat = np.linalg.lstsq(X, stock_est, rcond=None)[0]
            resid = stock_est - X @ beta_hat
            sigma_hat = np.std(resid, ddof=2)
            mkt_mean = np.mean(mkt_est)
            Sxx = np.sum((mkt_est - mkt_mean) ** 2)

            mkt_evt = np.random.normal(0.0005, 0.01, T2)
            stock_evt = mkt_evt + np.random.normal(0, 0.015, T2)
            predicted = beta_hat[0] + beta_hat[1] * mkt_evt
            ar = stock_evt - predicted

            sar = np.zeros(T2)
            for t in range(T2):
                C_t = 1 + 1/T1 + (mkt_evt[t] - mkt_mean)**2 / Sxx
                sar[t] = ar[t] / (sigma_hat * np.sqrt(C_t))

            # BMP DOF correction
            dof_correction = np.sqrt((T1 - 2) / (T1 - 4))
            scar = dof_correction * np.sum(sar) / np.sqrt(T2)
            scars.append(scar)

        scars = np.array(scars)
        scar_mean = np.mean(scars)
        scar_std = np.std(scars, ddof=1)
        t_bmp = np.sqrt(n_events) * scar_mean / scar_std

        from scipy.stats import t
        critical = t.ppf(0.975, df=n_events - 1)
        if abs(t_bmp) > critical:
            rejections += 1

    rejection_rate = rejections / n_simulations
    assert 0.03 < rejection_rate < 0.08  # ~5% false rejection
```

#### 7.3.3 DLRET Combination Validation

```python
def test_dlret_multiplicative_combination():
    """Validate DLRET is combined multiplicatively with RET."""
    # Scenario: Stock drops 20% on last day, then delists with -50% DLRET
    last_ret = -0.20
    dlret = -0.50

    # Correct: multiplicative combination
    combined = (1 + last_ret) * (1 + dlret) - 1
    # (1 - 0.20) * (1 - 0.50) - 1 = 0.80 * 0.50 - 1 = -0.60

    assert combined == pytest.approx(-0.60)

    # Wrong: additive (what we want to avoid)
    wrong_additive = last_ret + dlret  # -0.70 (incorrect)
    assert wrong_additive != combined


def test_dlret_dlstcd_fallbacks():
    """Validate DLSTCD-based fallbacks for missing DLRET."""
    from libs.analytics.event_study import _get_dlret_fallback

    # Dropped for cause (500-599) → -30%
    assert _get_dlret_fallback(dlstcd=520) == pytest.approx(-0.30)

    # Liquidation (400-499) → -100%
    assert _get_dlret_fallback(dlstcd=450) == pytest.approx(-1.0)

    # Merger (200-399) → 0%
    assert _get_dlret_fallback(dlstcd=250) == pytest.approx(0.0)
```

#### 7.3.4 Newey-West Lag Bounds Validation

```python
def test_newey_west_lag_bounds():
    """Validate lag bounds: 1 ≤ L ≤ T-1."""
    from libs.analytics.event_study import _compute_newey_west_se

    # T=3, L should be capped at 2
    ar = np.array([0.01, 0.02, 0.03])
    se = _compute_newey_west_se(ar, n_lags=10)  # Requested 10, should cap at 2
    assert not np.isnan(se)

    # T=2, L should be 1
    ar_short = np.array([0.01, 0.02])
    se_short = _compute_newey_west_se(ar_short)
    assert not np.isnan(se_short)

    # T=1, should return NaN
    ar_single = np.array([0.01])
    se_single = _compute_newey_west_se(ar_single)
    assert np.isnan(se_single)
```

#### 7.3.5 Clustered Standard Error Validation

```python
def test_clustered_se_size_control():
    """Validate clustered SE controls size under clustered null.

    Simulates events with same-day clustering where SCARs within a cluster
    are correlated. Verifies that clustered SE produces correct rejection rate.
    """
    np.random.seed(42)
    n_simulations = 500
    n_events = 60
    n_clusters = 20  # 3 events per cluster on average
    rejections_clustered = 0
    rejections_naive = 0

    for _ in range(n_simulations):
        # Generate clustered data: events on same day are correlated
        cluster_ids = np.repeat(np.arange(n_clusters), n_events // n_clusters)
        cluster_effects = np.random.normal(0, 0.5, n_clusters)  # Shared cluster shock

        scars = np.zeros(n_events)
        for i in range(n_events):
            c = cluster_ids[i]
            scars[i] = cluster_effects[c] + np.random.normal(0, 1)  # SCAR = cluster + idio

        scar_mean = np.mean(scars)

        # Naive SE (ignores clustering)
        naive_se = np.std(scars, ddof=1) / np.sqrt(n_events)
        t_naive = scar_mean / naive_se
        if abs(t_naive) > 1.96:
            rejections_naive += 1

        # Clustered SE
        from libs.analytics.event_study import _compute_clustered_se
        clustered_se, t_clustered, df = _compute_clustered_se(scars, cluster_ids)
        from scipy.stats import t
        critical = t.ppf(0.975, df=df)
        if abs(t_clustered) > critical:
            rejections_clustered += 1

    naive_rate = rejections_naive / n_simulations
    clustered_rate = rejections_clustered / n_simulations

    # Naive should over-reject (>10% expected due to clustering)
    assert naive_rate > 0.10, f"Naive should over-reject, got {naive_rate}"

    # Clustered should control size near 5%
    assert 0.03 < clustered_rate < 0.08, f"Clustered rejection rate {clustered_rate} not near 5%"
```

#### 7.3.6 Calendar-Time Portfolio Validation

```python
def test_calendar_time_portfolio_size_control():
    """Validate calendar-time portfolio controls size under clustered events."""
    np.random.seed(42)
    n_simulations = 500
    n_calendar_days = 50  # Days with events
    events_per_day = 3  # Avg events per day
    rejections = 0

    for _ in range(n_simulations):
        # Generate calendar-day level data
        # Each day: compute average AR across events on that day
        daily_ar_portfolio = []

        for _ in range(n_calendar_days):
            # Events on this day share some correlation
            cluster_effect = np.random.normal(0, 0.02)  # Market-wide shock
            n_events_today = np.random.poisson(events_per_day)
            n_events_today = max(1, n_events_today)

            # Individual event ARs = cluster effect + idiosyncratic
            event_ars = cluster_effect + np.random.normal(0, 0.03, n_events_today)
            daily_ar_portfolio.append(np.mean(event_ars))

        # Test if portfolio mean = 0 using Newey-West
        portfolio_ar = np.array(daily_ar_portfolio)
        T = len(portfolio_ar)
        portfolio_mean = np.mean(portfolio_ar)

        from libs.analytics.event_study import _compute_newey_west_se
        se = _compute_newey_west_se(portfolio_ar)
        t_stat = portfolio_mean / se if se > 0 else float("nan")

        if not np.isnan(t_stat) and abs(t_stat) > 1.96:
            rejections += 1

    rejection_rate = rejections / n_simulations
    # Should control size near 5%
    assert 0.03 < rejection_rate < 0.08, f"Calendar-time rejection rate {rejection_rate} not near 5%"
```

### 7.4 Test File Structure

```
tests/libs/analytics/test_event_study.py
├── TestConfiguration
│   ├── test_default_config
│   ├── test_custom_config
│   ├── test_invalid_config_raises
│   ├── test_patell_requires_market_model
│   ├── test_bmp_requires_market_model
│   ├── test_min_days_between_events_auto_from_window
│   └── test_min_days_between_events_validation
├── TestTradingCalendar
│   ├── test_offset_forward
│   ├── test_offset_backward
│   ├── test_roll_nontrading_to_next
│   ├── test_weekend_handling
│   └── test_holiday_handling
├── TestMarketModel
│   ├── test_ols_regression_basic
│   ├── test_ff3_model
│   ├── test_ff5_model
│   ├── test_mean_adjusted_model
│   ├── test_insufficient_data_error
│   ├── test_beta_capping
│   └── test_version_id_included
├── TestNeweyWest
│   ├── test_bartlett_weights
│   ├── test_auto_lag_selection
│   ├── test_minimum_lag_enforced
│   ├── test_maximum_lag_capped_at_T_minus_1
│   ├── test_mean_centering
│   ├── test_T_less_than_2_returns_nan
│   └── test_deterministic_vs_statsmodels
├── TestSignificanceTests
│   ├── test_patell_standardized_ar_with_forecast_error
│   ├── test_patell_cross_sectional_z
│   ├── test_patell_z_distribution_under_null
│   ├── test_bmp_dof_correction
│   ├── test_bmp_cross_sectional_std
│   ├── test_bmp_coverage_under_null
│   └── test_bmp_event_induced_variance_robust
├── TestClusteringMitigation
│   ├── test_detect_event_clustering
│   ├── test_calendar_time_portfolio
│   ├── test_clustered_standard_errors
│   ├── test_clustered_se_formula_validation
│   ├── test_clustered_se_fewer_than_2_clusters
│   ├── test_auto_mitigation_selection
│   └── test_clustering_diagnostics_in_result
├── TestDelistingHandling
│   ├── test_dlret_multiplicative_combination
│   ├── test_dlstcd_500_fallback
│   ├── test_dlstcd_400_fallback
│   ├── test_dlstcd_200_fallback
│   └── test_missing_dlret_and_dlstcd
├── TestGapEnforcement
│   ├── test_gap_enforced
│   ├── test_overlap_raises_error
│   └── test_gap_boundary_cases
├── TestCARComputation
│   ├── test_positive_car
│   ├── test_negative_car
│   ├── test_t_test_significance
│   ├── test_patell_test
│   ├── test_bmp_test
│   ├── test_window_configuration
│   ├── test_nontrading_date_roll
│   ├── test_delisting_adjustment
│   └── test_version_tracking
├── TestOverlapHandling
│   ├── test_drop_later_policy
│   ├── test_drop_earlier_policy
│   ├── test_warn_only_policy
│   └── test_min_days_between_events
├── TestPEAD
│   ├── test_quintile_sorting
│   ├── test_cross_sectional_formation
│   ├── test_drift_calculation
│   ├── test_insufficient_events_terciles
│   ├── test_overlap_exclusion
│   └── test_formation_day_after_announcement
├── TestIndexRebalance
│   ├── test_addition_effect
│   ├── test_deletion_effect
│   ├── test_announcement_date
│   ├── test_effective_date
│   ├── test_volume_changes
│   └── test_missing_announcement_date_warning
└── TestEdgeCases
    ├── test_delisted_stock
    ├── test_ipo_during_estimation
    ├── test_overlapping_windows_error
    ├── test_missing_rf_data
    └── test_extreme_beta_warning
```

---

## 8. Implementation Order

### Phase 1: Core Infrastructure (Day 1)
1. Create `event_study.py` with imports, enums, and dataclasses
2. Implement `EventStudyConfig` with validation
3. Implement `_get_version_info()` for composite versioning
4. Implement `_get_trading_calendar()` and `_roll_to_trading_day()`
5. Implement `_compute_trading_days_offset()`
6. Write unit tests for calendar helpers

### Phase 2: Statistical Helpers (Day 1)
1. Implement `_run_ols_regression()` for all models
2. Implement `_compute_newey_west_se()` with Bartlett kernel
3. Write deterministic tests for OLS
4. Write cross-validation tests for Newey-West vs statsmodels

### Phase 3: Market Model & CAR (Day 2)
1. Implement `estimate_market_model()` with all model types
2. Implement `_compute_excess_returns()` using FF RF
3. Implement `_handle_overlapping_events()` with all policies
4. Implement `compute_car()` with gap enforcement
5. Implement `_adjust_for_delisting()`
6. Write comprehensive tests for market model and CAR

### Phase 4: PEAD Analysis (Day 2-3)
1. Implement `analyze_pead()` with cross-sectional quintiles
2. Add formation day (day after announcement) logic
3. Add overlap handling
4. Write tests for PEAD

### Phase 5: Index Rebalance (Day 3)
1. Implement `analyze_index_rebalance()` with dual-date support
2. Add volume analysis
3. Write tests for index effects

### Phase 6: Integration & Documentation (Day 3-4)
1. Update `libs/analytics/__init__.py` exports
2. Create `docs/CONCEPTS/event-studies.md`
3. Run full test suite
4. Achieve >90% coverage
5. Run linters and fix issues

---

## 9. Acceptance Criteria Checklist

| Criterion | Implementation | Test Coverage |
|-----------|----------------|---------------|
| Market model estimation | `estimate_market_model()` | `TestMarketModel` |
| Multiple expected-return models | `ExpectedReturnModel` enum | `test_ff3_model`, `test_ff5_model` |
| CAR calculation | `compute_car()` | `TestCARComputation` |
| Configurable gap_days | `EventStudyConfig.gap_days` | `TestGapEnforcement` |
| Multiple significance tests | `SignificanceTest` enum | `test_patell_test`, `test_bmp_test` |
| Newey-West SE | `_compute_newey_west_se()` | `TestNeweyWest` |
| Overlap handling | `_handle_overlapping_events()` | `TestOverlapHandling` |
| Non-trading day handling | `_roll_to_trading_day()` | `test_nontrading_date_roll` |
| Delisting adjustments | `_adjust_for_delisting()` | `test_delisting_adjustment` |
| PEAD by quintile | `analyze_pead()` | `TestPEAD` |
| Index rebalance | `analyze_index_rebalance()` | `TestIndexRebalance` |
| Announcement/effective dates | dual-date support | `test_announcement_date` |
| Version propagation | All results | `test_version_id_included` |
| >90% coverage | All modules | pytest-cov report |

---

## 10. Risk Factors

| Risk | Mitigation |
|------|------------|
| Newey-West complexity | Validate against statsmodels; use basic implementation first |
| Trading calendar gaps | Use CRSP data directly; warn on gaps |
| Large PEAD dataset | Process in batches, use polars lazy evaluation |
| Statistical validation | Compare against known academic results |
| FF data availability | Check FF factors are synced before starting |

---

## 11. Dependencies

### Python Libraries (already in project):
- `polars` - DataFrame operations
- `numpy` - Numerical computations
- `scipy.stats` - T-distribution for p-values

### Development/Test Dependencies (add to requirements-dev.txt if missing):
- `statsmodels` - Required for Newey-West cross-validation tests
  - If missing: tests using statsmodels should be skipped with `pytest.importorskip("statsmodels")`
  - Add to requirements-dev.txt: `statsmodels>=0.14.0`

### Data Dependencies:
- **CRSP daily data:** Stock returns, prices, volume, delisting returns
- **Fama-French factors:** MKT-RF, SMB, HML, RMW, CMA, RF (daily)
- **Trading calendar:** Inferred from CRSP

---

## 12. Questions for Review (ANSWERED)

1. **Estimation window gap:** ✅ **Yes, enforce 5-10 day gap** (default: 5). Added `gap_days` parameter.

2. **Market return source:** ✅ **Use Fama-French MKT-RF + RF** to form excess returns.

3. **Volume normalization:** ✅ **Use estimation window average** (matching estimation_window length), winsorize 1% tails, warn if < 60 obs.

4. **PEAD quintile boundaries:** ✅ **Equal-count quintiles (20% each)** formed cross-sectionally per event date; fall back to terciles if any bin has < 5 events.

---

**Created:** 2025-12-08
**Author:** Dev B (Claude Code)
**Status:** v8 - Pending Codex Review (Gemini approved v7, Codex REJECTED v7 - fixes applied)
