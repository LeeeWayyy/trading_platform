# P4T1.1: Data Quality & Validation Framework - Implementation Plan

**Component:** T1.1-Data-Quality-Framework
**Task:** P4T1 Phase 4 Data Infrastructure
**Status:** APPROVED - Ready for Implementation
**Created:** 2025-12-03
**Last Updated:** 2025-12-03

---

## Overview

This component creates the foundational data quality and validation framework for WRDS data syncs. It provides:
- Sync manifest tracking for reproducibility
- Data validation (row counts, nulls, schema, date continuity)
- Schema drift detection with policy enforcement
- Checksum verification for data integrity

---

## Files to Create

### Library Code
| File | Purpose |
|------|---------|
| `libs/data_quality/__init__.py` | Module exports |
| `libs/data_quality/types.py` | Shared types: LockToken, TradingCalendar protocol, DiskSpaceStatus |
| `libs/data_quality/manifest.py` | SyncManifest model and ManifestManager |
| `libs/data_quality/validation.py` | DataValidator class |
| `libs/data_quality/schema.py` | SchemaRegistry and SchemaDrift |
| `libs/data_quality/exceptions.py` | Custom exceptions (extend DataQualityError) |

### Tests
| File | Purpose |
|------|---------|
| `tests/libs/data_quality/__init__.py` | Test module |
| `tests/libs/data_quality/test_types.py` | LockToken serialization/lifecycle tests |
| `tests/libs/data_quality/test_manifest.py` | Manifest tests |
| `tests/libs/data_quality/test_validation.py` | Validation tests |
| `tests/libs/data_quality/test_schema.py` | Schema registry tests |

### Documentation
| File | Purpose |
|------|---------|
| `docs/ADRs/0019-data-quality-framework.md` | Architecture Decision Record |

---

## Class Specifications

### 0. Shared Types (libs/data_quality/types.py)

```python
from dataclasses import dataclass
from datetime import datetime
from typing import Literal
from pathlib import Path

@dataclass
class LockToken:
    """
    Token proving exclusive lock ownership.

    Lock file format (JSON at data/locks/{dataset}.lock):
    {
        "pid": 12345,
        "hostname": "worker-01",
        "writer_id": "sync-job-abc123",
        "acquired_at": "2025-01-15T10:30:00Z",
        "expires_at": "2025-01-15T14:30:00Z"  # 4 hour max
    }

    Lifecycle:
    1. Acquire: O_CREAT | O_EXCL | O_WRONLY atomically creates lock file
    2. Validate: Check pid/hostname/writer_id match, not expired, mtime fresh
    3. Refresh: Update mtime periodically (every 60s) during long operations
    4. Release: Delete lock file on completion or crash recovery
    """
    pid: int
    hostname: str
    writer_id: str
    acquired_at: datetime
    expires_at: datetime
    lock_path: Path

    def is_expired(self) -> bool:
        """Check if lock has expired (> 4 hours)."""
        return datetime.now(self.acquired_at.tzinfo) > self.expires_at

    def to_dict(self) -> dict:
        """Serialize for lock file."""
        return {
            "pid": self.pid,
            "hostname": self.hostname,
            "writer_id": self.writer_id,
            "acquired_at": self.acquired_at.isoformat(),
            "expires_at": self.expires_at.isoformat(),
        }

    @classmethod
    def from_dict(cls, data: dict, lock_path: Path) -> "LockToken":
        """Deserialize from lock file."""
        return cls(
            pid=data["pid"],
            hostname=data["hostname"],
            writer_id=data["writer_id"],
            acquired_at=datetime.fromisoformat(data["acquired_at"]),
            expires_at=datetime.fromisoformat(data["expires_at"]),
            lock_path=lock_path,
        )


# TradingCalendar Protocol
# Concrete implementation: Use exchange_calendars library (pip install exchange-calendars)
# Import: from exchange_calendars import get_calendar
# Usage: calendar = get_calendar("XNYS")  # NYSE calendar
#
# For tests: Use libs.data_quality.testing.MockTradingCalendar
from typing import Protocol
import datetime

class TradingCalendar(Protocol):
    """
    Protocol for trading calendar implementations.

    Concrete implementations:
    - Production: exchange_calendars.get_calendar("XNYS") for NYSE
    - Testing: libs.data_quality.testing.MockTradingCalendar

    Example:
        from exchange_calendars import get_calendar
        nyse = get_calendar("XNYS")
        # Wrap with adapter if needed for Protocol compliance
    """

    def is_trading_day(self, date: datetime.date) -> bool:
        """Return True if date is a trading day."""
        ...

    def trading_days_between(
        self, start: datetime.date, end: datetime.date
    ) -> list[datetime.date]:
        """Return list of trading days in range (inclusive)."""
        ...


# Adapter for exchange_calendars (optional helper)
class ExchangeCalendarAdapter:
    """Adapter to make exchange_calendars compatible with TradingCalendar protocol."""

    def __init__(self, exchange_code: str = "XNYS"):
        from exchange_calendars import get_calendar
        self._cal = get_calendar(exchange_code)

    def is_trading_day(self, date: datetime.date) -> bool:
        import pandas as pd
        return self._cal.is_session(pd.Timestamp(date))

    def trading_days_between(
        self, start: datetime.date, end: datetime.date
    ) -> list[datetime.date]:
        import pandas as pd
        sessions = self._cal.sessions_in_range(
            pd.Timestamp(start), pd.Timestamp(end)
        )
        return [s.date() for s in sessions]
```

### 1. SyncManifest (Pydantic Model)

```python
from datetime import datetime, date
from typing import Literal
from pydantic import BaseModel, field_validator

class SyncManifest(BaseModel):
    """Tracks data sync state for reproducibility."""

    # Dataset identification
    dataset: str                              # e.g., "crsp_daily"

    # Sync metadata
    sync_timestamp: datetime                  # UTC timezone-aware
    start_date: date                          # Data range start
    end_date: date                            # Data range end

    # Data integrity
    row_count: int                            # Total rows synced
    checksum: str                             # SHA-256 of parquet file(s)
    checksum_algorithm: Literal["sha256"] = "sha256"

    # Schema tracking
    schema_version: str                       # e.g., "v1.0.0"
    wrds_query_hash: str                      # Hash of SQL query used

    # File tracking
    file_paths: list[str]                     # Parquet files included

    # Validation status
    validation_status: Literal["passed", "failed", "quarantined"]
    quarantine_path: str | None = None        # Path if quarantined

    # Versioning (for rollback)
    manifest_version: int = 1                 # Increments on update
    previous_checksum: str | None = None      # For rollback verification

    # Invariant validators
    @field_validator('sync_timestamp')
    @classmethod
    def validate_utc(cls, v):
        """Ensure timestamp is UTC (offset == 0), not just timezone-aware."""
        if v.tzinfo is None:
            raise ValueError("sync_timestamp must be timezone-aware")
        if v.utcoffset().total_seconds() != 0:
            raise ValueError("sync_timestamp must be UTC (offset == 0)")
        return v

    @field_validator('end_date')
    @classmethod
    def validate_date_range(cls, v, info):
        """Ensure start_date <= end_date."""
        if 'start_date' in info.data and v < info.data['start_date']:
            raise ValueError("end_date must be >= start_date")
        return v

    @field_validator('file_paths')
    @classmethod
    def validate_non_empty(cls, v):
        """Ensure file_paths is non-empty."""
        if not v:
            raise ValueError("file_paths must not be empty")
        return v
```

### 2. ManifestManager

```python
class ManifestManager:
    """Manages sync manifests with atomic writes and lock coupling."""

    MANIFEST_DIR = Path("data/manifests")
    REQUIRED_DISK_MULTIPLIER = 2.0  # >= 2x expected write size
    DISK_WARNING_THRESHOLD = 0.80   # 80% capacity
    DISK_CRITICAL_THRESHOLD = 0.90  # 90% capacity
    DISK_BLOCKED_THRESHOLD = 0.95   # 95% - refuse writes

    def __init__(self, storage_path: Path = MANIFEST_DIR):
        self.storage_path = storage_path
        self.storage_path.mkdir(parents=True, exist_ok=True)

    def load_manifest(self, dataset: str) -> SyncManifest | None:
        """Load manifest for dataset. Returns None if not found."""
        pass

    def save_manifest(
        self,
        manifest: SyncManifest,
        lock_token: LockToken,
        expected_bytes: int | None = None
    ) -> None:
        """
        Save manifest with atomic write.

        Args:
            manifest: The manifest to save
            lock_token: Token proving exclusive lock ownership
            expected_bytes: Expected size of data files being synced (for disk check).
                           If None, uses manifest JSON size * REQUIRED_DISK_MULTIPLIER.
                           For data syncs, pass sum of parquet file sizes.

        Requirements:
        1. assert_lock_held(lock_token) - verify caller holds exclusive lock
        2. check_disk_space(expected_bytes or estimate) - verify sufficient space
        3. Atomic write: temp file -> SHA-256 verify -> rename -> fsync
        4. Update manifest_version and previous_checksum

        Raises:
            LockNotHeldError: If lock not held or expired
            DiskSpaceError: If insufficient disk space
            QuarantineError: If write fails (ENOSPC)
        """
        pass

    def assert_lock_held(self, lock_token: LockToken) -> None:
        """
        Verify caller holds valid exclusive lock.

        Checks:
        - Lock file exists
        - Lock token matches (pid, hostname, writer_id)
        - Lock not expired (< 4 hours)
        - Lock file freshness (mtime recent)

        Raises:
            LockNotHeldError: If any check fails
        """
        pass

    def check_disk_space(self, required_bytes: int) -> DiskSpaceStatus:
        """
        Verify sufficient disk space for write.

        Policy (from P4T1):
        - Required: >= 2x expected write size
        - 80% capacity: WARNING logged, operation proceeds
        - 90% capacity: CRITICAL logged, operation proceeds with alert
        - 95% capacity: BLOCKED, raises DiskSpaceError

        Returns:
            DiskSpaceStatus with level (ok/warning/critical) and free_bytes

        Raises:
            DiskSpaceError: If blocked threshold (95%) exceeded
        """
        pass

    def rollback_on_failure(self, dataset: str) -> SyncManifest | None:
        """
        Restore previous manifest version on sync failure.

        Rollback mechanics:
        1. Load current manifest (if exists)
        2. Check previous_checksum is set (has rollback target)
        3. Load backup manifest from data/manifests/backups/{dataset}_{version}.json
        4. Verify backup checksum matches previous_checksum
        5. Atomic replace current manifest with backup
        6. Return restored manifest or None if no rollback possible

        Interacts with quarantine:
        - If current data is quarantined, rollback restores last known-good state
        - Quarantined files remain in quarantine_path for investigation
        - Rollback does NOT delete quarantined files (manual cleanup required)
        """
        pass

    def quarantine_data(
        self,
        manifest: SyncManifest,
        reason: str,
        lock_token: LockToken
    ) -> str:
        """
        Move failed sync data to quarantine.

        Quarantine path: data/quarantine/{dataset}/{timestamp}/
        - Copies manifest to quarantine dir
        - Updates manifest.quarantine_path
        - Updates manifest.validation_status = "quarantined"
        - Logs quarantine event with reason

        Returns:
            Quarantine path where data was moved

        Raises:
            QuarantineError: If quarantine operation fails
            LockNotHeldError: If lock not held
        """
        pass

    def list_manifests(self) -> list[SyncManifest]:
        """List all manifests."""
        pass

    def create_snapshot(self, version_tag: str) -> None:
        """Create immutable snapshot of all current manifests."""
        pass


@dataclass
class DiskSpaceStatus:
    """Result of disk space check."""
    level: Literal["ok", "warning", "critical"]
    free_bytes: int
    total_bytes: int
    used_pct: float
    message: str
```

### 3. DataValidator

```python
from dataclasses import dataclass
import polars as pl

@dataclass
class ValidationError:
    """Structured validation error."""
    field: str
    message: str
    severity: Literal["error", "warning"]
    value: Any = None

@dataclass
class AnomalyAlert:
    """Alert for anomalous data patterns."""
    metric: str
    current_value: float
    expected_value: float
    deviation_pct: float
    message: str

class DataValidator:
    """Validates data quality for WRDS syncs."""

    def validate_row_count(
        self,
        df: pl.DataFrame,
        expected: int,
        tolerance: float = 0.05
    ) -> list[ValidationError]:
        """
        Validate row count within tolerance.

        Returns errors if abs(actual - expected) / expected > tolerance.
        """
        pass

    def validate_null_percentage(
        self,
        df: pl.DataFrame,
        columns: dict[str, float]  # column -> max_null_pct
    ) -> list[ValidationError]:
        """Validate null percentages per column."""
        pass

    def validate_schema(
        self,
        df: pl.DataFrame,
        expected_schema: dict[str, str]  # column -> dtype string
    ) -> list[ValidationError]:
        """
        Validate DataFrame schema matches expected.

        Dtype mapping (expected_schema string -> polars DataType):
        - "int64" / "Int64" → pl.Int64
        - "float64" / "Float64" → pl.Float64
        - "str" / "Utf8" / "String" → pl.Utf8
        - "bool" / "Boolean" → pl.Boolean
        - "date" / "Date" → pl.Date
        - "datetime" / "Datetime" → pl.Datetime
        - "datetime[us]" → pl.Datetime("us")
        - "datetime[ns]" → pl.Datetime("ns")

        Comparison is case-insensitive and supports aliases.
        Unknown dtype strings raise ValidationError with severity="error".
        """
        pass

    # Dtype mapping constant for schema validation
    # Maps lowercase string aliases to actual polars DataType objects
    DTYPE_MAP: dict[str, pl.DataType] = {
        "int64": pl.Int64,
        "int32": pl.Int32,
        "int16": pl.Int16,
        "int8": pl.Int8,
        "float64": pl.Float64,
        "float32": pl.Float32,
        "str": pl.Utf8,
        "utf8": pl.Utf8,
        "string": pl.Utf8,
        "bool": pl.Boolean,
        "boolean": pl.Boolean,
        "date": pl.Date,
        "datetime": pl.Datetime("us"),      # Default microseconds
        "datetime[us]": pl.Datetime("us"),  # Explicit microseconds
        "datetime[ns]": pl.Datetime("ns"),  # Explicit nanoseconds
        "datetime[ms]": pl.Datetime("ms"),  # Explicit milliseconds
    }

    def validate_date_continuity(
        self,
        df: pl.DataFrame,
        date_col: str,
        calendar: TradingCalendar | None = None
    ) -> list[ValidationError]:
        """
        Validate date continuity (no unexpected gaps).

        If calendar provided, excludes market holidays from gap detection.
        """
        pass

    def compute_checksum(self, file_path: Path) -> str:
        """Compute SHA-256 checksum of single file."""
        pass

    def compute_aggregate_checksum(self, file_paths: list[Path]) -> str:
        """
        Compute aggregate SHA-256 checksum for multiple files.

        Algorithm:
        1. Sort file_paths alphabetically for determinism
        2. Compute individual SHA-256 for each file
        3. Concatenate hashes as: "path1:hash1\npath2:hash2\n..."
        4. Return SHA-256 of the concatenated string

        This ensures:
        - Order independence (sorted paths)
        - Content verification (individual file hashes)
        - Tamper detection (aggregate hash)
        """
        pass

    def verify_checksum(self, file_path: Path, expected: str) -> bool:
        """Verify single file checksum matches expected."""
        pass

    def verify_aggregate_checksum(
        self, file_paths: list[Path], expected: str
    ) -> bool:
        """Verify aggregate checksum for multiple files."""
        pass

    def detect_anomalies(
        self,
        current_stats: dict,
        prev_stats: dict | None
    ) -> list[AnomalyAlert]:
        """
        Detect anomalous changes from previous sync.

        Checks:
        - Sudden row count drops (> 10%)
        - Null percentage spikes (> 5% increase)
        - Missing date ranges
        """
        pass
```

### 4. SchemaRegistry

```python
@dataclass
class SchemaDrift:
    """Result of schema drift detection."""
    added_columns: list[str]
    removed_columns: list[str]
    changed_columns: list[tuple[str, str, str]]  # (col, old_type, new_type)

    @property
    def is_breaking(self) -> bool:
        """True if drift includes removed or changed columns."""
        return bool(self.removed_columns or self.changed_columns)

    @property
    def has_additions(self) -> bool:
        """True if new columns were added."""
        return bool(self.added_columns)

@dataclass
class DatasetSchema:
    """Schema definition for a dataset."""
    dataset: str
    version: str
    columns: dict[str, str]  # column -> dtype
    created_at: datetime
    description: str = ""

class SchemaRegistry:
    """
    Manages expected schemas with versioning and drift detection.

    Storage: JSON files at data/schemas/{dataset}.json

    Version format: "v{major}.{minor}.{patch}"
    - major: Breaking changes (manual intervention required)
    - minor: Additive changes (auto-bumped on new columns)
    - patch: Metadata/description updates only

    Increment rules:
    - New columns detected → minor += 1, patch = 0
    - Description update only → patch += 1
    - Breaking drift → error, no version bump

    Locking: Uses file-level locking for concurrent writer safety
    - Acquire lock before read-modify-write
    - Use O_EXCL for atomic schema file creation
    - Release lock after fsync completes
    """

    SCHEMA_DIR = Path("data/schemas")
    LOCK_DIR = Path("data/locks")

    def __init__(self, storage_path: Path = SCHEMA_DIR):
        self.storage_path = storage_path
        self.storage_path.mkdir(parents=True, exist_ok=True)

    def get_expected_schema(
        self,
        dataset: str,
        version: str | None = None
    ) -> DatasetSchema | None:
        """
        Get expected schema for dataset.

        If version is None, returns latest version.
        """
        pass

    def detect_drift(
        self,
        dataset: str,
        current_schema: dict[str, str]
    ) -> SchemaDrift:
        """
        Detect schema drift from expected.

        Policy (from P4T1):
        - New columns: Accept with warning, auto-version bump
        - Removed columns: Reject, block manifest update
        - Type changes: Reject, block manifest update
        """
        pass

    def register_schema(
        self,
        dataset: str,
        schema: dict[str, str],
        description: str = ""
    ) -> str:
        """
        Register new schema version.

        Uses atomic temp+fsync write with lock.
        Returns new version string.
        """
        pass

    def apply_drift_policy(
        self,
        dataset: str,
        drift: SchemaDrift,
        current_schema: dict[str, str]
    ) -> tuple[str, str]:
        """
        Apply drift policy and handle auto-version bump.

        If breaking drift (removed/changed columns):
            - Raises SchemaError
            - Blocks manifest update

        If additions only:
            1. Logs WARNING about new columns
            2. Creates new schema version (atomic write + fsync)
            3. Persists updated schema to registry
            4. Returns (new_version, message)

        Returns:
            (new_schema_version, message) - version string for manifest

        Raises:
            SchemaError: If drift is breaking (removed/changed)
        """
        pass

    def _atomic_write(self, path: Path, data: dict) -> None:
        """Atomic write with temp+fsync pattern."""
        pass
```

### 5. Exceptions

```python
from libs.common.exceptions import DataQualityError

class SyncValidationError(DataQualityError):
    """Raised when sync validation fails."""

    def __init__(self, errors: list[ValidationError], message: str = ""):
        self.errors = errors
        super().__init__(message or f"Validation failed: {len(errors)} error(s)")

class SchemaError(DataQualityError):
    """Raised when schema drift is breaking (removed/changed columns)."""

    def __init__(self, drift: SchemaDrift, message: str = ""):
        self.drift = drift
        super().__init__(message or f"Breaking schema drift: {drift}")

class ChecksumMismatchError(DataQualityError):
    """Raised when file checksum doesn't match expected."""

    def __init__(self, file_path: str, expected: str, actual: str):
        self.file_path = file_path
        self.expected = expected
        self.actual = actual
        super().__init__(f"Checksum mismatch for {file_path}: expected {expected}, got {actual}")

class QuarantineError(DataQualityError):
    """Raised when quarantine operation fails."""
    pass

class LockNotHeldError(DataQualityError):
    """Raised when required lock is not held."""
    pass

class DiskSpaceError(DataQualityError):
    """Raised when disk space is insufficient."""
    pass
```

---

## Test Coverage

### test_types.py
- [ ] LockToken serialization to_dict
- [ ] LockToken deserialization from_dict
- [ ] LockToken is_expired returns True when past expires_at
- [ ] LockToken is_expired returns False when before expires_at
- [ ] LockToken round-trip through JSON

### test_manifest.py
- [ ] SyncManifest serialization/deserialization
- [ ] SyncManifest invariants: UTC timestamp with offset == 0 (pass)
- [ ] SyncManifest invariants: non-UTC timezone rejected (e.g., EST, PST)
- [ ] SyncManifest invariants: naive timestamp rejected
- [ ] SyncManifest invariants: start_date <= end_date
- [ ] SyncManifest invariants: file_paths non-empty
- [ ] ManifestManager atomic write (temp + rename + fsync)
- [ ] ManifestManager assert_lock_held success
- [ ] ManifestManager assert_lock_held fails with expired lock
- [ ] ManifestManager assert_lock_held fails with mismatched token
- [ ] ManifestManager assert_lock_held split-brain scenario
- [ ] ManifestManager assert_lock_held stale lock detection
- [ ] ManifestManager check_disk_space 80% warning (proceeds with warning log)
- [ ] ManifestManager check_disk_space 90% critical (proceeds with alert)
- [ ] ManifestManager check_disk_space 95% blocked (raises DiskSpaceError)
- [ ] ManifestManager rollback_on_failure restores previous
- [ ] ManifestManager rollback_on_failure verifies previous_checksum matches backup
- [ ] ManifestManager rollback_on_failure returns None when no previous version
- [ ] ManifestManager save fails without lock (LockNotHeldError)
- [ ] ManifestManager save updates manifest_version increment
- [ ] ManifestManager save updates previous_checksum from current
- [ ] ManifestManager ENOSPC triggers quarantine
- [ ] ManifestManager quarantine_data moves to correct path
- [ ] ManifestManager quarantine_data updates validation_status
- [ ] ManifestManager list_manifests returns ordered by dataset name
- [ ] ManifestManager create_snapshot creates immutable copy
- [ ] ManifestManager create_snapshot fails on duplicate version_tag

### test_validation.py
- [ ] Row count validation within tolerance (pass)
- [ ] Row count validation exceeds tolerance (fail)
- [ ] Null percentage detection below threshold (pass)
- [ ] Null percentage detection above threshold (fail)
- [ ] SHA-256 checksum computation single file
- [ ] SHA-256 checksum verification success
- [ ] SHA-256 checksum verification failure
- [ ] Aggregate checksum computation multiple files
- [ ] Aggregate checksum deterministic ordering (sorted paths)
- [ ] Aggregate checksum verification success
- [ ] Aggregate checksum verification failure (single file changed)
- [ ] Schema validation matches expected
- [ ] Schema validation missing columns
- [ ] Schema validation extra columns
- [ ] Schema validation dtype mapping (string -> polars type)
- [ ] Schema validation unknown dtype raises error
- [ ] Schema validation case-insensitive dtype matching
- [ ] Date continuity no gaps (pass)
- [ ] Date continuity with gaps (fail)
- [ ] Date continuity excludes holidays
- [ ] Anomaly detection: row count drop (> 10%)
- [ ] Anomaly detection: null spike (> 5% increase)
- [ ] Anomaly detection: missing date ranges
- [ ] Anomaly detection: combined anomalies (multiple issues)
- [ ] Validation failure prevents manifest update

### test_schema.py
- [ ] Schema registry get_expected_schema success
- [ ] Schema registry get_expected_schema not found
- [ ] Schema registry get_expected_schema specific version
- [ ] Schema drift detection: no drift
- [ ] Schema drift detection: new columns (warning)
- [ ] Schema drift detection: removed columns (error)
- [ ] Schema drift detection: type changes (error)
- [ ] Schema drift policy: additions-only triggers auto-version bump
- [ ] Schema drift policy: additions-only persists new schema (atomic write + fsync)
- [ ] Schema drift policy: returns new version string to caller
- [ ] Schema drift policy: version format is "v{major}.{minor}.{patch}"
- [ ] Schema drift policy: minor version increments on new columns
- [ ] Schema drift policy: reject breaking changes (SchemaError)
- [ ] Schema registry atomic write
- [ ] Schema registry version increment persisted after additive drift
- [ ] Schema registry concurrent writer with locking
- [ ] Schema registry concurrent writer conflict detection

---

## ADR-0019 Topics

1. **Checksum Algorithm**: SHA-256 chosen over MD5 for collision resistance
2. **Schema Drift Policy**: Accept new (warning), reject removed/changed
3. **Lock+fsync Coupling**: Manifest writes require exclusive lock
4. **Disk-full Procedures**: 2x space check, quarantine on ENOSPC
5. **Reader Cache Invalidation**: DuckDB PRAGMA disable_object_cache guidance
6. **Anomaly Detection Thresholds**: >10% row drop, >5% null increase

---

## Dependencies

- **Pydantic**: Data validation and serialization
- **Polars**: DataFrame operations
- **hashlib**: SHA-256 checksums
- **libs.common.exceptions**: DataQualityError base class

---

## Review Log

### Review 1: Gemini + Codex (2025-12-03)
- Gemini: APPROVED
- Codex: CHANGES_REQUESTED
  - Add schema.py file
  - Switch to SHA-256
  - Lock+fsync coupling
  - Disk-full handling

### Review 2: Gemini + Codex (2025-12-03)
- Gemini: APPROVED
- Codex: CHANGES_REQUESTED
  - ADR number conflict (use 0019)
  - Schema registry atomic writes
  - Disk-space policy alignment
  - SyncManifest invariants
  - Lock assertion mechanics

### Review 3: Gemini + Codex (2025-12-03)
- Gemini: APPROVED
  - Multi-file checksums note (aggregate hash)
- Codex: CHANGES_REQUESTED
  - Tighten UTC enforcement (offset == 0)
  - Add 90% disk threshold test
  - Clarify auto-version bump for additive drift

### Review 4: Gemini + Codex (2025-12-03)
- Gemini: APPROVED
- Codex: APPROVED (from previous session)

### Review 5: Gemini + Codex (2025-12-03)
- Gemini: APPROVED
  - Comprehensive, correct, architecturally sound
  - Note: LockNotHeldError/DiskSpaceError could fit broader hierarchy
- Codex: CHANGES_REQUESTED
  - LockToken type and lifecycle undefined
  - save_manifest missing expected_bytes parameter
  - Multi-file checksum aggregation needed
  - Schema versioning format/locking undefined
  - TradingCalendar/LockToken import paths
  - Polars dtype mapping policy
  - Quarantine mechanics unclear
  - Additional test coverage gaps

### Review 5 Fixes Applied:
- Added types.py with LockToken dataclass and lifecycle documentation
- Added TradingCalendar Protocol
- Updated save_manifest signature with expected_bytes parameter
- Added compute_aggregate_checksum and verify_aggregate_checksum methods
- Added SchemaRegistry versioning format "v{major}.{minor}.{patch}"
- Added SchemaRegistry locking documentation
- Added DTYPE_MAP for polars type mapping
- Added quarantine_data method and rollback mechanics documentation
- Added test_types.py test coverage
- Expanded test coverage for all identified gaps

### Review 6: Codex (2025-12-03)
- Codex: CHANGES_REQUESTED
  - DTYPE_MAP values were strings, should be actual pl.DataType objects
  - TradingCalendar import path was ambiguous

### Review 6 Fixes Applied:
- Fixed DTYPE_MAP to use actual polars DataType objects (pl.Int64, pl.Utf8, etc.)
- Added datetime variants: datetime[us], datetime[ns], datetime[ms]
- Specified concrete TradingCalendar implementation: exchange_calendars library
- Added ExchangeCalendarAdapter helper class
- Documented test mock: libs.data_quality.testing.MockTradingCalendar

### Review 7: Gemini + Codex (2025-12-03)
- Gemini: APPROVED (carried from Review 5)
- Codex: APPROVED
  - All issues resolved
  - Ready to proceed with implementation

---

**Version:** 1.5 (Plan Iteration 7) - FINAL
