# P4T4-T5.1: Backtest Job Queue Infrastructure

**Parent Task:** [P4T4_TASK.md](./P4T4_TASK.md)
**Task ID:** P4T4-T5.1

### T5.1: Backtest Job Queue Infrastructure

**Effort:** 3-4 days | **PR:** `feat(p4): backtest job queue`
**Status:** ⏳ Pending
**Priority:** P0 (Foundation - prevents UI blocking)

**Problem:** Long-running backtests block the web process and can be killed mid-run.

**CRITICAL:** Use Redis + RQ only. **DO NOT use file-based queue** (prone to race conditions, locking issues under load).

**Deliverables:**
- Redis-based job queue using RQ (simpler than Celery for single-worker)
- Background worker with progress tracking via Redis keys
- Job prioritization (high/normal/low queues with separate workers)
- Resource limits (max concurrent jobs, memory limits, worker topology)
- Idempotent job execution via job_id hash (with safe NoSuchJobError handling)
- Job timeout handling (configurable, default 1 hour)
- **T5.1a (0.5d, prerequisite step inside same PR/commit):** Modify `libs/alpha/research_platform.py` to extend `PITBacktester.run_backtest` with `progress_callback` and `cancel_check` parameters **before wiring the worker**.
  **CRITICAL: Within the same PR/commit, T5.1a must be implemented and tested first (callbacks added + tests passing) before T5.1b wiring is considered complete.**
  Target signature:
  `def run_backtest(self, alpha, start_date: date, end_date: date, snapshot_id: str | None, weight_method: str, progress_callback: Callable[[int, Optional[date]], None] | None = None, cancel_check: Callable[[], None] | None = None) -> BacktestResult:`
  **T5.1a Acceptance Criteria:**
  - [ ] `PITBacktester.run_backtest` accepts `progress_callback` and `cancel_check` parameters (type-checked)
  - [ ] `progress_callback` is invoked at least every 30 seconds during backtest execution (unit test with mock)
  - [ ] `cancel_check` is invoked at least every 30 seconds; raises `JobCancelled` when flag is set (unit test)
  - [ ] Existing tests pass with callbacks set to `None` (backward compatible)
  - [ ] T5.1a portion is verified (tests passing) before marking T5.1b complete, even if same PR

  **T5.1a Plan Review Additions (Gemini + Codex, 2025-12-12):**
  - [ ] **Callback coverage in helper loops:** Callbacks must also wrap long-running helper loops (`_compute_daily_ic`, `_compute_horizon_returns`, decay loop) to guarantee ≤30s cadence even on large datasets
  - [ ] **Progress emit 0 and 100:** Emit initial progress (0) before compute starts and final progress (100 or last pct) on ALL exits including early stop and cancellation
  - [ ] **Progress granularity:** Calculate progress using `(processed_days / total_days) * 100` for smooth updates within batches (not coarse batch-level)
  - [ ] **Cleanup on exception:** Wrap `run_backtest` main body in `try/finally` to clear caches (`_snapshot`, `_prices_cache`, `_fundamentals_cache`) when `JobCancelled` or other exceptions propagate
  - [ ] **Time tracking:** Use `time.monotonic()` for robust elapsed time calculation in throttling logic
  - [ ] **JobCancelled placement:** Define `JobCancelled` in `libs/alpha/exceptions.py` (shared module) and export from `libs/alpha/__init__.py`
  - [ ] **Test with fake time:** Tests should use monkeypatch on `time.monotonic` to verify throttling and ≥1 callback per 30s window
  - [ ] **Regression test:** Verify `snapshot_id` and `dataset_version_ids` remain populated after callbacks execute
- **T5.1b (3d, depends on T5.1a completion):** Wire BacktestWorker/RQ enqueueing to the extended PITBacktester callbacks.
- **Blocker:** T5.1b work cannot be considered done until the T5.1a callbacks/tests are in place within the same PR/commit (sequencing enforced).

**Job Status Vocabulary Contract:**
The API uses unified status vocabulary across DB and RQ. Mapping:
| DB Status | RQ Status | Description |
|-----------|-----------|-------------|
| pending | queued | Job enqueued, not yet started |
| running | started | Worker picked up job |
| completed | finished | Job succeeded |
| failed | failed | Job raised exception |
| cancelled | canceled (queued kill) / finished (cooperative return) | Job cancelled (queued jobs use `Job.cancel()` → `canceled`; running jobs return success payload with `cancelled: true`, which RQ marks `finished`) |
| failed | deferred | RQ deferred = failed in DB (requeued by RQ, treat as failed until worker resumes) |
| failed | stopped | RQ stopped state maps to DB failed (manual worker stop) |
| failed (manual override) | queued→failed without run | Pending job missing worker but RQ failure hook fired; DB forces `failed` for visibility |

**Payload disambiguation contract (FORMAL):**
RQ marks cooperative cancellations as `finished`; always inspect `job.result` to disambiguate:
| RQ Status | Payload | DB Status | Notes |
|-----------|---------|-----------|-------|
| `finished` | `{"cancelled": True}` | `cancelled` | Cooperative cancellation |
| `finished` | `{"cancelled": False}` or missing key | `completed` | Normal success |
| `finished` | `None` (payload missing) | `failed` | Error: set `error_message="RQ job finished but payload missing"` |
Helper function `_resolve_rq_finished_status(job)` encapsulates this logic:

```python
def _resolve_rq_finished_status(job: Job) -> tuple[str, str | None]:
    """
    Resolve DB status from RQ finished job (success vs cancelled vs error).

    Returns: (db_status, error_message)
    - ("completed", None) for normal success
    - ("cancelled", None) for cooperative cancellation
    - ("failed", "RQ job finished but payload missing") for missing payload
    - ("unknown", None) if job not in finished state (caller should handle)
    """
    if job.get_status() != "finished":
        return ("unknown", None)

    result = job.result
    if result is None:
        return ("failed", "RQ job finished but payload missing")

    if isinstance(result, dict) and result.get("cancelled") is True:
        return ("cancelled", None)

    return ("completed", None)
```

Note: RQ's internal spelling is `canceled` (one "l"). DB continues to use `cancelled`; do not compare DB values to RQ values directly—DB is the source of truth.

CRITICAL: Use DB status as source of truth. RQ 'finished' may be success OR cancellation.

**DB Status Transition Contract:**
```
pending  ─────────────────────────────> cancelled  (queued job cancelled)
    │                                        ▲
    │ worker starts                          │ cooperative cancel
    ▼                                        │
running ────────────────────────────────────┘
    │
    ├──────> completed  (success, sets completed_at)
    │
    └──────> failed     (exception, sets error_message)
```

| Transition | Trigger | Timestamp Set | Notes |
|------------|---------|---------------|-------|
| pending→running | Worker starts | started_at | Worker calls update_db_status |
| pending→cancelled | cancel_job() for queued | completed_at | Immediate, in cancel_job |
| running→completed | Worker finishes | completed_at | Worker calls update_db_status |
| running→failed | Exception raised | completed_at | Worker catches, sets error_message |
| running→cancelled | JobCancelled | completed_at | Worker catches, updates status |

All terminal transitions (completed/failed/cancelled) must set `completed_at` for auditability and retention cleanup.

**Retry Semantics:**
- Automatic retries only: `retry_count` increments inside the RQ retry hook using `func.coalesce(BacktestJob.retry_count, 0) + 1` (no enqueue-side increments).
- User-triggered re-runs (after any terminal state) must pass `is_rerun=True` when recreating the DB row so `retry_count` resets to 0; user reruns should never inherit automated retry counts.
- Heal operations that recreate missing RQ jobs **preserve** `retry_count` (unlike user reruns) to maintain accurate retry history; heals log a `heal_count` metric for observability and are tracked separately from RQ retries. To prevent infinite re-enqueue loops, the watchdog marks jobs as failed after 3 consecutive heals within 1 hour (tracked via Redis key `backtest:heal_count:{job_id}` with 1h TTL).
- If DB row says `pending`/`running` but RQ job is missing, enqueue heals by recreating the RQ job instead of returning `None`; this counts as an automated retry.
- After 3 automatic retries, job moves to dead-letter queue (`backtest_failed`).

**Progress Tracking Contract:**
- Worker writes progress to Redis key `backtest:progress:{job_id}` (not pub/sub - simple polling)
- Updates emitted every ≤30 seconds during execution
- Progress stages: 0% (started), 10% (loading_data), 20-90% (computing for PITBacktester mapping only), 92% (saving_parquet), 95% (saving_db), 100% (completed)
- Progress payload: `{"pct": int, "stage": str, "current_date": str, "updated_at": iso_timestamp}`
- Key TTL: Dynamic, refreshed on each update. Base TTL = `max(job_timeout, 3600)` seconds
- Redis writes must use a pipeline (`set` + `expire`) to make TTL update atomic and avoid races
- For long jobs (>1h), worker refreshes TTL on each progress update to prevent expiry
- TTL refreshes (progress, heartbeat, cancel flags) must all use the same pipeline pattern; avoid `exists()+expire()` races.
- **DB Progress Sync:** Sync to Postgres every 10% (pct % 10 == 0) to keep coarse progress even if Redis expires. This is a **coarse fallback only**; fine-grained progress lives in Redis. (Note: 90 and 100 are already divisible by 10, so no separate >=90 clause needed.)
- **Redis namespace convention:** all keys are prefixed `backtest:*` (e.g., `backtest:progress:{job_id}`, `backtest:cancel:{job_id}`) to avoid collisions.

**Cancellation Contract:**
- Queued jobs: Immediate cancellation via `job.cancel()`
- Running jobs: Cooperative interruption via Redis flag `backtest:cancel:{job_id}`
- Cancel flag TTL: Dynamic, refreshed to `max(job_timeout, 3600)` on each progress update
- Periodic cancellation checker also refreshes cancel-flag TTL to avoid expiry during long compute loops.
- Worker checks cancel flag: (1) every progress update (≤30s), AND (2) every 10s via periodic check for long compute loops
- Periodic check via `check_cancellation_periodic(job_id, job_timeout)` called from PITBacktester's `cancel_check` callback
- **PITBacktester Contract:** MUST call `cancel_check` callback at least every 30s during compute loops
- **Implementation Note:** T5.1 must extend PITBacktester to add `cancel_check` and `progress_callback` parameters; these callbacks are mandatory in the worker wiring below.
- Acceptance: Cancelled jobs transition to "cancelled" status within 30 seconds (cooperative, not instant)
- Progress is preserved on cancellation (no reset to 0); UI shows the last emitted percentage.

**Resource Limits Design:**
- **Max concurrent jobs:** 2 per queue (configurable via `BACKTEST_MAX_WORKERS`)
- **Memory limit:** Worker process monitored via `psutil`; job killed if RSS > 4GB
- **Worker topology:** 2 workers per priority queue (replicas=2 in docker-compose) → max 2 concurrent jobs per queue, 6 total
- **Starvation prevention:** High-priority queue processed first, but workers cycle to prevent total low-priority starvation
- **Dead-letter queue:** Jobs failing 3x moved to `backtest_failed` queue for inspection

**Serialization:**
- `BacktestJobConfig` uses `to_dict()`/`from_dict()` for JSON serialization (no pickle)

**API Contracts (Web UI → Job Queue):**

The Web UI interacts with the job queue via Python function calls (NOT REST). These contracts define the interface:

| Operation | Function | Input | Output | Error Handling |
|-----------|----------|-------|--------|----------------|
| **Enqueue** | `BacktestJobQueue.enqueue(config, priority, created_by, timeout, is_rerun)` | `BacktestJobConfig`, `JobPriority`, `str`, `int`, `bool` | `rq.Job` | `ValueError` for invalid timeout; `RuntimeError` for lock contention |
| **Get Status** | `BacktestJobQueue.get_job_status(job_id)` | `str` | `dict` with status, progress, timestamps | Returns `{"status": "not_found"}` if missing |
| **Cancel** | `BacktestJobQueue.cancel_job(job_id, job_timeout)` | `str`, `int|None` | `bool` (success) | Returns `False` if job not found or not cancellable |
| **List Jobs** | `BacktestResultStorage.list_jobs(created_by, alpha_name, status, limit, offset)` | Filter params | `list[dict]` | Empty list if no matches |
| **Get Result** | `BacktestResultStorage.get_result(job_id)` | `str` | `BacktestResult|None` | `ValueError` if Parquet/summary.json corrupt |

**Response Payload Contracts:**

```python
# get_job_status response
{
    "job_id": str,
    "status": Literal["pending", "running", "completed", "failed", "cancelled", "not_found"],
    "progress_pct": int,  # 0-100
    "progress_stage": str,  # "started", "loading_data", "computing", etc.
    "progress_date": str | None,  # Current date being processed (ISO format)
    "progress_updated_at": str | None,  # Last progress update (ISO timestamp)
    "created_at": str | None,
    "started_at": str | None,
    "completed_at": str | None,
    "error_message": str | None,
    "result_path": str | None,  # Local path to result directory
}

# list_jobs response item
{
    "job_id": str,
    "status": str,
    "alpha_name": str,
    "start_date": str,  # ISO date
    "end_date": str,    # ISO date
    "created_by": str,
    "created_at": str,  # ISO timestamp
    "mean_ic": float | None,
    "icir": float | None,
}
```

**result_path Contract:**
- Path format: `data/backtest_results/{job_id}/` (relative path from project root)
- Contains: `daily_signals.parquet`, `daily_weights.parquet`, `daily_ic.parquet`, `summary.json`
- UI loads Parquet files directly from this path via `polars.read_parquet()`

**Watchdog Process Contract:**

The watchdog runs as a **periodic task within the Streamlit web_console process** (NOT a separate service):
- **Location:** `apps/web_console/tasks/watchdog.py`
- **Trigger:** Called by Streamlit's background scheduler every 60 seconds
- **Function:** `BacktestJobQueue.watchdog_fail_lost_jobs()` marks running jobs as failed if heartbeat expired
- **Required Env Vars:** `REDIS_URL`, `DATABASE_URL` (same as web_console)
- **Threshold:** Job marked failed if heartbeat older than `max(job_timeout, 3600)` seconds
- **Alternative Deployment:** For production with multiple web_console replicas, run watchdog as a separate cron job or Kubernetes CronJob to avoid duplicate execution

**PITBacktester Dependency Contract:**
Worker initialization requires the following components (per `libs/alpha/research_platform.py`):
| Dependency | Class | Purpose |
|------------|-------|---------|
| `version_manager` | `DatasetVersionManager` | Snapshot management, PIT data access |
| `crsp_provider` | `CRSPLocalProvider` | Price/return data (snapshot-locked) |
| `compustat_provider` | `CompustatLocalProvider` | Fundamental data (snapshot-locked) |
| `metrics_adapter` | `AlphaMetricsAdapter` | IC/ICIR computation (optional, auto-created) |

**PITBacktester contract (updated):**
- `run_backtest(...) -> BacktestResult` (deterministic given snapshot + seed); thread-safe for concurrent reads because providers are read-only, but avoid sharing mutable state across threads when injecting custom adapters.
- Raises: `JobCancelled` (propagated to worker as cooperative cancellation), `ValueError` for invalid config/date ranges, provider-specific exceptions bubble up and are recorded in `error_message`.
- Caller must pass `progress_callback` and `cancel_check`; progress callback may be called from worker thread only and must remain side-effect-free.
- **Reproducibility fields lifecycle:** BacktestResult's `snapshot_id: str` and `dataset_version_ids: dict[str, str]` are **required, non-optional fields** in the dataclass (not `Optional`). PITBacktester MUST populate them before returning. The worker validates these fields via direct attribute access (`result.snapshot_id`, not `getattr` fallbacks) and raises `ValueError` if `None`. Summary metrics (IC/ICIR/coverage/turnover/etc.) are optional/nullable.

T5.1 Extension Required:
- Add `progress_callback: Callable[[int, Optional[date]], None] | None = None` parameter (import `Optional` or enable `from __future__ import annotations`)
- Add `cancel_check: Callable[[], None] | None = None` parameter
- Call `cancel_check()` at least every 30s during compute loops
- Call `progress_callback(pct, current_date)` for progress updates

**Implementation:**
```python
# libs/backtest/job_queue.py
from dataclasses import dataclass, field
from datetime import datetime, date, UTC
from enum import Enum
from typing import Any
import hashlib
import json
import os
import structlog

from psycopg.rows import dict_row
from psycopg_pool import ConnectionPool
from redis import Redis
from rq import Queue, Retry
from rq.job import Job, NoSuchJobError

class JobPriority(Enum):
    HIGH = "high"
    NORMAL = "normal"
    LOW = "low"

@dataclass
class BacktestJobConfig:
    """Configuration for a backtest job."""
    alpha_name: str
    start_date: date
    end_date: date
    weight_method: str = "zscore"
    extra_params: dict[str, Any] = field(default_factory=dict)

    def compute_job_id(self, created_by: str) -> str:
        content = json.dumps(
            {
                "alpha": self.alpha_name,
                "start": str(self.start_date),
                "end": str(self.end_date),
                "weight": self.weight_method,
                "params": self.extra_params,
                "created_by": created_by,
            },
            sort_keys=True,
        )
        return hashlib.sha256(content.encode()).hexdigest()[:32]

    def to_dict(self) -> dict[str, Any]:
        return {
            "alpha_name": self.alpha_name,
            "start_date": str(self.start_date),
            "end_date": str(self.end_date),
            "weight_method": self.weight_method,
            "extra_params": self.extra_params,
        }

    @classmethod
    def from_dict(cls, data: dict[str, Any]) -> "BacktestJobConfig":
        return cls(
            alpha_name=data["alpha_name"],
            start_date=date.fromisoformat(data["start_date"]),
            end_date=date.fromisoformat(data["end_date"]),
            weight_method=data.get("weight_method", "zscore"),
            extra_params=data.get("extra_params", {}),
        )

class BacktestJobQueue:
    """Redis-based job queue for backtests."""

    DEFAULT_TIMEOUT = 3600  # 1 hour
    MAX_RETRIES = 3

    def __init__(
        self,
        redis_client: Redis,
        db_pool: ConnectionPool,
        default_queue: str = "backtest_normal",
    ):
        self.redis = redis_client
        self.db_pool = db_pool
        self.queues = {
            JobPriority.HIGH: Queue("backtest_high", connection=redis_client),
            JobPriority.NORMAL: Queue("backtest_normal", connection=redis_client),
            JobPriority.LOW: Queue("backtest_low", connection=redis_client),
        }
        self.default_queue = self.queues[JobPriority.NORMAL]
        self.logger = structlog.get_logger(__name__)

    def _safe_fetch_job(self, job_id: str) -> Job | None:
        try:
            return Job.fetch(job_id, connection=self.redis)
        except NoSuchJobError:
            return None

    def _fetch_db_job(self, job_id: str) -> dict[str, Any] | None:
        sql = "SELECT * FROM backtest_jobs WHERE job_id = %s"
        with self.db_pool.connection() as conn, conn.cursor(row_factory=dict_row) as cur:
            cur.execute(sql, (job_id,))
            return cur.fetchone()

    def _create_db_job(
        self,
        job_id: str,
        config: BacktestJobConfig,
        created_by: str,
        job_timeout: int,
        *,
        is_rerun: bool = False,
    ) -> None:
        # Schema reference: See P4T4_TASK.md "Result Storage Schema" for authoritative DDL.
        # Column list below must stay synchronized with the parent schema definition.
        upsert_sql = """
        INSERT INTO backtest_jobs (
            job_id, status, alpha_name, start_date, end_date, weight_method,
            config_json, created_by, retry_count, progress_pct, job_timeout
        ) VALUES (%(job_id)s, 'pending', %(alpha)s, %(start)s, %(end)s, %(weight)s,
                  %(config)s, %(created_by)s, 0, 0, %(timeout)s)
        ON CONFLICT (job_id) DO UPDATE SET
            status = 'pending',
            retry_count = CASE WHEN %(is_rerun)s THEN 0 ELSE COALESCE(backtest_jobs.retry_count,0) END,
            started_at = NULL,
            completed_at = NULL,
            worker_id = NULL,
            progress_pct = 0,
            error_message = NULL,
            job_timeout = %(timeout)s,
            result_path = NULL,
            mean_ic = NULL,
            icir = NULL,
            hit_rate = NULL,
            coverage = NULL,
            long_short_spread = NULL,
            average_turnover = NULL,
            decay_half_life = NULL,
            snapshot_id = NULL,
            dataset_version_ids = NULL;
        """

        params = {
            "job_id": job_id,
            "alpha": config.alpha_name,
            "start": config.start_date,
            "end": config.end_date,
            "weight": config.weight_method,
            "config": json.dumps(config.to_dict()),
            "created_by": created_by,
            "timeout": job_timeout,
            "is_rerun": is_rerun,
        }

        with self.db_pool.connection() as conn:
            with conn.cursor() as cur:
                cur.execute(upsert_sql, params)
                conn.commit()

    def enqueue(
        self,
        config: BacktestJobConfig,
        priority: JobPriority = JobPriority.NORMAL,
        created_by: str = "system",
        timeout: int | None = None,
        *,
        is_rerun: bool = False,  # user-triggered rerun resets retry_count
    ) -> Job:
        """
        Enqueue a backtest job.

        CRITICAL: Creates BacktestJob DB row BEFORE enqueueing to Redis.
        Returns existing job if same config+user already queued/running (idempotent).
        TOCTOU mitigation: guard enqueue with a short-lived Redis `SETNX backtest:lock:{job_id}` (or rely on the DB unique index on job_id); if the lock fails, return the existing job to avoid double-enqueue.
        Timeout guardrails: validate `job_timeout` within [300, 14400] seconds to avoid starving workers or running forever.

        Re-enqueue Policy:
        - If job is queued/started: return existing (no-op)
        - If job is finished/failed: delete from RQ, reset DB, create new RQ job
        - If DB row is pending/running but RQ job is missing: heal by recreating the RQ job (never return None)
        - User-triggered reruns must call enqueue(..., is_rerun=True) to reset retry_count to 0 (automatic retries handled by RQ retry hook)
        - Healing must NOT bump retry_count; only the RQ retry hook increments it to avoid double-counting.
        """
        from libs.backtest.models import BacktestJob

        job_id = config.compute_job_id(created_by)  # Include user in hash

        job_timeout = timeout or self.DEFAULT_TIMEOUT
        if not 300 <= job_timeout <= 14_400:
            raise ValueError("timeout must be between 300 and 14,400 seconds")

        lock_key = f"backtest:lock:{job_id}"
        if not self.redis.set(lock_key, "1", nx=True, ex=10):
            # Another enqueue in-flight; return existing job if present
            existing = self._safe_fetch_job(job_id)
            if existing:
                return existing
            # Lock contention but no RQ job—retry with backoff before failing
            import time
            time.sleep(0.1)  # 100ms backoff
            if not self.redis.set(lock_key, "1", nx=True, ex=10):
                raise RuntimeError("enqueue lock contention after retry; another enqueue in progress")

        try:
            # DB status is source of truth for idempotency (pending/running = active)
            db_job = self._fetch_db_job(job_id)
            if db_job and db_job["status"] in ("pending", "running"):
                existing = self._safe_fetch_job(job_id)
                if existing:
                    return existing
                # DB says active but RQ job missing → recreate RQ job deterministically
                # Track heal count in Redis to prevent infinite re-enqueue loops (max 3 per hour)
                heal_key = f"backtest:heal_count:{job_id}"
                heal_count = int(self.redis.get(heal_key) or 0)
                if heal_count >= 3:
                    # Too many heals → fail the job instead of looping forever
                    with self.db_pool.connection() as conn, conn.cursor() as cur:
                        cur.execute(
                            """
                            UPDATE backtest_jobs
                            SET status = 'failed',
                                error_message = %s,
                                completed_at = %s
                            WHERE job_id = %s
                            """,
                            (
                                f"Job healed {heal_count} times in 1h; marked failed to prevent infinite loop",
                                datetime.now(UTC),
                                job_id,
                            ),
                        )
                        conn.commit()
                    self.logger.error("heal_loop_breaker", job_id=job_id, heal_count=heal_count)
                    raise RuntimeError(f"Job {job_id} exceeded max heal attempts")
                # Dynamic TTL: use job's configured timeout (min 1h) to align with job lifecycle
                heal_ttl = max(job_timeout, 3600)
                self.redis.setex(heal_key, heal_ttl, str(heal_count + 1))
                self._create_db_job(job_id, config, created_by, job_timeout, is_rerun=True)  # Reset retry_count like user rerun
                queue = self.queues[priority]
                healed_job = queue.enqueue(
                    "libs.backtest.worker.run_backtest",
                    kwargs={
                        "config": config.to_dict(),
                        "created_by": created_by,
                    },
                    job_id=job_id,
                    job_timeout=job_timeout,
                    retry=Retry(max=self.MAX_RETRIES, interval=[60, 300, 900]),
                    result_ttl=86400 * 7,
                    failure_ttl=86400 * 30,
                )
                self.logger.info("healed_missing_rq_job", job_id=job_id, heal_count=heal_count + 1)
                return healed_job

            # Check for existing job (idempotency) - safe lookup in RQ
            existing = self._safe_fetch_job(job_id)
            if existing:
                status = existing.get_status()
                if status in ("queued", "started"):
                    return existing
                else:
                    existing.delete()

            # Create/reset DB row (worker will update status)
            self._create_db_job(job_id, config, created_by, job_timeout, is_rerun=is_rerun)

            queue = self.queues[priority]
            job = queue.enqueue(
                "libs.backtest.worker.run_backtest",
                kwargs={
                    "config": config.to_dict(),
                    "created_by": created_by,
                },
                job_id=job_id,
                job_timeout=job_timeout,
                retry=Retry(max=self.MAX_RETRIES, interval=[60, 300, 900]),
                result_ttl=86400 * 7,  # Keep results 7 days
                failure_ttl=86400 * 30,  # Keep failed job info 30 days
            )
            return job
        finally:
            self.redis.delete(lock_key)

    def get_job_status(self, job_id: str) -> dict[str, Any]:
        """
        Get job status with progress information.

        CRITICAL: Uses DB status as source of truth (not RQ status).
        Falls back to RQ status only if DB row not found.
        """
        # DB is source of truth
        db_job = self._fetch_db_job(job_id)
        rq_job = self._safe_fetch_job(job_id)
        if not db_job:
            # Fallback to RQ if DB row missing (shouldn't happen in normal flow)
            if not rq_job:
                return {"status": "not_found"}
            payload = rq_job.result if isinstance(rq_job.result, dict) else {}
            cancelled = payload.get("cancelled") is True if isinstance(payload, dict) else False
            effective_status = "cancelled" if cancelled else "unknown"
            return {
                "job_id": job_id,
                "status": effective_status,
                "rq_status": rq_job.get_status(),
                "warning": "DB row missing; derived from RQ payload",
            }

        # RQ marks cooperative cancellations as "finished"; inspect payload to disambiguate
        rq_payload = rq_job.result if rq_job and isinstance(rq_job.result, dict) else {}
        rq_cancelled = rq_payload.get("cancelled") is True if isinstance(rq_payload, dict) else False

        # Get progress from Redis (JSON payload set by worker)
        # Falls back to DB progress_pct if Redis key expired (synced every 10% and >=90%)
        progress_raw = self.redis.get(f"backtest:progress:{job_id}")
        if progress_raw:
            progress = json.loads(progress_raw)
        else:
            # Redis expired - use DB progress as fallback (preserve last-known pct even when cancelled)
            fallback_pct = db_job.get("progress_pct") or 0
            progress = {"pct": fallback_pct, "stage": db_job["status"]}

        return {
            "job_id": job_id,
            "status": "cancelled" if rq_cancelled else db_job["status"],  # DB status as source of truth with RQ payload override for cooperative cancel
            "progress_pct": progress.get("pct", db_job.get("progress_pct") or 0),
            "progress_stage": progress.get("stage", db_job["status"]),
            "progress_date": progress.get("current_date"),
            "progress_updated_at": progress.get("updated_at"),
            "created_at": db_job["created_at"].isoformat() if db_job.get("created_at") else None,
            "started_at": db_job["started_at"].isoformat() if db_job.get("started_at") else None,
            "completed_at": db_job["completed_at"].isoformat() if db_job.get("completed_at") else None,
            "error_message": db_job.get("error_message"),
            "result_path": db_job.get("result_path"),
        }

    def cancel_job(self, job_id: str, job_timeout: int | None = None) -> bool:
        """
        Cancel a queued or running job.

        CRITICAL: Updates DB status immediately for queued jobs.
        For running jobs, sets cancel flag and lets worker update DB.

        TOCTOU Race Window Note:
        There's a small race window between checking RQ status and updating DB.
        If a job transitions from queued→started between the status check and
        the DB update, the DB update may not execute (status already "running").
        This is acceptable because:
        1. The cooperative cancel flag will still be set for running jobs
        2. Worker will pick up the flag within 30s via progress/cancel checks
        3. Eventual consistency is sufficient for cancellation (not instant)

        For stricter consistency (if needed), use `SELECT ... FOR UPDATE SKIP LOCKED` with psycopg.
        """

        job = self._safe_fetch_job(job_id)
        db_job = self._fetch_db_job(job_id)

        # Orphan handling: if RQ job is gone but DB says active, mark cancelled to avoid stuck UI
        if not job:
            if db_job and db_job["status"] in ("pending", "running"):
                with self.db_pool.connection() as conn, conn.cursor() as cur:
                    cur.execute(
                        "UPDATE backtest_jobs SET status='cancelled', completed_at=%s WHERE job_id=%s",
                        (datetime.now(UTC), job_id),
                    )
                    conn.commit()
                self.logger.info("cancel_orphan_db_only", job_id=job_id, status="cancelled")
                return True
            return False

        status = job.get_status()
        if status == "queued":
            # Queued job: immediate cancellation + DB update
            job.cancel()
            # Update DB status immediately (worker won't run)
            # NOTE: If job transitioned to started between status check and here,
            # this condition fails safely; cooperative flag handles running jobs.
            if db_job and db_job["status"] == "pending":
                with self.db_pool.connection() as conn, conn.cursor() as cur:
                    cur.execute(
                        "UPDATE backtest_jobs SET status='cancelled', completed_at=%s WHERE job_id=%s",
                        (datetime.now(UTC), job_id),
                    )
                    conn.commit()
            # Also set cancel flag in case of race (job may have started)
            self.redis.setex(f"backtest:cancel:{job_id}", 3600, "1")
            self.logger.info("cancelled_queued_job", job_id=job_id)
            return True
        elif status == "started":
            # Running job: set cooperative cancellation flag
            # Worker checks this flag every progress update (≤30s)
            # Dynamic TTL aligned with the job's timeout (DB/RQ-derived)
            effective_timeout = int(
                (job.timeout if job else None)
                or (db_job.get("job_timeout") if db_job else None)
                or (job_timeout if job_timeout is not None else None)
                or BacktestJobQueue.DEFAULT_TIMEOUT
                or 3600
            )
            ttl = max(effective_timeout, 3600)
            self.redis.setex(f"backtest:cancel:{job_id}", ttl, "1")
            self.logger.info("cancel_flag_set", job_id=job_id, ttl=ttl, status=status)
            return True
        return False

    def watchdog_fail_lost_jobs(self) -> int:
        """
        Mark running jobs as failed if their heartbeat expired (lost worker).

        Called by a periodic watchdog (cron/Streamlit background). Returns
        number of jobs marked failed.
        """
        now_ts = datetime.now(UTC).timestamp()
        with self.db_pool.connection() as conn, conn.cursor(row_factory=dict_row) as cur:
            cur.execute("SELECT * FROM backtest_jobs WHERE status = 'running'")
            running_jobs = cur.fetchall()
        failures = 0
        for job in running_jobs:
            threshold = now_ts - max(int(job["job_timeout"]), 3600)
            heartbeat_raw = self.redis.get(f"backtest:heartbeat:{job['job_id']}")
            try:
                heartbeat_ts = datetime.fromisoformat(heartbeat_raw.decode()).timestamp() if heartbeat_raw else None
            except (ValueError, UnicodeDecodeError):
                # Parse failure intent: treat corrupted heartbeat as "missing" → job will be failed.
                # This is fail-fast behavior: we don't retry parsing because a corrupted heartbeat
                # indicates a bug or manual tampering, and failing immediately is safer than
                # leaving the job in an indeterminate state.
                self.logger.warning("heartbeat_parse_failed", job_id=job["job_id"], raw=heartbeat_raw)
                heartbeat_ts = None  # Will trigger failure below
            if heartbeat_ts is None or heartbeat_ts < threshold:
                with self.db_pool.connection() as conn, conn.cursor() as cur:
                    cur.execute(
                        """
                        UPDATE backtest_jobs
                        SET status='failed',
                            error_message='Worker heartbeat lost; marked failed by watchdog',
                            completed_at=%s
                        WHERE job_id=%s
                        """,
                        (datetime.now(UTC), job["job_id"]),
                    )
                    conn.commit()
                failures += 1
        return failures
```

**Worker Entrypoint Contract:**

The `run_backtest` function is the RQ job entrypoint. Contract:
- **Input:** `config: dict` (serialized BacktestJobConfig), `created_by: str`
- **Output:** `dict` with `job_id`, `result_path`, `summary_metrics`
- **Side effects:** Writes progress to Redis, persists results to Postgres (sync) and Parquet
- **Errors:** Raises `JobCancelled`, `MemoryError`, or propagates PITBacktester exceptions

```python
# libs/backtest/worker.py
import os
import json
import time
import shutil
from datetime import datetime, date, UTC
from pathlib import Path
from typing import Any, Callable, Optional

import psutil
import structlog
from psycopg.rows import dict_row
from psycopg_pool import ConnectionPool
from redis import Redis
from rq import get_current_job

from libs.alpha.research_platform import PITBacktester, BacktestResult
from libs.backtest.job_queue import BacktestJobConfig, BacktestJobQueue

class JobCancelled(Exception):
    """Raised when job cancellation is requested."""
    pass

class BacktestWorker:
    """Worker with cooperative cancellation and memory monitoring."""

    MAX_RSS_BYTES = int(os.getenv("BACKTEST_JOB_MEMORY_LIMIT", 4 * 1024 * 1024 * 1024))  # 4GB default
    CANCEL_CHECK_INTERVAL = 10  # Check cancel flag every 10s even without progress

    def __init__(self, redis: Redis, db_pool: ConnectionPool):
        self.redis = redis
        self.db_pool = db_pool  # psycopg_pool connection pool
        self.process = psutil.Process()
        self._last_cancel_check = 0.0
        self.logger = structlog.get_logger(__name__)

    def check_cancellation(self, job_id: str) -> None:
        """Check if cancellation requested; raise if so."""
        if self.redis.exists(f"backtest:cancel:{job_id}"):
            # Clean up cancel flag
            self.redis.delete(f"backtest:cancel:{job_id}")
            raise JobCancelled(f"Job {job_id} cancelled by user")

    def check_cancellation_periodic(self, job_id: str, job_timeout: int) -> None:
        """Check cancellation AND memory on interval, for long loops without progress updates."""
        now = time.monotonic()
        if now - self._last_cancel_check >= self.CANCEL_CHECK_INTERVAL:
            self.check_cancellation(job_id)
            # Keep cancel flag alive during long compute loops to avoid TTL expiry races
            ttl = max(int(job_timeout or 3600), 3600)
            pipe = self.redis.pipeline()
            # Refresh heartbeat even when no progress events fire (prevents false watchdog alerts)
            pipe.set(f"backtest:heartbeat:{job_id}", datetime.now(UTC).isoformat())
            pipe.expire(f"backtest:heartbeat:{job_id}", ttl)
            pipe.expire(f"backtest:cancel:{job_id}", ttl)
            pipe.execute()
            self.check_memory()  # Also check memory during long compute loops
            self._last_cancel_check = now

    def check_memory(self) -> None:
        """Kill job if memory exceeds limit."""
        rss = self.process.memory_info().rss
        if rss > self.MAX_RSS_BYTES:
            raise MemoryError(f"Job exceeded {self.MAX_RSS_BYTES // 1e9:.0f}GB limit")

    def update_progress(
        self,
        job_id: str,
        pct: int,
        stage: str,
        current_date: str | None = None,
        job_timeout: int = 3600,
        *,
        skip_cancel_check: bool = False,
        skip_memory_check: bool = False,
    ) -> None:
        """
        Update progress and check cancellation/memory.

        Progress is stored in Redis for fast UI polling.
        Sync to DB every 10% and at/above 90% so coarse progress survives Redis key expiry.
        """
        if not skip_cancel_check:
            self.check_cancellation(job_id)
        if not skip_memory_check:
            self.check_memory()
        self._last_cancel_check = time.monotonic()

        # Dynamic TTL: max(job_timeout, 3600) to support long-running jobs
        ttl = max(job_timeout, 3600)
        payload = json.dumps({
            "pct": pct,
            "stage": stage,
            "current_date": current_date,
            "updated_at": datetime.now(UTC).isoformat(),
        })

        # Atomic write+expire to avoid TTL race between set and expire
        pipe = self.redis.pipeline()
        pipe.set(f"backtest:progress:{job_id}", payload)
        pipe.expire(f"backtest:progress:{job_id}", ttl)
        # Heartbeat for watchdog to detect stuck/lost workers
        pipe.set(f"backtest:heartbeat:{job_id}", datetime.now(UTC).isoformat())
        pipe.expire(f"backtest:heartbeat:{job_id}", ttl)
        # Also refresh cancel flag TTL if it exists.
        # NOTE: Redis EXPIRE on a non-existent key is a safe no-op (returns 0, no error).
        # This is intentional: cancel flag is only set by cancel_job(); refreshing during
        # progress is defensive but not critical. We accept the no-op rather than adding
        # EXISTS check overhead.
        pipe.expire(f"backtest:cancel:{job_id}", ttl)
        pipe.execute()

        # Sync to DB at coarse checkpoints (every 10%) for resilience
        if self.should_sync_db_progress(pct):
            self.update_db_progress(job_id, pct)

    def update_db_status(self, job_id: str, status: str, **kwargs: Any) -> None:
        """Update job status in Postgres (sync)."""
        TERMINAL_STATES = {"completed", "failed", "cancelled"}
        with self.db_pool.connection() as conn, conn.cursor(row_factory=dict_row) as cur:
            cur.execute("SELECT status FROM backtest_jobs WHERE job_id = %s", (job_id,))
            row = cur.fetchone()
            if not row:
                return
            if row["status"] in TERMINAL_STATES:
                return
            if status == "cancelled" and row["status"] not in ("running", "pending"):
                return
            sets = ["status = %s"]
            values = [status]
            for key, value in kwargs.items():
                sets.append(f"{key} = %s")
                values.append(value)
            values.append(job_id)
            cur.execute(f"UPDATE backtest_jobs SET {', '.join(sets)} WHERE job_id = %s", values)
            conn.commit()

    def update_db_progress(self, job_id: str, pct: int) -> None:
        """
        Persist progress to DB periodically for fallback when Redis expires.
        """
        with self.db_pool.connection() as conn, conn.cursor() as cur:
            cur.execute(
                "UPDATE backtest_jobs SET progress_pct = %s WHERE job_id = %s",
                (pct, job_id),
            )
            conn.commit()

    def should_sync_db_progress(self, pct: int) -> bool:
        """
        Determine if DB progress sync is needed.

        Syncs at 0, 10, 20, ..., 90, 100 (every 10%).
        No separate >=90 clause needed since 90 and 100 are divisible by 10.
        """
        return pct % 10 == 0


_RETRY_POOL: ConnectionPool | None = None


def _get_retry_pool() -> ConnectionPool:
    """
    Lazily create a shared psycopg pool for retry hook to avoid per-retry connections.

    IMPORTANT: This pool is a global singleton for the worker process lifetime.
    - DATABASE_URL must be set before any retry occurs; if missing, the hook fails loudly.
    - Pool persists until worker shutdown.
    """
    global _RETRY_POOL
    if _RETRY_POOL is None:
        db_url = os.getenv("DATABASE_URL")
        if not db_url:
            raise RuntimeError("DATABASE_URL not set; cannot create retry hook pool")
        _RETRY_POOL = ConnectionPool(conninfo=db_url)
        _RETRY_POOL.open()
    return _RETRY_POOL


def record_retry(job, *exc_info):
    """RQ retry hook: increment retry_count for automated retries."""
    pool = _get_retry_pool()
    with pool.connection() as conn, conn.cursor() as cur:
        cur.execute(
            "UPDATE backtest_jobs SET retry_count = COALESCE(retry_count,0) + 1 WHERE job_id = %s",
            (job.id,),
        )
        conn.commit()
    return False  # allow default exception handling to continue


# apps/backtest_worker/entrypoint.py - Worker bootstrap with retry handler
"""
Backtest worker entrypoint for RQ.

Validates environment, registers exception handlers, and starts the worker loop.
"""
import os
import sys
import structlog

from redis import Redis
from rq import Worker

from libs.backtest.worker import record_retry

logger = structlog.get_logger(__name__)

def main() -> None:
    """Worker entrypoint - validates env and starts RQ worker loop."""
    # Validate required environment variables
    redis_url = os.getenv("REDIS_URL")
    db_url = os.getenv("DATABASE_URL")

    if not redis_url:
        logger.error("worker_startup_failed", reason="REDIS_URL not set")
        sys.exit(1)
    if not db_url:
        logger.error("worker_startup_failed", reason="DATABASE_URL not set")
        sys.exit(1)

    redis = Redis.from_url(redis_url)

    # Verify Redis connectivity
    try:
        redis.ping()
    except Exception as e:
        logger.error("redis_connection_failed", error=str(e))
        sys.exit(1)

    # Create worker for all backtest queues (priority order: high, normal, low)
    queues = ["backtest_high", "backtest_normal", "backtest_low"]
    worker = Worker(queues, connection=redis)

    # Register retry handler for automatic retry_count increments
    worker.push_exc_handler(record_retry)

    logger.info("worker_starting", queues=queues, pid=os.getpid())
    worker.work(with_scheduler=False)


if __name__ == "__main__":
    main()


def run_backtest(config: dict[str, Any], created_by: str) -> dict[str, Any]:
    """
    RQ job entrypoint for backtest execution.

    Contract:
    - Input: config dict (from BacktestJobConfig.to_dict()), created_by string
    - Output: dict with job_id, result_path, summary_metrics
    - Progress: Redis key backtest:progress:{job_id} updated every ≤30s
    - Persistence: Results saved to Postgres (sync) and Parquet files
    - Cancellation: JobCancelled caught and returns {"cancelled": True} (NOT raised)

    CRITICAL: JobCancelled must NOT be raised - RQ marks raised exceptions as 'failed'.
    Instead, catch JobCancelled, update DB to 'cancelled', and return success with cancelled flag.
    """
    redis = Redis.from_url(os.getenv("REDIS_URL", "redis://localhost:6379"))
    db_pool = ConnectionPool(conninfo=os.getenv("DATABASE_URL"))
    db_pool.open()

    with db_pool.connection() as conn:
        job_config = BacktestJobConfig.from_dict(config)
        job_id = job_config.compute_job_id(created_by)  # Include user in hash
        current_job = get_current_job()
        job_timeout = int(current_job.timeout or BacktestJobQueue.DEFAULT_TIMEOUT) if current_job else BacktestJobQueue.DEFAULT_TIMEOUT
        worker = BacktestWorker(redis, db_pool)

        # Fail closed on dependency init: initialize all dependencies FIRST; only mark running after successful init
        try:
            # Initialize backtester with required dependencies
            worker.update_progress(job_id, 5, "init_dependencies", job_timeout=job_timeout)

            # PITBacktester Initialization Contract (per libs/alpha/research_platform.py):
            # - version_manager: DatasetVersionManager for snapshot management and PIT data access
            # - crsp_provider: CRSPLocalProvider for price/return data (snapshot-locked)
            # - compustat_provider: CompustatLocalProvider for fundamental data (snapshot-locked)
            # - metrics_adapter: AlphaMetricsAdapter for IC/ICIR computation (optional, auto-created)
            from libs.data_quality.versioning import DatasetVersionManager
            from libs.data_providers.crsp_local_provider import CRSPLocalProvider
            from libs.data_providers.compustat_local_provider import CompustatLocalProvider
            from libs.alpha.metrics import AlphaMetricsAdapter

            version_manager = DatasetVersionManager()
            crsp_provider = CRSPLocalProvider()
            compustat_provider = CompustatLocalProvider()
            metrics_adapter = AlphaMetricsAdapter()

            backtester = PITBacktester(
                version_manager=version_manager,
                crsp_provider=crsp_provider,
                compustat_provider=compustat_provider,
                metrics_adapter=metrics_adapter,
            )

            # Mark as running only after dependencies are up
            worker.update_db_status(job_id, "running", started_at=datetime.now(UTC))
            worker.update_progress(job_id, 0, "started", job_timeout=job_timeout)
            worker.update_progress(job_id, 10, "loading_data", job_timeout=job_timeout)

            # Get snapshot_id from config if provided (for reproducibility)
            snapshot_id = job_config.extra_params.get("snapshot_id")

            # Load alpha definition from registry
            from libs.alpha.registry import get_alpha_by_name
            alpha = get_alpha_by_name(job_config.alpha_name)

            # Run backtest
            # PITBacktester.run_backtest signature AFTER T5.1 (must be implemented):
            #   run_backtest(
            #       alpha,
            #       start_date,
            #       end_date,
            #       snapshot_id,
            #       weight_method,
            #       progress_callback: Callable[[int, Optional[date]], None] | None = None,
            #       cancel_check: Callable[[], None] | None = None,
            #   )
            # Progress callback maps PIT progress [0-100] → UI range [20-90]
            # and ensures cooperative cancellation checks run during compute loops.
            result = backtester.run_backtest(
                alpha=alpha,
                start_date=job_config.start_date,
                end_date=job_config.end_date,
                snapshot_id=snapshot_id,
                weight_method=job_config.weight_method,
                progress_callback=lambda pct, d: worker.update_progress(
                    job_id,
                    20 + round(pct * 0.7),   # stretch to 20-90 band while computing (PITBacktester only)
                    "computing",
                    str(d) if d else None,
                    job_timeout=job_timeout,
                ),
                cancel_check=lambda: worker.check_cancellation_periodic(job_id, job_timeout),
            )

            # Save results
            worker.update_progress(job_id, 90, "saving_parquet", job_timeout=job_timeout)
            result_path = _save_parquet_artifacts(job_id, result)

            worker.update_progress(job_id, 95, "saving_db", job_timeout=job_timeout)
            _save_result_to_db(conn, job_id, result, result_path)

            worker.update_progress(job_id, 100, "completed", job_timeout=job_timeout)
            worker.update_db_status(job_id, "completed", completed_at=datetime.now(UTC))

            return {
                "job_id": job_id,
                "result_path": str(result_path),
                "summary_metrics": {
                    "mean_ic": result.mean_ic,
                    "icir": result.icir,
                    "hit_rate": result.hit_rate,
                },
            }

        except JobCancelled:
            # CRITICAL: Do NOT re-raise - RQ would mark as 'failed'
            # Instead, return success with cancelled flag while preserving last progress
            last_progress = redis.get(f"backtest:progress:{job_id}")
            last_pct = json.loads(last_progress)["pct"] if last_progress else 0
            shutil.rmtree(Path("data/backtest_results") / job_id, ignore_errors=True)
            worker.update_db_status(job_id, "cancelled", completed_at=datetime.now(UTC))
            worker.update_progress(
                job_id,
                last_pct,
                "cancelled",
                skip_cancel_check=True,  # avoid re-raising JobCancelled inside handler
                skip_memory_check=True,
            )
            return {"job_id": job_id, "cancelled": True}

        except Exception as e:
            worker.update_db_status(
                job_id,
                "failed",
                error_message=str(e),
                completed_at=datetime.now(UTC),
            )
            raise


def _save_parquet_artifacts(job_id: str, result: BacktestResult) -> Path:
    """
    Save bulk time-series data to Parquet files.

    Schema Contract: See P4T4_TASK.md "Parquet Schema Contract (Authoritative)" section.
    T5.2 reader MUST stay synchronized with this writer.
    """
    import polars as pl

    def _validate_schema(df: pl.DataFrame, required: dict[str, pl.datatypes.DataType]) -> None:
        missing_cols = set(required.keys()) - set(df.columns)
        if missing_cols:
            raise ValueError(f"missing columns: {missing_cols}")
        for col, dtype in required.items():
            if df[col].dtype != dtype:
                raise ValueError(f"column {col} has type {df[col].dtype}, expected {dtype}")

    result_dir = Path("data/backtest_results") / job_id
    result_dir.mkdir(parents=True, exist_ok=True)

    required_signal_schema = {"date": pl.Date, "permno": pl.Int64, "signal": pl.Float64}
    required_weight_schema = {"date": pl.Date, "permno": pl.Int64, "weight": pl.Float64}
    required_ic_schema = {"date": pl.Date, "ic": pl.Float64, "rank_ic": pl.Float64}

    _validate_schema(result.daily_signals, required_signal_schema)
    _validate_schema(result.daily_weights, required_weight_schema)
    if result.daily_ic is None:
        raise ValueError("daily_ic DataFrame must be populated before parquet export")
    _validate_schema(result.daily_ic, required_ic_schema)

    # Validation runs post-compute (late) by design:
    # - BacktestResult schema is determined by PITBacktester output, which is only known after compute.
    # - Early validation would require duplicating the schema definition, risking drift.
    # - The alternative (add __post_init__ validation to BacktestResult) was considered but rejected
    #   because BacktestResult is defined in research_platform.py and modifying it for queue concerns
    #   would violate separation of concerns.
    # - Late validation is acceptable: schema violations are extremely unlikely if PITBacktester is correct,
    #   and the error message clearly identifies the issue when it does occur.

    # Save signals with explicit schema cast
    result.daily_signals.select(["date", "permno", "signal"]).cast(required_signal_schema).write_parquet(
        result_dir / "daily_signals.parquet",
        compression="snappy"
    )

    # Save weights with explicit schema cast
    result.daily_weights.select(["date", "permno", "weight"]).cast(required_weight_schema).write_parquet(
        result_dir / "daily_weights.parquet",
        compression="snappy"
    )

    # Save IC time series with validation
    result.daily_ic.select(["date", "ic", "rank_ic"]).cast(required_ic_schema).write_parquet(
        result_dir / "daily_ic.parquet",
        compression="snappy"
    )

    _write_summary_json(result_dir, result)

    return result_dir


def _write_summary_json(result_dir: Path, result: BacktestResult) -> None:
    """Persist summary metrics and reproducibility metadata alongside Parquet artifacts."""
    import json

    summary = {
        "mean_ic": result.mean_ic,
        "icir": result.icir,
        "hit_rate": result.hit_rate,
        "snapshot_id": result.snapshot_id,
        "dataset_version_ids": result.dataset_version_ids,
    }
    (result_dir / "summary.json").write_text(json.dumps(summary, default=str, indent=2))


def _save_result_to_db(conn, job_id: str, result: BacktestResult, result_path: Path) -> None:
    """Save summary metrics to Postgres (psycopg)."""
    if result.snapshot_id is None or result.dataset_version_ids is None:
        raise ValueError("BacktestResult must include snapshot_id and dataset_version_ids for reproducibility")

    with conn.cursor() as cur:
        cur.execute(
            """
            UPDATE backtest_jobs
            SET status='completed',
                result_path=%s,
                mean_ic=%s,
                icir=%s,
                hit_rate=%s,
                coverage=%s,
                long_short_spread=%s,
                average_turnover=%s,
                decay_half_life=%s,
                snapshot_id=%s,
                dataset_version_ids=%s,
                completed_at=%s
            WHERE job_id=%s
            """,
            (
                str(result_path),
                result.mean_ic,
                result.icir,
                result.hit_rate,
                result.coverage,
                result.long_short_spread,
                result.average_turnover,
                result.decay_half_life,
                result.snapshot_id,
                json.dumps(result.dataset_version_ids),
                datetime.now(UTC),
                job_id,
            ),
        )
        if cur.rowcount == 0:
            raise RuntimeError(f"BacktestJob {job_id} missing when saving result")
        conn.commit()
```

**Files to Create:**
- `libs/backtest/__init__.py`
- `libs/backtest/job_queue.py`
- `libs/backtest/worker.py`
- `libs/backtest/progress.py` (progress tracking helpers)
- `apps/backtest_worker/__init__.py`
- `apps/backtest_worker/entrypoint.py` (RQ worker bootstrap)
- `apps/backtest_worker/requirements.txt`
- `apps/backtest_worker/Dockerfile`
- `tests/libs/backtest/__init__.py`
- `tests/libs/backtest/test_job_queue.py`
- `tests/libs/backtest/test_worker.py`
- `docs/ADRs/ADR-0024-backtest-job-architecture.md`

**Files to Modify:**
- `docker-compose.yml` - Add `backtest_worker_{high,normal,low}` services and `backtest_data` volume
- `docker-compose.staging.yml` - Modify existing staging file with the same additions
- `docker-compose.ci.yml` - Modify existing CI file with the same additions
- `requirements.txt` - Add `rq>=1.16,<2.0.0`, `psutil>=5.9`

**ADR Content (ADR-0024):**
- Decision: Use RQ over Celery (simpler, single-worker sufficient)
- Decision: Redis-only queue (no file-based - race conditions)
- Decision: Idempotent job IDs via config hash
- Decision: Cooperative cancellation via Redis flag (not SIGTERM)
- Decision: Memory monitoring via psutil (RQ lacks native support)
- Trade-off: RQ lacks Celery's workflow chains, but backtests are independent

---

---
## Acceptance Criteria
### T5.1 Job Queue
- [ ] Jobs can be enqueued with 3 priority levels (high/normal/low)
- [ ] Same configuration produces identical job ID (unit test)
- [ ] Worker processes high-priority jobs before low (integration test)
- [ ] Progress updates written to Redis every ≤30 seconds during execution
- [ ] Queued jobs cancelled immediately; running jobs cancelled within 30 seconds (cooperative)
- [ ] Failed jobs retry 3 times with intervals [60s, 300s, 900s]
- [ ] Jobs exceeding 4GB RSS are killed by worker memory monitor
- [ ] Job enqueue completes in <200ms under normal load (unit test with `time.perf_counter()`)
- [ ] Queue handles 50 jobs/minute burst without failures (integration test with fakeredis)
- [ ] Watchdog marks jobs `failed` within 60s of heartbeat expiry (integration test)
- [ ] Watchdog does NOT mark jobs failed if heartbeat present (regression test)
- [ ] Heal counter prevents infinite re-enqueue loops (max 3 heals/hour per job)
- [ ] **Reproducibility:** Completed jobs MUST have `snapshot_id` and `dataset_version_ids` populated (worker raises `ValueError` if missing)
- [ ] **Performance Tests:** Use realistic tolerances or mock timing (e.g., `time.perf_counter` with injected clock) to avoid flaky timing-dependent assertions


---
## Risk Mitigation
| Risk | Impact | Mitigation |
|------|--------|------------|
| RQ scalability limits | Can't run many jobs | Monitor queue depth; upgrade to Celery if >10 concurrent needed |
| Long jobs consume shared resources | WRDS/DuckDB contention | Use read-only snapshots during backtests; respect domain locks |
| Worker memory exhaustion | OOM kills | psutil monitoring (RSS > 4GB kills job); job-level timeout |
| Redis unavailable | Queue operations fail | Health check in worker startup; graceful degradation to sync mode |
